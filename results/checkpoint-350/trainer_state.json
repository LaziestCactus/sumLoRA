{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 350,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005714285714285714,
      "grad_norm": 4.734104156494141,
      "learning_rate": 2.5e-05,
      "loss": 13.9076,
      "step": 1
    },
    {
      "epoch": 0.011428571428571429,
      "grad_norm": 4.688688278198242,
      "learning_rate": 5e-05,
      "loss": 14.1902,
      "step": 2
    },
    {
      "epoch": 0.017142857142857144,
      "grad_norm": 4.669456481933594,
      "learning_rate": 4.985632183908046e-05,
      "loss": 14.0286,
      "step": 3
    },
    {
      "epoch": 0.022857142857142857,
      "grad_norm": 4.867029190063477,
      "learning_rate": 4.971264367816092e-05,
      "loss": 14.2237,
      "step": 4
    },
    {
      "epoch": 0.02857142857142857,
      "grad_norm": 5.015556812286377,
      "learning_rate": 4.9568965517241384e-05,
      "loss": 13.616,
      "step": 5
    },
    {
      "epoch": 0.03428571428571429,
      "grad_norm": 5.157218933105469,
      "learning_rate": 4.9425287356321845e-05,
      "loss": 13.8035,
      "step": 6
    },
    {
      "epoch": 0.04,
      "grad_norm": 5.4685258865356445,
      "learning_rate": 4.92816091954023e-05,
      "loss": 14.0089,
      "step": 7
    },
    {
      "epoch": 0.045714285714285714,
      "grad_norm": 5.5777788162231445,
      "learning_rate": 4.913793103448276e-05,
      "loss": 13.4059,
      "step": 8
    },
    {
      "epoch": 0.05142857142857143,
      "grad_norm": 5.738358020782471,
      "learning_rate": 4.899425287356322e-05,
      "loss": 13.6484,
      "step": 9
    },
    {
      "epoch": 0.05714285714285714,
      "grad_norm": 5.921424865722656,
      "learning_rate": 4.885057471264368e-05,
      "loss": 13.5004,
      "step": 10
    },
    {
      "epoch": 0.06285714285714286,
      "grad_norm": 6.192677974700928,
      "learning_rate": 4.870689655172414e-05,
      "loss": 13.3534,
      "step": 11
    },
    {
      "epoch": 0.06857142857142857,
      "grad_norm": 6.892590045928955,
      "learning_rate": 4.85632183908046e-05,
      "loss": 13.5042,
      "step": 12
    },
    {
      "epoch": 0.07428571428571429,
      "grad_norm": 6.896397113800049,
      "learning_rate": 4.8419540229885056e-05,
      "loss": 13.1871,
      "step": 13
    },
    {
      "epoch": 0.08,
      "grad_norm": 7.2038092613220215,
      "learning_rate": 4.827586206896552e-05,
      "loss": 12.8569,
      "step": 14
    },
    {
      "epoch": 0.08571428571428572,
      "grad_norm": 7.192388534545898,
      "learning_rate": 4.813218390804598e-05,
      "loss": 12.6651,
      "step": 15
    },
    {
      "epoch": 0.09142857142857143,
      "grad_norm": 8.11798095703125,
      "learning_rate": 4.798850574712644e-05,
      "loss": 12.4462,
      "step": 16
    },
    {
      "epoch": 0.09714285714285714,
      "grad_norm": 7.660323619842529,
      "learning_rate": 4.78448275862069e-05,
      "loss": 12.4763,
      "step": 17
    },
    {
      "epoch": 0.10285714285714286,
      "grad_norm": 8.504059791564941,
      "learning_rate": 4.770114942528736e-05,
      "loss": 12.4963,
      "step": 18
    },
    {
      "epoch": 0.10857142857142857,
      "grad_norm": 9.043463706970215,
      "learning_rate": 4.755747126436782e-05,
      "loss": 12.0191,
      "step": 19
    },
    {
      "epoch": 0.11428571428571428,
      "grad_norm": 9.260602951049805,
      "learning_rate": 4.741379310344828e-05,
      "loss": 11.9981,
      "step": 20
    },
    {
      "epoch": 0.12,
      "grad_norm": 9.49374771118164,
      "learning_rate": 4.7270114942528734e-05,
      "loss": 11.7289,
      "step": 21
    },
    {
      "epoch": 0.12571428571428572,
      "grad_norm": 10.545248031616211,
      "learning_rate": 4.7126436781609195e-05,
      "loss": 11.6962,
      "step": 22
    },
    {
      "epoch": 0.13142857142857142,
      "grad_norm": 10.240142822265625,
      "learning_rate": 4.698275862068966e-05,
      "loss": 11.3761,
      "step": 23
    },
    {
      "epoch": 0.13714285714285715,
      "grad_norm": 10.286664009094238,
      "learning_rate": 4.6839080459770116e-05,
      "loss": 11.2499,
      "step": 24
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 10.407567024230957,
      "learning_rate": 4.669540229885058e-05,
      "loss": 10.9162,
      "step": 25
    },
    {
      "epoch": 0.14857142857142858,
      "grad_norm": 10.623666763305664,
      "learning_rate": 4.655172413793104e-05,
      "loss": 10.7591,
      "step": 26
    },
    {
      "epoch": 0.15428571428571428,
      "grad_norm": 10.69950008392334,
      "learning_rate": 4.640804597701149e-05,
      "loss": 10.5221,
      "step": 27
    },
    {
      "epoch": 0.16,
      "grad_norm": 10.864320755004883,
      "learning_rate": 4.626436781609196e-05,
      "loss": 10.5572,
      "step": 28
    },
    {
      "epoch": 0.1657142857142857,
      "grad_norm": 10.357378005981445,
      "learning_rate": 4.612068965517242e-05,
      "loss": 10.0629,
      "step": 29
    },
    {
      "epoch": 0.17142857142857143,
      "grad_norm": 10.4811429977417,
      "learning_rate": 4.597701149425287e-05,
      "loss": 10.1075,
      "step": 30
    },
    {
      "epoch": 0.17714285714285713,
      "grad_norm": 9.502153396606445,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 9.781,
      "step": 31
    },
    {
      "epoch": 0.18285714285714286,
      "grad_norm": 8.958112716674805,
      "learning_rate": 4.5689655172413794e-05,
      "loss": 9.5399,
      "step": 32
    },
    {
      "epoch": 0.18857142857142858,
      "grad_norm": 8.763415336608887,
      "learning_rate": 4.5545977011494255e-05,
      "loss": 9.522,
      "step": 33
    },
    {
      "epoch": 0.19428571428571428,
      "grad_norm": 8.113065719604492,
      "learning_rate": 4.5402298850574716e-05,
      "loss": 9.2024,
      "step": 34
    },
    {
      "epoch": 0.2,
      "grad_norm": 7.524439334869385,
      "learning_rate": 4.5258620689655176e-05,
      "loss": 9.2066,
      "step": 35
    },
    {
      "epoch": 0.2057142857142857,
      "grad_norm": 6.698114395141602,
      "learning_rate": 4.511494252873563e-05,
      "loss": 8.9114,
      "step": 36
    },
    {
      "epoch": 0.21142857142857144,
      "grad_norm": 5.377439975738525,
      "learning_rate": 4.49712643678161e-05,
      "loss": 8.6293,
      "step": 37
    },
    {
      "epoch": 0.21714285714285714,
      "grad_norm": 5.496832370758057,
      "learning_rate": 4.482758620689655e-05,
      "loss": 8.9604,
      "step": 38
    },
    {
      "epoch": 0.22285714285714286,
      "grad_norm": 4.224181652069092,
      "learning_rate": 4.468390804597701e-05,
      "loss": 8.5006,
      "step": 39
    },
    {
      "epoch": 0.22857142857142856,
      "grad_norm": 3.8616721630096436,
      "learning_rate": 4.454022988505747e-05,
      "loss": 8.4884,
      "step": 40
    },
    {
      "epoch": 0.2342857142857143,
      "grad_norm": 3.9038562774658203,
      "learning_rate": 4.4396551724137933e-05,
      "loss": 8.4343,
      "step": 41
    },
    {
      "epoch": 0.24,
      "grad_norm": 3.4288997650146484,
      "learning_rate": 4.4252873563218394e-05,
      "loss": 8.5282,
      "step": 42
    },
    {
      "epoch": 0.24571428571428572,
      "grad_norm": 2.8785414695739746,
      "learning_rate": 4.4109195402298855e-05,
      "loss": 8.443,
      "step": 43
    },
    {
      "epoch": 0.25142857142857145,
      "grad_norm": 2.583573579788208,
      "learning_rate": 4.396551724137931e-05,
      "loss": 8.4057,
      "step": 44
    },
    {
      "epoch": 0.2571428571428571,
      "grad_norm": 2.570979595184326,
      "learning_rate": 4.382183908045977e-05,
      "loss": 8.3133,
      "step": 45
    },
    {
      "epoch": 0.26285714285714284,
      "grad_norm": 2.3105335235595703,
      "learning_rate": 4.367816091954024e-05,
      "loss": 8.1461,
      "step": 46
    },
    {
      "epoch": 0.26857142857142857,
      "grad_norm": 2.1689274311065674,
      "learning_rate": 4.353448275862069e-05,
      "loss": 8.2865,
      "step": 47
    },
    {
      "epoch": 0.2742857142857143,
      "grad_norm": 2.0186712741851807,
      "learning_rate": 4.339080459770115e-05,
      "loss": 8.1457,
      "step": 48
    },
    {
      "epoch": 0.28,
      "grad_norm": 2.0910353660583496,
      "learning_rate": 4.324712643678161e-05,
      "loss": 8.1536,
      "step": 49
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 2.0380172729492188,
      "learning_rate": 4.3103448275862066e-05,
      "loss": 7.9387,
      "step": 50
    },
    {
      "epoch": 0.2914285714285714,
      "grad_norm": 2.040254831314087,
      "learning_rate": 4.295977011494253e-05,
      "loss": 8.0297,
      "step": 51
    },
    {
      "epoch": 0.29714285714285715,
      "grad_norm": 1.9704344272613525,
      "learning_rate": 4.2816091954022994e-05,
      "loss": 8.025,
      "step": 52
    },
    {
      "epoch": 0.3028571428571429,
      "grad_norm": 1.7914743423461914,
      "learning_rate": 4.267241379310345e-05,
      "loss": 8.0595,
      "step": 53
    },
    {
      "epoch": 0.30857142857142855,
      "grad_norm": 1.8056188821792603,
      "learning_rate": 4.252873563218391e-05,
      "loss": 7.8901,
      "step": 54
    },
    {
      "epoch": 0.3142857142857143,
      "grad_norm": 1.6990169286727905,
      "learning_rate": 4.238505747126437e-05,
      "loss": 7.8299,
      "step": 55
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.6180509328842163,
      "learning_rate": 4.224137931034483e-05,
      "loss": 8.0284,
      "step": 56
    },
    {
      "epoch": 0.32571428571428573,
      "grad_norm": 1.7032561302185059,
      "learning_rate": 4.209770114942529e-05,
      "loss": 7.9414,
      "step": 57
    },
    {
      "epoch": 0.3314285714285714,
      "grad_norm": 1.662298560142517,
      "learning_rate": 4.195402298850575e-05,
      "loss": 7.9831,
      "step": 58
    },
    {
      "epoch": 0.33714285714285713,
      "grad_norm": 1.8195732831954956,
      "learning_rate": 4.1810344827586205e-05,
      "loss": 7.8169,
      "step": 59
    },
    {
      "epoch": 0.34285714285714286,
      "grad_norm": 1.513528823852539,
      "learning_rate": 4.166666666666667e-05,
      "loss": 7.714,
      "step": 60
    },
    {
      "epoch": 0.3485714285714286,
      "grad_norm": 1.566235899925232,
      "learning_rate": 4.1522988505747126e-05,
      "loss": 7.745,
      "step": 61
    },
    {
      "epoch": 0.35428571428571426,
      "grad_norm": 1.5564112663269043,
      "learning_rate": 4.1379310344827587e-05,
      "loss": 7.569,
      "step": 62
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.7248448133468628,
      "learning_rate": 4.123563218390805e-05,
      "loss": 7.6897,
      "step": 63
    },
    {
      "epoch": 0.3657142857142857,
      "grad_norm": 1.644946575164795,
      "learning_rate": 4.109195402298851e-05,
      "loss": 7.6094,
      "step": 64
    },
    {
      "epoch": 0.37142857142857144,
      "grad_norm": 1.443126916885376,
      "learning_rate": 4.094827586206897e-05,
      "loss": 7.6283,
      "step": 65
    },
    {
      "epoch": 0.37714285714285717,
      "grad_norm": 1.3367385864257812,
      "learning_rate": 4.080459770114943e-05,
      "loss": 7.5984,
      "step": 66
    },
    {
      "epoch": 0.38285714285714284,
      "grad_norm": 1.5631877183914185,
      "learning_rate": 4.066091954022988e-05,
      "loss": 7.6894,
      "step": 67
    },
    {
      "epoch": 0.38857142857142857,
      "grad_norm": 1.375396490097046,
      "learning_rate": 4.0517241379310344e-05,
      "loss": 7.5174,
      "step": 68
    },
    {
      "epoch": 0.3942857142857143,
      "grad_norm": 1.4456536769866943,
      "learning_rate": 4.037356321839081e-05,
      "loss": 7.4662,
      "step": 69
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.3888661861419678,
      "learning_rate": 4.0229885057471265e-05,
      "loss": 7.6544,
      "step": 70
    },
    {
      "epoch": 0.4057142857142857,
      "grad_norm": 1.44789457321167,
      "learning_rate": 4.0086206896551726e-05,
      "loss": 7.4208,
      "step": 71
    },
    {
      "epoch": 0.4114285714285714,
      "grad_norm": 1.5650137662887573,
      "learning_rate": 3.9942528735632186e-05,
      "loss": 7.4408,
      "step": 72
    },
    {
      "epoch": 0.41714285714285715,
      "grad_norm": 1.4765836000442505,
      "learning_rate": 3.979885057471265e-05,
      "loss": 7.3618,
      "step": 73
    },
    {
      "epoch": 0.4228571428571429,
      "grad_norm": 1.3371119499206543,
      "learning_rate": 3.965517241379311e-05,
      "loss": 7.5209,
      "step": 74
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 1.4826536178588867,
      "learning_rate": 3.951149425287357e-05,
      "loss": 7.331,
      "step": 75
    },
    {
      "epoch": 0.4342857142857143,
      "grad_norm": 1.195361852645874,
      "learning_rate": 3.936781609195402e-05,
      "loss": 7.3602,
      "step": 76
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.3845562934875488,
      "learning_rate": 3.922413793103448e-05,
      "loss": 7.3402,
      "step": 77
    },
    {
      "epoch": 0.44571428571428573,
      "grad_norm": 1.2941266298294067,
      "learning_rate": 3.908045977011495e-05,
      "loss": 7.305,
      "step": 78
    },
    {
      "epoch": 0.4514285714285714,
      "grad_norm": 1.3069372177124023,
      "learning_rate": 3.8936781609195404e-05,
      "loss": 7.2277,
      "step": 79
    },
    {
      "epoch": 0.45714285714285713,
      "grad_norm": 1.2004444599151611,
      "learning_rate": 3.8793103448275865e-05,
      "loss": 7.3385,
      "step": 80
    },
    {
      "epoch": 0.46285714285714286,
      "grad_norm": 1.1602362394332886,
      "learning_rate": 3.8649425287356325e-05,
      "loss": 7.4368,
      "step": 81
    },
    {
      "epoch": 0.4685714285714286,
      "grad_norm": 1.20919930934906,
      "learning_rate": 3.850574712643678e-05,
      "loss": 7.3484,
      "step": 82
    },
    {
      "epoch": 0.4742857142857143,
      "grad_norm": 1.3515769243240356,
      "learning_rate": 3.8362068965517246e-05,
      "loss": 7.2325,
      "step": 83
    },
    {
      "epoch": 0.48,
      "grad_norm": 1.211908221244812,
      "learning_rate": 3.82183908045977e-05,
      "loss": 7.2479,
      "step": 84
    },
    {
      "epoch": 0.4857142857142857,
      "grad_norm": 1.0954363346099854,
      "learning_rate": 3.807471264367816e-05,
      "loss": 7.1294,
      "step": 85
    },
    {
      "epoch": 0.49142857142857144,
      "grad_norm": 1.063289999961853,
      "learning_rate": 3.793103448275862e-05,
      "loss": 7.1341,
      "step": 86
    },
    {
      "epoch": 0.49714285714285716,
      "grad_norm": 1.3500391244888306,
      "learning_rate": 3.778735632183908e-05,
      "loss": 7.1305,
      "step": 87
    },
    {
      "epoch": 0.5028571428571429,
      "grad_norm": 1.38892662525177,
      "learning_rate": 3.764367816091954e-05,
      "loss": 7.04,
      "step": 88
    },
    {
      "epoch": 0.5085714285714286,
      "grad_norm": 1.0226433277130127,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 7.3132,
      "step": 89
    },
    {
      "epoch": 0.5142857142857142,
      "grad_norm": 1.0495212078094482,
      "learning_rate": 3.735632183908046e-05,
      "loss": 7.1278,
      "step": 90
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.9005922675132751,
      "learning_rate": 3.721264367816092e-05,
      "loss": 7.3023,
      "step": 91
    },
    {
      "epoch": 0.5257142857142857,
      "grad_norm": 1.1640640497207642,
      "learning_rate": 3.7068965517241385e-05,
      "loss": 7.0332,
      "step": 92
    },
    {
      "epoch": 0.5314285714285715,
      "grad_norm": 1.0552617311477661,
      "learning_rate": 3.692528735632184e-05,
      "loss": 7.056,
      "step": 93
    },
    {
      "epoch": 0.5371428571428571,
      "grad_norm": 1.1221542358398438,
      "learning_rate": 3.67816091954023e-05,
      "loss": 7.1293,
      "step": 94
    },
    {
      "epoch": 0.5428571428571428,
      "grad_norm": 0.8952711820602417,
      "learning_rate": 3.663793103448276e-05,
      "loss": 7.0121,
      "step": 95
    },
    {
      "epoch": 0.5485714285714286,
      "grad_norm": 0.9861471652984619,
      "learning_rate": 3.649425287356322e-05,
      "loss": 6.9997,
      "step": 96
    },
    {
      "epoch": 0.5542857142857143,
      "grad_norm": 0.9376500844955444,
      "learning_rate": 3.635057471264368e-05,
      "loss": 7.0763,
      "step": 97
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.083903431892395,
      "learning_rate": 3.620689655172414e-05,
      "loss": 7.2686,
      "step": 98
    },
    {
      "epoch": 0.5657142857142857,
      "grad_norm": 0.8859241008758545,
      "learning_rate": 3.6063218390804596e-05,
      "loss": 7.0389,
      "step": 99
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.9634149670600891,
      "learning_rate": 3.591954022988506e-05,
      "loss": 7.2023,
      "step": 100
    },
    {
      "epoch": 0.5771428571428572,
      "grad_norm": 0.9053223133087158,
      "learning_rate": 3.5775862068965524e-05,
      "loss": 7.0582,
      "step": 101
    },
    {
      "epoch": 0.5828571428571429,
      "grad_norm": 1.30876624584198,
      "learning_rate": 3.563218390804598e-05,
      "loss": 7.1995,
      "step": 102
    },
    {
      "epoch": 0.5885714285714285,
      "grad_norm": 0.7509104609489441,
      "learning_rate": 3.548850574712644e-05,
      "loss": 6.956,
      "step": 103
    },
    {
      "epoch": 0.5942857142857143,
      "grad_norm": 0.8174710869789124,
      "learning_rate": 3.53448275862069e-05,
      "loss": 7.069,
      "step": 104
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.8188071250915527,
      "learning_rate": 3.5201149425287353e-05,
      "loss": 6.9901,
      "step": 105
    },
    {
      "epoch": 0.6057142857142858,
      "grad_norm": 0.9899850487709045,
      "learning_rate": 3.505747126436782e-05,
      "loss": 6.8039,
      "step": 106
    },
    {
      "epoch": 0.6114285714285714,
      "grad_norm": 0.8077340126037598,
      "learning_rate": 3.4913793103448275e-05,
      "loss": 6.9605,
      "step": 107
    },
    {
      "epoch": 0.6171428571428571,
      "grad_norm": 0.833648681640625,
      "learning_rate": 3.4770114942528735e-05,
      "loss": 6.9374,
      "step": 108
    },
    {
      "epoch": 0.6228571428571429,
      "grad_norm": 0.6740597486495972,
      "learning_rate": 3.4626436781609196e-05,
      "loss": 7.0232,
      "step": 109
    },
    {
      "epoch": 0.6285714285714286,
      "grad_norm": 0.7466224431991577,
      "learning_rate": 3.4482758620689657e-05,
      "loss": 6.893,
      "step": 110
    },
    {
      "epoch": 0.6342857142857142,
      "grad_norm": 0.7170855402946472,
      "learning_rate": 3.433908045977012e-05,
      "loss": 6.8466,
      "step": 111
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.7870339751243591,
      "learning_rate": 3.419540229885058e-05,
      "loss": 6.8486,
      "step": 112
    },
    {
      "epoch": 0.6457142857142857,
      "grad_norm": 0.7868096232414246,
      "learning_rate": 3.405172413793103e-05,
      "loss": 6.7991,
      "step": 113
    },
    {
      "epoch": 0.6514285714285715,
      "grad_norm": 0.7256597280502319,
      "learning_rate": 3.390804597701149e-05,
      "loss": 6.893,
      "step": 114
    },
    {
      "epoch": 0.6571428571428571,
      "grad_norm": 0.7966641187667847,
      "learning_rate": 3.376436781609196e-05,
      "loss": 6.9282,
      "step": 115
    },
    {
      "epoch": 0.6628571428571428,
      "grad_norm": 0.7102231383323669,
      "learning_rate": 3.3620689655172414e-05,
      "loss": 6.7647,
      "step": 116
    },
    {
      "epoch": 0.6685714285714286,
      "grad_norm": 0.715661883354187,
      "learning_rate": 3.3477011494252874e-05,
      "loss": 6.7768,
      "step": 117
    },
    {
      "epoch": 0.6742857142857143,
      "grad_norm": 0.9077039957046509,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 6.7229,
      "step": 118
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.8525540828704834,
      "learning_rate": 3.3189655172413796e-05,
      "loss": 6.9566,
      "step": 119
    },
    {
      "epoch": 0.6857142857142857,
      "grad_norm": 0.7130221128463745,
      "learning_rate": 3.3045977011494256e-05,
      "loss": 6.6608,
      "step": 120
    },
    {
      "epoch": 0.6914285714285714,
      "grad_norm": 0.7121731638908386,
      "learning_rate": 3.290229885057472e-05,
      "loss": 6.7702,
      "step": 121
    },
    {
      "epoch": 0.6971428571428572,
      "grad_norm": 1.043737530708313,
      "learning_rate": 3.275862068965517e-05,
      "loss": 7.1461,
      "step": 122
    },
    {
      "epoch": 0.7028571428571428,
      "grad_norm": 1.0193015336990356,
      "learning_rate": 3.261494252873563e-05,
      "loss": 7.1041,
      "step": 123
    },
    {
      "epoch": 0.7085714285714285,
      "grad_norm": 0.9358054399490356,
      "learning_rate": 3.24712643678161e-05,
      "loss": 6.8803,
      "step": 124
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 0.6234232187271118,
      "learning_rate": 3.232758620689655e-05,
      "loss": 6.8027,
      "step": 125
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.7101345062255859,
      "learning_rate": 3.218390804597701e-05,
      "loss": 6.7755,
      "step": 126
    },
    {
      "epoch": 0.7257142857142858,
      "grad_norm": 0.670452356338501,
      "learning_rate": 3.2040229885057474e-05,
      "loss": 6.796,
      "step": 127
    },
    {
      "epoch": 0.7314285714285714,
      "grad_norm": 0.8013310432434082,
      "learning_rate": 3.1896551724137935e-05,
      "loss": 6.6561,
      "step": 128
    },
    {
      "epoch": 0.7371428571428571,
      "grad_norm": 1.1173397302627563,
      "learning_rate": 3.1752873563218395e-05,
      "loss": 6.6202,
      "step": 129
    },
    {
      "epoch": 0.7428571428571429,
      "grad_norm": 0.7588788270950317,
      "learning_rate": 3.160919540229885e-05,
      "loss": 6.6964,
      "step": 130
    },
    {
      "epoch": 0.7485714285714286,
      "grad_norm": 0.7814266681671143,
      "learning_rate": 3.146551724137931e-05,
      "loss": 6.971,
      "step": 131
    },
    {
      "epoch": 0.7542857142857143,
      "grad_norm": 0.9376457333564758,
      "learning_rate": 3.132183908045977e-05,
      "loss": 6.7952,
      "step": 132
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.8544796705245972,
      "learning_rate": 3.117816091954023e-05,
      "loss": 6.9636,
      "step": 133
    },
    {
      "epoch": 0.7657142857142857,
      "grad_norm": 0.7589293718338013,
      "learning_rate": 3.103448275862069e-05,
      "loss": 6.846,
      "step": 134
    },
    {
      "epoch": 0.7714285714285715,
      "grad_norm": 0.6650277376174927,
      "learning_rate": 3.089080459770115e-05,
      "loss": 6.7789,
      "step": 135
    },
    {
      "epoch": 0.7771428571428571,
      "grad_norm": 0.6996923685073853,
      "learning_rate": 3.0747126436781606e-05,
      "loss": 6.7039,
      "step": 136
    },
    {
      "epoch": 0.7828571428571428,
      "grad_norm": 0.6492699384689331,
      "learning_rate": 3.060344827586207e-05,
      "loss": 6.7705,
      "step": 137
    },
    {
      "epoch": 0.7885714285714286,
      "grad_norm": 0.9465277791023254,
      "learning_rate": 3.045977011494253e-05,
      "loss": 6.8385,
      "step": 138
    },
    {
      "epoch": 0.7942857142857143,
      "grad_norm": 0.7356674671173096,
      "learning_rate": 3.031609195402299e-05,
      "loss": 6.7908,
      "step": 139
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.7467672824859619,
      "learning_rate": 3.017241379310345e-05,
      "loss": 6.7474,
      "step": 140
    },
    {
      "epoch": 0.8057142857142857,
      "grad_norm": 0.7478603720664978,
      "learning_rate": 3.0028735632183906e-05,
      "loss": 6.7355,
      "step": 141
    },
    {
      "epoch": 0.8114285714285714,
      "grad_norm": 0.9961782097816467,
      "learning_rate": 2.988505747126437e-05,
      "loss": 6.8824,
      "step": 142
    },
    {
      "epoch": 0.8171428571428572,
      "grad_norm": 1.0760139226913452,
      "learning_rate": 2.974137931034483e-05,
      "loss": 6.7681,
      "step": 143
    },
    {
      "epoch": 0.8228571428571428,
      "grad_norm": 0.7593245506286621,
      "learning_rate": 2.9597701149425288e-05,
      "loss": 6.9095,
      "step": 144
    },
    {
      "epoch": 0.8285714285714286,
      "grad_norm": 0.666109025478363,
      "learning_rate": 2.945402298850575e-05,
      "loss": 6.9057,
      "step": 145
    },
    {
      "epoch": 0.8342857142857143,
      "grad_norm": 0.6952930688858032,
      "learning_rate": 2.9310344827586206e-05,
      "loss": 6.7661,
      "step": 146
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.809601366519928,
      "learning_rate": 2.916666666666667e-05,
      "loss": 6.7482,
      "step": 147
    },
    {
      "epoch": 0.8457142857142858,
      "grad_norm": 0.7971353530883789,
      "learning_rate": 2.9022988505747127e-05,
      "loss": 6.6361,
      "step": 148
    },
    {
      "epoch": 0.8514285714285714,
      "grad_norm": 0.9636760950088501,
      "learning_rate": 2.8879310344827588e-05,
      "loss": 6.6073,
      "step": 149
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 0.7329898476600647,
      "learning_rate": 2.8735632183908045e-05,
      "loss": 6.8508,
      "step": 150
    },
    {
      "epoch": 0.8628571428571429,
      "grad_norm": 0.8117210865020752,
      "learning_rate": 2.859195402298851e-05,
      "loss": 6.6815,
      "step": 151
    },
    {
      "epoch": 0.8685714285714285,
      "grad_norm": 0.7563768029212952,
      "learning_rate": 2.844827586206897e-05,
      "loss": 6.6319,
      "step": 152
    },
    {
      "epoch": 0.8742857142857143,
      "grad_norm": 0.7067774534225464,
      "learning_rate": 2.8304597701149427e-05,
      "loss": 6.7814,
      "step": 153
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.5822588801383972,
      "learning_rate": 2.8160919540229884e-05,
      "loss": 6.6917,
      "step": 154
    },
    {
      "epoch": 0.8857142857142857,
      "grad_norm": 0.646780788898468,
      "learning_rate": 2.8017241379310345e-05,
      "loss": 6.7429,
      "step": 155
    },
    {
      "epoch": 0.8914285714285715,
      "grad_norm": 0.6159152388572693,
      "learning_rate": 2.787356321839081e-05,
      "loss": 6.6941,
      "step": 156
    },
    {
      "epoch": 0.8971428571428571,
      "grad_norm": 0.7186821103096008,
      "learning_rate": 2.7729885057471266e-05,
      "loss": 6.862,
      "step": 157
    },
    {
      "epoch": 0.9028571428571428,
      "grad_norm": 0.6553775072097778,
      "learning_rate": 2.7586206896551727e-05,
      "loss": 6.7538,
      "step": 158
    },
    {
      "epoch": 0.9085714285714286,
      "grad_norm": 0.7856001257896423,
      "learning_rate": 2.7442528735632184e-05,
      "loss": 6.7246,
      "step": 159
    },
    {
      "epoch": 0.9142857142857143,
      "grad_norm": 1.0811220407485962,
      "learning_rate": 2.7298850574712648e-05,
      "loss": 6.7535,
      "step": 160
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.8875728845596313,
      "learning_rate": 2.7155172413793105e-05,
      "loss": 6.8745,
      "step": 161
    },
    {
      "epoch": 0.9257142857142857,
      "grad_norm": 1.267198920249939,
      "learning_rate": 2.7011494252873566e-05,
      "loss": 6.9565,
      "step": 162
    },
    {
      "epoch": 0.9314285714285714,
      "grad_norm": 1.2330619096755981,
      "learning_rate": 2.6867816091954023e-05,
      "loss": 7.0475,
      "step": 163
    },
    {
      "epoch": 0.9371428571428572,
      "grad_norm": 0.6517418622970581,
      "learning_rate": 2.672413793103448e-05,
      "loss": 6.5813,
      "step": 164
    },
    {
      "epoch": 0.9428571428571428,
      "grad_norm": 0.7548835277557373,
      "learning_rate": 2.6580459770114948e-05,
      "loss": 6.8112,
      "step": 165
    },
    {
      "epoch": 0.9485714285714286,
      "grad_norm": 0.7822244167327881,
      "learning_rate": 2.6436781609195405e-05,
      "loss": 6.7004,
      "step": 166
    },
    {
      "epoch": 0.9542857142857143,
      "grad_norm": 0.6529566049575806,
      "learning_rate": 2.6293103448275862e-05,
      "loss": 6.8502,
      "step": 167
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.8044252991676331,
      "learning_rate": 2.6149425287356323e-05,
      "loss": 6.7701,
      "step": 168
    },
    {
      "epoch": 0.9657142857142857,
      "grad_norm": 0.9309370517730713,
      "learning_rate": 2.600574712643678e-05,
      "loss": 6.8721,
      "step": 169
    },
    {
      "epoch": 0.9714285714285714,
      "grad_norm": 0.8181586265563965,
      "learning_rate": 2.5862068965517244e-05,
      "loss": 6.8806,
      "step": 170
    },
    {
      "epoch": 0.9771428571428571,
      "grad_norm": 0.6466830372810364,
      "learning_rate": 2.57183908045977e-05,
      "loss": 6.7925,
      "step": 171
    },
    {
      "epoch": 0.9828571428571429,
      "grad_norm": 0.6848157644271851,
      "learning_rate": 2.5574712643678162e-05,
      "loss": 6.8215,
      "step": 172
    },
    {
      "epoch": 0.9885714285714285,
      "grad_norm": 0.764509379863739,
      "learning_rate": 2.543103448275862e-05,
      "loss": 6.8307,
      "step": 173
    },
    {
      "epoch": 0.9942857142857143,
      "grad_norm": 0.7374814748764038,
      "learning_rate": 2.5287356321839083e-05,
      "loss": 6.7607,
      "step": 174
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7522782683372498,
      "learning_rate": 2.5143678160919544e-05,
      "loss": 6.7787,
      "step": 175
    },
    {
      "epoch": 1.0057142857142858,
      "grad_norm": 0.7035015821456909,
      "learning_rate": 2.5e-05,
      "loss": 6.8182,
      "step": 176
    },
    {
      "epoch": 1.0114285714285713,
      "grad_norm": 0.904575526714325,
      "learning_rate": 2.485632183908046e-05,
      "loss": 6.7934,
      "step": 177
    },
    {
      "epoch": 1.0171428571428571,
      "grad_norm": 0.9241635203361511,
      "learning_rate": 2.4712643678160922e-05,
      "loss": 6.7107,
      "step": 178
    },
    {
      "epoch": 1.022857142857143,
      "grad_norm": 0.786632776260376,
      "learning_rate": 2.456896551724138e-05,
      "loss": 6.7812,
      "step": 179
    },
    {
      "epoch": 1.0285714285714285,
      "grad_norm": 0.8757497668266296,
      "learning_rate": 2.442528735632184e-05,
      "loss": 6.7454,
      "step": 180
    },
    {
      "epoch": 1.0342857142857143,
      "grad_norm": 0.8794564604759216,
      "learning_rate": 2.42816091954023e-05,
      "loss": 6.5768,
      "step": 181
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.696137011051178,
      "learning_rate": 2.413793103448276e-05,
      "loss": 6.892,
      "step": 182
    },
    {
      "epoch": 1.0457142857142858,
      "grad_norm": 0.7743188738822937,
      "learning_rate": 2.399425287356322e-05,
      "loss": 6.7699,
      "step": 183
    },
    {
      "epoch": 1.0514285714285714,
      "grad_norm": 0.717099666595459,
      "learning_rate": 2.385057471264368e-05,
      "loss": 6.7778,
      "step": 184
    },
    {
      "epoch": 1.0571428571428572,
      "grad_norm": 0.8388769030570984,
      "learning_rate": 2.370689655172414e-05,
      "loss": 6.8852,
      "step": 185
    },
    {
      "epoch": 1.062857142857143,
      "grad_norm": 0.8177732229232788,
      "learning_rate": 2.3563218390804597e-05,
      "loss": 6.8803,
      "step": 186
    },
    {
      "epoch": 1.0685714285714285,
      "grad_norm": 0.8145492076873779,
      "learning_rate": 2.3419540229885058e-05,
      "loss": 6.8766,
      "step": 187
    },
    {
      "epoch": 1.0742857142857143,
      "grad_norm": 0.7254337668418884,
      "learning_rate": 2.327586206896552e-05,
      "loss": 6.7578,
      "step": 188
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.727483868598938,
      "learning_rate": 2.313218390804598e-05,
      "loss": 6.5384,
      "step": 189
    },
    {
      "epoch": 1.0857142857142856,
      "grad_norm": 0.83061283826828,
      "learning_rate": 2.2988505747126437e-05,
      "loss": 6.8199,
      "step": 190
    },
    {
      "epoch": 1.0914285714285714,
      "grad_norm": 0.7623107433319092,
      "learning_rate": 2.2844827586206897e-05,
      "loss": 6.7391,
      "step": 191
    },
    {
      "epoch": 1.0971428571428572,
      "grad_norm": 1.060218334197998,
      "learning_rate": 2.2701149425287358e-05,
      "loss": 6.9087,
      "step": 192
    },
    {
      "epoch": 1.1028571428571428,
      "grad_norm": 0.6576517820358276,
      "learning_rate": 2.2557471264367815e-05,
      "loss": 6.5856,
      "step": 193
    },
    {
      "epoch": 1.1085714285714285,
      "grad_norm": 0.7236393094062805,
      "learning_rate": 2.2413793103448276e-05,
      "loss": 6.8349,
      "step": 194
    },
    {
      "epoch": 1.1142857142857143,
      "grad_norm": 0.9564034938812256,
      "learning_rate": 2.2270114942528736e-05,
      "loss": 6.9315,
      "step": 195
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.8460580110549927,
      "learning_rate": 2.2126436781609197e-05,
      "loss": 6.7584,
      "step": 196
    },
    {
      "epoch": 1.1257142857142857,
      "grad_norm": 0.9434083700180054,
      "learning_rate": 2.1982758620689654e-05,
      "loss": 6.8343,
      "step": 197
    },
    {
      "epoch": 1.1314285714285715,
      "grad_norm": 0.5953662991523743,
      "learning_rate": 2.183908045977012e-05,
      "loss": 6.7411,
      "step": 198
    },
    {
      "epoch": 1.1371428571428572,
      "grad_norm": 1.225170612335205,
      "learning_rate": 2.1695402298850576e-05,
      "loss": 6.7126,
      "step": 199
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 0.7605767846107483,
      "learning_rate": 2.1551724137931033e-05,
      "loss": 6.8714,
      "step": 200
    },
    {
      "epoch": 1.1485714285714286,
      "grad_norm": 1.0481804609298706,
      "learning_rate": 2.1408045977011497e-05,
      "loss": 6.7395,
      "step": 201
    },
    {
      "epoch": 1.1542857142857144,
      "grad_norm": 1.045406699180603,
      "learning_rate": 2.1264367816091954e-05,
      "loss": 6.6626,
      "step": 202
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.6884213089942932,
      "learning_rate": 2.1120689655172415e-05,
      "loss": 6.8294,
      "step": 203
    },
    {
      "epoch": 1.1657142857142857,
      "grad_norm": 1.1430480480194092,
      "learning_rate": 2.0977011494252875e-05,
      "loss": 6.5956,
      "step": 204
    },
    {
      "epoch": 1.1714285714285715,
      "grad_norm": 0.682840883731842,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 6.8577,
      "step": 205
    },
    {
      "epoch": 1.177142857142857,
      "grad_norm": 0.7034410238265991,
      "learning_rate": 2.0689655172413793e-05,
      "loss": 6.8151,
      "step": 206
    },
    {
      "epoch": 1.1828571428571428,
      "grad_norm": 0.8354460000991821,
      "learning_rate": 2.0545977011494254e-05,
      "loss": 6.8971,
      "step": 207
    },
    {
      "epoch": 1.1885714285714286,
      "grad_norm": 0.8076030611991882,
      "learning_rate": 2.0402298850574715e-05,
      "loss": 6.5571,
      "step": 208
    },
    {
      "epoch": 1.1942857142857144,
      "grad_norm": 0.7906699776649475,
      "learning_rate": 2.0258620689655172e-05,
      "loss": 6.6069,
      "step": 209
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.7445952892303467,
      "learning_rate": 2.0114942528735632e-05,
      "loss": 6.7437,
      "step": 210
    },
    {
      "epoch": 1.2057142857142857,
      "grad_norm": 0.7042129635810852,
      "learning_rate": 1.9971264367816093e-05,
      "loss": 6.7606,
      "step": 211
    },
    {
      "epoch": 1.2114285714285715,
      "grad_norm": 0.9864792823791504,
      "learning_rate": 1.9827586206896554e-05,
      "loss": 6.4798,
      "step": 212
    },
    {
      "epoch": 1.217142857142857,
      "grad_norm": 0.9899852871894836,
      "learning_rate": 1.968390804597701e-05,
      "loss": 6.9784,
      "step": 213
    },
    {
      "epoch": 1.2228571428571429,
      "grad_norm": 0.836073100566864,
      "learning_rate": 1.9540229885057475e-05,
      "loss": 6.6716,
      "step": 214
    },
    {
      "epoch": 1.2285714285714286,
      "grad_norm": 1.0307509899139404,
      "learning_rate": 1.9396551724137932e-05,
      "loss": 6.6324,
      "step": 215
    },
    {
      "epoch": 1.2342857142857142,
      "grad_norm": 0.7405692338943481,
      "learning_rate": 1.925287356321839e-05,
      "loss": 6.6671,
      "step": 216
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.3284406661987305,
      "learning_rate": 1.910919540229885e-05,
      "loss": 6.3804,
      "step": 217
    },
    {
      "epoch": 1.2457142857142858,
      "grad_norm": 0.9009160399436951,
      "learning_rate": 1.896551724137931e-05,
      "loss": 6.8138,
      "step": 218
    },
    {
      "epoch": 1.2514285714285713,
      "grad_norm": 0.7886279821395874,
      "learning_rate": 1.882183908045977e-05,
      "loss": 6.5302,
      "step": 219
    },
    {
      "epoch": 1.2571428571428571,
      "grad_norm": 1.0475308895111084,
      "learning_rate": 1.867816091954023e-05,
      "loss": 6.5127,
      "step": 220
    },
    {
      "epoch": 1.262857142857143,
      "grad_norm": 0.8820207715034485,
      "learning_rate": 1.8534482758620693e-05,
      "loss": 6.6629,
      "step": 221
    },
    {
      "epoch": 1.2685714285714287,
      "grad_norm": 0.9832252264022827,
      "learning_rate": 1.839080459770115e-05,
      "loss": 6.4668,
      "step": 222
    },
    {
      "epoch": 1.2742857142857142,
      "grad_norm": 0.6566945910453796,
      "learning_rate": 1.824712643678161e-05,
      "loss": 6.6544,
      "step": 223
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.8750917315483093,
      "learning_rate": 1.810344827586207e-05,
      "loss": 6.5839,
      "step": 224
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 1.1329548358917236,
      "learning_rate": 1.795977011494253e-05,
      "loss": 6.8676,
      "step": 225
    },
    {
      "epoch": 1.2914285714285714,
      "grad_norm": 0.7641705870628357,
      "learning_rate": 1.781609195402299e-05,
      "loss": 6.6759,
      "step": 226
    },
    {
      "epoch": 1.2971428571428572,
      "grad_norm": 0.8657205104827881,
      "learning_rate": 1.767241379310345e-05,
      "loss": 6.6146,
      "step": 227
    },
    {
      "epoch": 1.302857142857143,
      "grad_norm": 1.0381094217300415,
      "learning_rate": 1.752873563218391e-05,
      "loss": 6.8159,
      "step": 228
    },
    {
      "epoch": 1.3085714285714285,
      "grad_norm": 0.7413362860679626,
      "learning_rate": 1.7385057471264368e-05,
      "loss": 6.6624,
      "step": 229
    },
    {
      "epoch": 1.3142857142857143,
      "grad_norm": 0.785140335559845,
      "learning_rate": 1.7241379310344828e-05,
      "loss": 6.7335,
      "step": 230
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.6497049331665039,
      "learning_rate": 1.709770114942529e-05,
      "loss": 6.7804,
      "step": 231
    },
    {
      "epoch": 1.3257142857142856,
      "grad_norm": 0.6942557096481323,
      "learning_rate": 1.6954022988505746e-05,
      "loss": 6.7159,
      "step": 232
    },
    {
      "epoch": 1.3314285714285714,
      "grad_norm": 0.7536571621894836,
      "learning_rate": 1.6810344827586207e-05,
      "loss": 6.5523,
      "step": 233
    },
    {
      "epoch": 1.3371428571428572,
      "grad_norm": 0.7552345991134644,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 6.8085,
      "step": 234
    },
    {
      "epoch": 1.342857142857143,
      "grad_norm": 0.6545723676681519,
      "learning_rate": 1.6522988505747128e-05,
      "loss": 6.7389,
      "step": 235
    },
    {
      "epoch": 1.3485714285714285,
      "grad_norm": 0.8456192016601562,
      "learning_rate": 1.6379310344827585e-05,
      "loss": 6.6087,
      "step": 236
    },
    {
      "epoch": 1.3542857142857143,
      "grad_norm": 1.667680025100708,
      "learning_rate": 1.623563218390805e-05,
      "loss": 6.457,
      "step": 237
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.8172698616981506,
      "learning_rate": 1.6091954022988507e-05,
      "loss": 6.6941,
      "step": 238
    },
    {
      "epoch": 1.3657142857142857,
      "grad_norm": 0.8040780425071716,
      "learning_rate": 1.5948275862068967e-05,
      "loss": 6.8025,
      "step": 239
    },
    {
      "epoch": 1.3714285714285714,
      "grad_norm": 0.703827977180481,
      "learning_rate": 1.5804597701149425e-05,
      "loss": 6.6441,
      "step": 240
    },
    {
      "epoch": 1.3771428571428572,
      "grad_norm": 0.6604601144790649,
      "learning_rate": 1.5660919540229885e-05,
      "loss": 6.6366,
      "step": 241
    },
    {
      "epoch": 1.3828571428571428,
      "grad_norm": 1.0257680416107178,
      "learning_rate": 1.5517241379310346e-05,
      "loss": 6.7328,
      "step": 242
    },
    {
      "epoch": 1.3885714285714286,
      "grad_norm": 0.742458164691925,
      "learning_rate": 1.5373563218390803e-05,
      "loss": 6.5254,
      "step": 243
    },
    {
      "epoch": 1.3942857142857144,
      "grad_norm": 1.3324638605117798,
      "learning_rate": 1.5229885057471265e-05,
      "loss": 6.562,
      "step": 244
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.6527742147445679,
      "learning_rate": 1.5086206896551724e-05,
      "loss": 6.7194,
      "step": 245
    },
    {
      "epoch": 1.4057142857142857,
      "grad_norm": 0.6857516169548035,
      "learning_rate": 1.4942528735632185e-05,
      "loss": 6.5876,
      "step": 246
    },
    {
      "epoch": 1.4114285714285715,
      "grad_norm": 0.7583711743354797,
      "learning_rate": 1.4798850574712644e-05,
      "loss": 6.7034,
      "step": 247
    },
    {
      "epoch": 1.4171428571428573,
      "grad_norm": 0.7653309106826782,
      "learning_rate": 1.4655172413793103e-05,
      "loss": 6.5767,
      "step": 248
    },
    {
      "epoch": 1.4228571428571428,
      "grad_norm": 0.8907797932624817,
      "learning_rate": 1.4511494252873564e-05,
      "loss": 6.5344,
      "step": 249
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 0.7383517026901245,
      "learning_rate": 1.4367816091954022e-05,
      "loss": 6.5397,
      "step": 250
    },
    {
      "epoch": 1.4342857142857142,
      "grad_norm": 0.8881139755249023,
      "learning_rate": 1.4224137931034485e-05,
      "loss": 6.7979,
      "step": 251
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.6755727529525757,
      "learning_rate": 1.4080459770114942e-05,
      "loss": 6.7083,
      "step": 252
    },
    {
      "epoch": 1.4457142857142857,
      "grad_norm": 0.8101673126220703,
      "learning_rate": 1.3936781609195404e-05,
      "loss": 6.6183,
      "step": 253
    },
    {
      "epoch": 1.4514285714285715,
      "grad_norm": 0.8002943992614746,
      "learning_rate": 1.3793103448275863e-05,
      "loss": 6.5402,
      "step": 254
    },
    {
      "epoch": 1.457142857142857,
      "grad_norm": 0.8018909692764282,
      "learning_rate": 1.3649425287356324e-05,
      "loss": 6.5352,
      "step": 255
    },
    {
      "epoch": 1.4628571428571429,
      "grad_norm": 0.9602996110916138,
      "learning_rate": 1.3505747126436783e-05,
      "loss": 6.83,
      "step": 256
    },
    {
      "epoch": 1.4685714285714286,
      "grad_norm": 0.8549309968948364,
      "learning_rate": 1.336206896551724e-05,
      "loss": 6.5934,
      "step": 257
    },
    {
      "epoch": 1.4742857142857142,
      "grad_norm": 0.7166292071342468,
      "learning_rate": 1.3218390804597702e-05,
      "loss": 6.6355,
      "step": 258
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.8246389627456665,
      "learning_rate": 1.3074712643678161e-05,
      "loss": 6.5575,
      "step": 259
    },
    {
      "epoch": 1.4857142857142858,
      "grad_norm": 0.7486384510993958,
      "learning_rate": 1.2931034482758622e-05,
      "loss": 6.5588,
      "step": 260
    },
    {
      "epoch": 1.4914285714285715,
      "grad_norm": 0.654077410697937,
      "learning_rate": 1.2787356321839081e-05,
      "loss": 6.587,
      "step": 261
    },
    {
      "epoch": 1.497142857142857,
      "grad_norm": 1.2516353130340576,
      "learning_rate": 1.2643678160919542e-05,
      "loss": 6.2502,
      "step": 262
    },
    {
      "epoch": 1.502857142857143,
      "grad_norm": 0.779015839099884,
      "learning_rate": 1.25e-05,
      "loss": 6.6126,
      "step": 263
    },
    {
      "epoch": 1.5085714285714285,
      "grad_norm": 0.9600476026535034,
      "learning_rate": 1.2356321839080461e-05,
      "loss": 6.5811,
      "step": 264
    },
    {
      "epoch": 1.5142857142857142,
      "grad_norm": 1.1039371490478516,
      "learning_rate": 1.221264367816092e-05,
      "loss": 6.4534,
      "step": 265
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.7163998484611511,
      "learning_rate": 1.206896551724138e-05,
      "loss": 6.5364,
      "step": 266
    },
    {
      "epoch": 1.5257142857142858,
      "grad_norm": 0.9848192930221558,
      "learning_rate": 1.192528735632184e-05,
      "loss": 6.6023,
      "step": 267
    },
    {
      "epoch": 1.5314285714285716,
      "grad_norm": 1.2526495456695557,
      "learning_rate": 1.1781609195402299e-05,
      "loss": 6.3666,
      "step": 268
    },
    {
      "epoch": 1.5371428571428571,
      "grad_norm": 0.7879625558853149,
      "learning_rate": 1.163793103448276e-05,
      "loss": 6.4765,
      "step": 269
    },
    {
      "epoch": 1.5428571428571427,
      "grad_norm": 1.6230329275131226,
      "learning_rate": 1.1494252873563218e-05,
      "loss": 6.9144,
      "step": 270
    },
    {
      "epoch": 1.5485714285714285,
      "grad_norm": 0.7979252934455872,
      "learning_rate": 1.1350574712643679e-05,
      "loss": 6.5304,
      "step": 271
    },
    {
      "epoch": 1.5542857142857143,
      "grad_norm": 1.3966954946517944,
      "learning_rate": 1.1206896551724138e-05,
      "loss": 6.6769,
      "step": 272
    },
    {
      "epoch": 1.56,
      "grad_norm": 1.0282666683197021,
      "learning_rate": 1.1063218390804599e-05,
      "loss": 6.6859,
      "step": 273
    },
    {
      "epoch": 1.5657142857142858,
      "grad_norm": 0.9236896634101868,
      "learning_rate": 1.091954022988506e-05,
      "loss": 6.6946,
      "step": 274
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 0.9664683938026428,
      "learning_rate": 1.0775862068965516e-05,
      "loss": 6.7093,
      "step": 275
    },
    {
      "epoch": 1.5771428571428572,
      "grad_norm": 0.7333167791366577,
      "learning_rate": 1.0632183908045977e-05,
      "loss": 6.4455,
      "step": 276
    },
    {
      "epoch": 1.5828571428571427,
      "grad_norm": 0.9594359397888184,
      "learning_rate": 1.0488505747126438e-05,
      "loss": 6.4285,
      "step": 277
    },
    {
      "epoch": 1.5885714285714285,
      "grad_norm": 0.9605685472488403,
      "learning_rate": 1.0344827586206897e-05,
      "loss": 6.5305,
      "step": 278
    },
    {
      "epoch": 1.5942857142857143,
      "grad_norm": 1.0141985416412354,
      "learning_rate": 1.0201149425287357e-05,
      "loss": 6.703,
      "step": 279
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.1315340995788574,
      "learning_rate": 1.0057471264367816e-05,
      "loss": 6.4833,
      "step": 280
    },
    {
      "epoch": 1.6057142857142859,
      "grad_norm": 1.0308184623718262,
      "learning_rate": 9.913793103448277e-06,
      "loss": 6.499,
      "step": 281
    },
    {
      "epoch": 1.6114285714285714,
      "grad_norm": 0.8958332538604736,
      "learning_rate": 9.770114942528738e-06,
      "loss": 6.6497,
      "step": 282
    },
    {
      "epoch": 1.617142857142857,
      "grad_norm": 0.7080104351043701,
      "learning_rate": 9.626436781609195e-06,
      "loss": 6.6714,
      "step": 283
    },
    {
      "epoch": 1.6228571428571428,
      "grad_norm": 0.765373706817627,
      "learning_rate": 9.482758620689655e-06,
      "loss": 6.6205,
      "step": 284
    },
    {
      "epoch": 1.6285714285714286,
      "grad_norm": 1.075498342514038,
      "learning_rate": 9.339080459770114e-06,
      "loss": 6.6768,
      "step": 285
    },
    {
      "epoch": 1.6342857142857143,
      "grad_norm": 0.8733986616134644,
      "learning_rate": 9.195402298850575e-06,
      "loss": 6.8681,
      "step": 286
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.7941567897796631,
      "learning_rate": 9.051724137931036e-06,
      "loss": 6.7662,
      "step": 287
    },
    {
      "epoch": 1.6457142857142857,
      "grad_norm": 0.779097855091095,
      "learning_rate": 8.908045977011495e-06,
      "loss": 6.7257,
      "step": 288
    },
    {
      "epoch": 1.6514285714285715,
      "grad_norm": 0.9209754467010498,
      "learning_rate": 8.764367816091955e-06,
      "loss": 6.7018,
      "step": 289
    },
    {
      "epoch": 1.657142857142857,
      "grad_norm": 0.8874213695526123,
      "learning_rate": 8.620689655172414e-06,
      "loss": 6.5337,
      "step": 290
    },
    {
      "epoch": 1.6628571428571428,
      "grad_norm": 0.7731742858886719,
      "learning_rate": 8.477011494252873e-06,
      "loss": 6.603,
      "step": 291
    },
    {
      "epoch": 1.6685714285714286,
      "grad_norm": 0.7082297801971436,
      "learning_rate": 8.333333333333334e-06,
      "loss": 6.7591,
      "step": 292
    },
    {
      "epoch": 1.6742857142857144,
      "grad_norm": 0.9649941921234131,
      "learning_rate": 8.189655172413793e-06,
      "loss": 6.5948,
      "step": 293
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.6516252756118774,
      "learning_rate": 8.045977011494253e-06,
      "loss": 6.5505,
      "step": 294
    },
    {
      "epoch": 1.6857142857142857,
      "grad_norm": 0.8853458762168884,
      "learning_rate": 7.902298850574712e-06,
      "loss": 6.6639,
      "step": 295
    },
    {
      "epoch": 1.6914285714285713,
      "grad_norm": 0.9279752373695374,
      "learning_rate": 7.758620689655173e-06,
      "loss": 6.6475,
      "step": 296
    },
    {
      "epoch": 1.697142857142857,
      "grad_norm": 0.8652651906013489,
      "learning_rate": 7.614942528735633e-06,
      "loss": 6.6185,
      "step": 297
    },
    {
      "epoch": 1.7028571428571428,
      "grad_norm": 1.2576631307601929,
      "learning_rate": 7.4712643678160925e-06,
      "loss": 6.5348,
      "step": 298
    },
    {
      "epoch": 1.7085714285714286,
      "grad_norm": 0.7146162390708923,
      "learning_rate": 7.3275862068965514e-06,
      "loss": 6.7155,
      "step": 299
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 0.7662160992622375,
      "learning_rate": 7.183908045977011e-06,
      "loss": 6.7809,
      "step": 300
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.8875370025634766,
      "learning_rate": 7.040229885057471e-06,
      "loss": 6.7244,
      "step": 301
    },
    {
      "epoch": 1.7257142857142858,
      "grad_norm": 0.8403069972991943,
      "learning_rate": 6.896551724137932e-06,
      "loss": 6.5618,
      "step": 302
    },
    {
      "epoch": 1.7314285714285713,
      "grad_norm": 0.8607391119003296,
      "learning_rate": 6.7528735632183914e-06,
      "loss": 6.715,
      "step": 303
    },
    {
      "epoch": 1.737142857142857,
      "grad_norm": 0.74555903673172,
      "learning_rate": 6.609195402298851e-06,
      "loss": 6.7177,
      "step": 304
    },
    {
      "epoch": 1.7428571428571429,
      "grad_norm": 0.8607789874076843,
      "learning_rate": 6.465517241379311e-06,
      "loss": 6.5197,
      "step": 305
    },
    {
      "epoch": 1.7485714285714287,
      "grad_norm": 0.9659470915794373,
      "learning_rate": 6.321839080459771e-06,
      "loss": 6.6059,
      "step": 306
    },
    {
      "epoch": 1.7542857142857144,
      "grad_norm": 0.7810735106468201,
      "learning_rate": 6.178160919540231e-06,
      "loss": 6.6403,
      "step": 307
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.7523465752601624,
      "learning_rate": 6.03448275862069e-06,
      "loss": 6.5521,
      "step": 308
    },
    {
      "epoch": 1.7657142857142856,
      "grad_norm": 0.7821825742721558,
      "learning_rate": 5.890804597701149e-06,
      "loss": 6.6552,
      "step": 309
    },
    {
      "epoch": 1.7714285714285714,
      "grad_norm": 0.8541289567947388,
      "learning_rate": 5.747126436781609e-06,
      "loss": 6.4721,
      "step": 310
    },
    {
      "epoch": 1.7771428571428571,
      "grad_norm": 0.7165960073471069,
      "learning_rate": 5.603448275862069e-06,
      "loss": 6.6705,
      "step": 311
    },
    {
      "epoch": 1.782857142857143,
      "grad_norm": 0.8279760479927063,
      "learning_rate": 5.45977011494253e-06,
      "loss": 6.4679,
      "step": 312
    },
    {
      "epoch": 1.7885714285714287,
      "grad_norm": 0.8499174118041992,
      "learning_rate": 5.3160919540229885e-06,
      "loss": 6.7525,
      "step": 313
    },
    {
      "epoch": 1.7942857142857143,
      "grad_norm": 0.7920328378677368,
      "learning_rate": 5.172413793103448e-06,
      "loss": 6.7662,
      "step": 314
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.7852120995521545,
      "learning_rate": 5.028735632183908e-06,
      "loss": 6.7371,
      "step": 315
    },
    {
      "epoch": 1.8057142857142856,
      "grad_norm": 0.6666961908340454,
      "learning_rate": 4.885057471264369e-06,
      "loss": 6.5537,
      "step": 316
    },
    {
      "epoch": 1.8114285714285714,
      "grad_norm": 0.8363389372825623,
      "learning_rate": 4.741379310344828e-06,
      "loss": 6.8601,
      "step": 317
    },
    {
      "epoch": 1.8171428571428572,
      "grad_norm": 0.9842319488525391,
      "learning_rate": 4.5977011494252875e-06,
      "loss": 6.5693,
      "step": 318
    },
    {
      "epoch": 1.822857142857143,
      "grad_norm": 0.8966851234436035,
      "learning_rate": 4.454022988505747e-06,
      "loss": 6.7197,
      "step": 319
    },
    {
      "epoch": 1.8285714285714287,
      "grad_norm": 1.0103132724761963,
      "learning_rate": 4.310344827586207e-06,
      "loss": 6.5335,
      "step": 320
    },
    {
      "epoch": 1.8342857142857143,
      "grad_norm": 0.8865261077880859,
      "learning_rate": 4.166666666666667e-06,
      "loss": 6.5945,
      "step": 321
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1.0713624954223633,
      "learning_rate": 4.022988505747127e-06,
      "loss": 6.7458,
      "step": 322
    },
    {
      "epoch": 1.8457142857142856,
      "grad_norm": 0.8605607151985168,
      "learning_rate": 3.8793103448275865e-06,
      "loss": 6.7172,
      "step": 323
    },
    {
      "epoch": 1.8514285714285714,
      "grad_norm": 1.123935341835022,
      "learning_rate": 3.7356321839080462e-06,
      "loss": 6.5978,
      "step": 324
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 0.9971835613250732,
      "learning_rate": 3.5919540229885056e-06,
      "loss": 6.7293,
      "step": 325
    },
    {
      "epoch": 1.862857142857143,
      "grad_norm": 0.8837582468986511,
      "learning_rate": 3.448275862068966e-06,
      "loss": 6.809,
      "step": 326
    },
    {
      "epoch": 1.8685714285714285,
      "grad_norm": 0.8268870115280151,
      "learning_rate": 3.3045977011494256e-06,
      "loss": 6.4126,
      "step": 327
    },
    {
      "epoch": 1.8742857142857143,
      "grad_norm": 0.718085527420044,
      "learning_rate": 3.1609195402298854e-06,
      "loss": 6.5556,
      "step": 328
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.7681236863136292,
      "learning_rate": 3.017241379310345e-06,
      "loss": 6.7085,
      "step": 329
    },
    {
      "epoch": 1.8857142857142857,
      "grad_norm": 0.9010298252105713,
      "learning_rate": 2.8735632183908046e-06,
      "loss": 6.8492,
      "step": 330
    },
    {
      "epoch": 1.8914285714285715,
      "grad_norm": 0.8027539253234863,
      "learning_rate": 2.729885057471265e-06,
      "loss": 6.442,
      "step": 331
    },
    {
      "epoch": 1.8971428571428572,
      "grad_norm": 1.1439448595046997,
      "learning_rate": 2.586206896551724e-06,
      "loss": 6.7835,
      "step": 332
    },
    {
      "epoch": 1.9028571428571428,
      "grad_norm": 0.8006725907325745,
      "learning_rate": 2.4425287356321844e-06,
      "loss": 6.5764,
      "step": 333
    },
    {
      "epoch": 1.9085714285714286,
      "grad_norm": 0.916698694229126,
      "learning_rate": 2.2988505747126437e-06,
      "loss": 6.7126,
      "step": 334
    },
    {
      "epoch": 1.9142857142857141,
      "grad_norm": 0.7744926810264587,
      "learning_rate": 2.1551724137931035e-06,
      "loss": 6.7009,
      "step": 335
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.8285636305809021,
      "learning_rate": 2.0114942528735633e-06,
      "loss": 6.5898,
      "step": 336
    },
    {
      "epoch": 1.9257142857142857,
      "grad_norm": 0.761895477771759,
      "learning_rate": 1.8678160919540231e-06,
      "loss": 6.65,
      "step": 337
    },
    {
      "epoch": 1.9314285714285715,
      "grad_norm": 0.8070390820503235,
      "learning_rate": 1.724137931034483e-06,
      "loss": 6.713,
      "step": 338
    },
    {
      "epoch": 1.9371428571428573,
      "grad_norm": 1.229725956916809,
      "learning_rate": 1.5804597701149427e-06,
      "loss": 6.7361,
      "step": 339
    },
    {
      "epoch": 1.9428571428571428,
      "grad_norm": 0.7567991018295288,
      "learning_rate": 1.4367816091954023e-06,
      "loss": 6.6905,
      "step": 340
    },
    {
      "epoch": 1.9485714285714286,
      "grad_norm": 0.7240900993347168,
      "learning_rate": 1.293103448275862e-06,
      "loss": 6.7978,
      "step": 341
    },
    {
      "epoch": 1.9542857142857142,
      "grad_norm": 0.9182749390602112,
      "learning_rate": 1.1494252873563219e-06,
      "loss": 6.7852,
      "step": 342
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.8476371169090271,
      "learning_rate": 1.0057471264367817e-06,
      "loss": 6.6965,
      "step": 343
    },
    {
      "epoch": 1.9657142857142857,
      "grad_norm": 0.7561036348342896,
      "learning_rate": 8.620689655172415e-07,
      "loss": 6.5141,
      "step": 344
    },
    {
      "epoch": 1.9714285714285715,
      "grad_norm": 0.9580115079879761,
      "learning_rate": 7.183908045977011e-07,
      "loss": 6.9229,
      "step": 345
    },
    {
      "epoch": 1.977142857142857,
      "grad_norm": 0.8540986180305481,
      "learning_rate": 5.747126436781609e-07,
      "loss": 6.8138,
      "step": 346
    },
    {
      "epoch": 1.9828571428571429,
      "grad_norm": 0.7216700315475464,
      "learning_rate": 4.3103448275862073e-07,
      "loss": 6.7208,
      "step": 347
    },
    {
      "epoch": 1.9885714285714284,
      "grad_norm": 1.1538419723510742,
      "learning_rate": 2.8735632183908047e-07,
      "loss": 6.6003,
      "step": 348
    },
    {
      "epoch": 1.9942857142857142,
      "grad_norm": 0.844150185585022,
      "learning_rate": 1.4367816091954023e-07,
      "loss": 6.6041,
      "step": 349
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.8096569776535034,
      "learning_rate": 0.0,
      "loss": 6.6906,
      "step": 350
    }
  ],
  "logging_steps": 1,
  "max_steps": 350,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 923242070016000.0,
  "train_batch_size": 80,
  "trial_name": null,
  "trial_params": null
}
