{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 700,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005714285714285714,
      "grad_norm": 4.794899940490723,
      "learning_rate": 2.5e-05,
      "loss": 13.9076,
      "step": 1
    },
    {
      "epoch": 0.011428571428571429,
      "grad_norm": 4.723300457000732,
      "learning_rate": 5e-05,
      "loss": 14.1902,
      "step": 2
    },
    {
      "epoch": 0.017142857142857144,
      "grad_norm": 4.746112823486328,
      "learning_rate": 4.9928366762177656e-05,
      "loss": 14.0279,
      "step": 3
    },
    {
      "epoch": 0.022857142857142857,
      "grad_norm": 4.903796195983887,
      "learning_rate": 4.98567335243553e-05,
      "loss": 14.2223,
      "step": 4
    },
    {
      "epoch": 0.02857142857142857,
      "grad_norm": 5.031614303588867,
      "learning_rate": 4.9785100286532956e-05,
      "loss": 13.6126,
      "step": 5
    },
    {
      "epoch": 0.03428571428571429,
      "grad_norm": 5.19708776473999,
      "learning_rate": 4.97134670487106e-05,
      "loss": 13.797,
      "step": 6
    },
    {
      "epoch": 0.04,
      "grad_norm": 5.499693393707275,
      "learning_rate": 4.9641833810888256e-05,
      "loss": 14.0017,
      "step": 7
    },
    {
      "epoch": 0.045714285714285714,
      "grad_norm": 5.6214189529418945,
      "learning_rate": 4.957020057306591e-05,
      "loss": 13.3958,
      "step": 8
    },
    {
      "epoch": 0.05142857142857143,
      "grad_norm": 5.856240272521973,
      "learning_rate": 4.9498567335243556e-05,
      "loss": 13.6329,
      "step": 9
    },
    {
      "epoch": 0.05714285714285714,
      "grad_norm": 6.021900177001953,
      "learning_rate": 4.942693409742121e-05,
      "loss": 13.4819,
      "step": 10
    },
    {
      "epoch": 0.06285714285714286,
      "grad_norm": 6.315803527832031,
      "learning_rate": 4.9355300859598855e-05,
      "loss": 13.3275,
      "step": 11
    },
    {
      "epoch": 0.06857142857142857,
      "grad_norm": 6.930027008056641,
      "learning_rate": 4.928366762177651e-05,
      "loss": 13.4783,
      "step": 12
    },
    {
      "epoch": 0.07428571428571429,
      "grad_norm": 6.9589033126831055,
      "learning_rate": 4.9212034383954155e-05,
      "loss": 13.1557,
      "step": 13
    },
    {
      "epoch": 0.08,
      "grad_norm": 7.218785762786865,
      "learning_rate": 4.914040114613181e-05,
      "loss": 12.8283,
      "step": 14
    },
    {
      "epoch": 0.08571428571428572,
      "grad_norm": 7.288783550262451,
      "learning_rate": 4.906876790830946e-05,
      "loss": 12.6328,
      "step": 15
    },
    {
      "epoch": 0.09142857142857143,
      "grad_norm": 8.211856842041016,
      "learning_rate": 4.899713467048711e-05,
      "loss": 12.3916,
      "step": 16
    },
    {
      "epoch": 0.09714285714285714,
      "grad_norm": 7.872536659240723,
      "learning_rate": 4.892550143266476e-05,
      "loss": 12.4259,
      "step": 17
    },
    {
      "epoch": 0.10285714285714286,
      "grad_norm": 8.678061485290527,
      "learning_rate": 4.885386819484241e-05,
      "loss": 12.4418,
      "step": 18
    },
    {
      "epoch": 0.10857142857142857,
      "grad_norm": 9.039084434509277,
      "learning_rate": 4.878223495702006e-05,
      "loss": 11.9543,
      "step": 19
    },
    {
      "epoch": 0.11428571428571428,
      "grad_norm": 9.508535385131836,
      "learning_rate": 4.8710601719197715e-05,
      "loss": 11.9142,
      "step": 20
    },
    {
      "epoch": 0.12,
      "grad_norm": 9.729390144348145,
      "learning_rate": 4.863896848137536e-05,
      "loss": 11.6403,
      "step": 21
    },
    {
      "epoch": 0.12571428571428572,
      "grad_norm": 10.779572486877441,
      "learning_rate": 4.856733524355301e-05,
      "loss": 11.5932,
      "step": 22
    },
    {
      "epoch": 0.13142857142857142,
      "grad_norm": 10.485432624816895,
      "learning_rate": 4.849570200573066e-05,
      "loss": 11.2554,
      "step": 23
    },
    {
      "epoch": 0.13714285714285715,
      "grad_norm": 10.499908447265625,
      "learning_rate": 4.842406876790831e-05,
      "loss": 11.1319,
      "step": 24
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 10.632591247558594,
      "learning_rate": 4.835243553008596e-05,
      "loss": 10.7963,
      "step": 25
    },
    {
      "epoch": 0.14857142857142858,
      "grad_norm": 10.765848159790039,
      "learning_rate": 4.828080229226361e-05,
      "loss": 10.6234,
      "step": 26
    },
    {
      "epoch": 0.15428571428571428,
      "grad_norm": 10.740030288696289,
      "learning_rate": 4.820916905444126e-05,
      "loss": 10.3758,
      "step": 27
    },
    {
      "epoch": 0.16,
      "grad_norm": 10.853652000427246,
      "learning_rate": 4.8137535816618915e-05,
      "loss": 10.4139,
      "step": 28
    },
    {
      "epoch": 0.1657142857142857,
      "grad_norm": 10.217020988464355,
      "learning_rate": 4.806590257879656e-05,
      "loss": 9.914,
      "step": 29
    },
    {
      "epoch": 0.17142857142857143,
      "grad_norm": 10.311026573181152,
      "learning_rate": 4.7994269340974215e-05,
      "loss": 9.9505,
      "step": 30
    },
    {
      "epoch": 0.17714285714285713,
      "grad_norm": 9.150945663452148,
      "learning_rate": 4.792263610315186e-05,
      "loss": 9.6279,
      "step": 31
    },
    {
      "epoch": 0.18285714285714286,
      "grad_norm": 8.463910102844238,
      "learning_rate": 4.7851002865329515e-05,
      "loss": 9.3827,
      "step": 32
    },
    {
      "epoch": 0.18857142857142858,
      "grad_norm": 8.203803062438965,
      "learning_rate": 4.777936962750716e-05,
      "loss": 9.3586,
      "step": 33
    },
    {
      "epoch": 0.19428571428571428,
      "grad_norm": 7.423237323760986,
      "learning_rate": 4.7707736389684815e-05,
      "loss": 9.0476,
      "step": 34
    },
    {
      "epoch": 0.2,
      "grad_norm": 6.753605842590332,
      "learning_rate": 4.763610315186247e-05,
      "loss": 9.0588,
      "step": 35
    },
    {
      "epoch": 0.2057142857142857,
      "grad_norm": 5.861922740936279,
      "learning_rate": 4.7564469914040115e-05,
      "loss": 8.7779,
      "step": 36
    },
    {
      "epoch": 0.21142857142857144,
      "grad_norm": 4.5370941162109375,
      "learning_rate": 4.749283667621777e-05,
      "loss": 8.5255,
      "step": 37
    },
    {
      "epoch": 0.21714285714285714,
      "grad_norm": 4.596197128295898,
      "learning_rate": 4.7421203438395415e-05,
      "loss": 8.8441,
      "step": 38
    },
    {
      "epoch": 0.22285714285714286,
      "grad_norm": 3.582444190979004,
      "learning_rate": 4.734957020057307e-05,
      "loss": 8.4089,
      "step": 39
    },
    {
      "epoch": 0.22857142857142856,
      "grad_norm": 3.30299973487854,
      "learning_rate": 4.727793696275072e-05,
      "loss": 8.4016,
      "step": 40
    },
    {
      "epoch": 0.2342857142857143,
      "grad_norm": 3.163322925567627,
      "learning_rate": 4.720630372492837e-05,
      "loss": 8.3443,
      "step": 41
    },
    {
      "epoch": 0.24,
      "grad_norm": 2.915701389312744,
      "learning_rate": 4.713467048710602e-05,
      "loss": 8.4443,
      "step": 42
    },
    {
      "epoch": 0.24571428571428572,
      "grad_norm": 2.5237414836883545,
      "learning_rate": 4.706303724928367e-05,
      "loss": 8.378,
      "step": 43
    },
    {
      "epoch": 0.25142857142857145,
      "grad_norm": 2.3416688442230225,
      "learning_rate": 4.699140401146132e-05,
      "loss": 8.3474,
      "step": 44
    },
    {
      "epoch": 0.2571428571428571,
      "grad_norm": 2.3425731658935547,
      "learning_rate": 4.6919770773638974e-05,
      "loss": 8.2507,
      "step": 45
    },
    {
      "epoch": 0.26285714285714284,
      "grad_norm": 2.131606340408325,
      "learning_rate": 4.684813753581662e-05,
      "loss": 8.0888,
      "step": 46
    },
    {
      "epoch": 0.26857142857142857,
      "grad_norm": 2.0472495555877686,
      "learning_rate": 4.6776504297994274e-05,
      "loss": 8.2322,
      "step": 47
    },
    {
      "epoch": 0.2742857142857143,
      "grad_norm": 1.9451230764389038,
      "learning_rate": 4.670487106017192e-05,
      "loss": 8.0965,
      "step": 48
    },
    {
      "epoch": 0.28,
      "grad_norm": 1.987609624862671,
      "learning_rate": 4.6633237822349574e-05,
      "loss": 8.0962,
      "step": 49
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 2.0164902210235596,
      "learning_rate": 4.656160458452722e-05,
      "loss": 7.8926,
      "step": 50
    },
    {
      "epoch": 0.2914285714285714,
      "grad_norm": 1.9235872030258179,
      "learning_rate": 4.6489971346704874e-05,
      "loss": 7.979,
      "step": 51
    },
    {
      "epoch": 0.29714285714285715,
      "grad_norm": 1.9333778619766235,
      "learning_rate": 4.641833810888253e-05,
      "loss": 7.9743,
      "step": 52
    },
    {
      "epoch": 0.3028571428571429,
      "grad_norm": 1.7322421073913574,
      "learning_rate": 4.6346704871060174e-05,
      "loss": 8.01,
      "step": 53
    },
    {
      "epoch": 0.30857142857142855,
      "grad_norm": 1.69844388961792,
      "learning_rate": 4.627507163323783e-05,
      "loss": 7.842,
      "step": 54
    },
    {
      "epoch": 0.3142857142857143,
      "grad_norm": 1.637011170387268,
      "learning_rate": 4.6203438395415474e-05,
      "loss": 7.7793,
      "step": 55
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.601215124130249,
      "learning_rate": 4.613180515759313e-05,
      "loss": 7.9771,
      "step": 56
    },
    {
      "epoch": 0.32571428571428573,
      "grad_norm": 1.6545077562332153,
      "learning_rate": 4.606017191977078e-05,
      "loss": 7.885,
      "step": 57
    },
    {
      "epoch": 0.3314285714285714,
      "grad_norm": 1.630068302154541,
      "learning_rate": 4.598853868194843e-05,
      "loss": 7.9313,
      "step": 58
    },
    {
      "epoch": 0.33714285714285713,
      "grad_norm": 1.726891279220581,
      "learning_rate": 4.591690544412608e-05,
      "loss": 7.7735,
      "step": 59
    },
    {
      "epoch": 0.34285714285714286,
      "grad_norm": 1.4686059951782227,
      "learning_rate": 4.584527220630373e-05,
      "loss": 7.661,
      "step": 60
    },
    {
      "epoch": 0.3485714285714286,
      "grad_norm": 1.5229463577270508,
      "learning_rate": 4.5773638968481374e-05,
      "loss": 7.6853,
      "step": 61
    },
    {
      "epoch": 0.35428571428571426,
      "grad_norm": 1.4702353477478027,
      "learning_rate": 4.570200573065903e-05,
      "loss": 7.5159,
      "step": 62
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.6822022199630737,
      "learning_rate": 4.5630372492836674e-05,
      "loss": 7.6333,
      "step": 63
    },
    {
      "epoch": 0.3657142857142857,
      "grad_norm": 1.5622893571853638,
      "learning_rate": 4.555873925501433e-05,
      "loss": 7.5487,
      "step": 64
    },
    {
      "epoch": 0.37142857142857144,
      "grad_norm": 1.389730453491211,
      "learning_rate": 4.548710601719198e-05,
      "loss": 7.5677,
      "step": 65
    },
    {
      "epoch": 0.37714285714285717,
      "grad_norm": 1.2837084531784058,
      "learning_rate": 4.541547277936963e-05,
      "loss": 7.5409,
      "step": 66
    },
    {
      "epoch": 0.38285714285714284,
      "grad_norm": 1.4713339805603027,
      "learning_rate": 4.534383954154728e-05,
      "loss": 7.6198,
      "step": 67
    },
    {
      "epoch": 0.38857142857142857,
      "grad_norm": 1.326295256614685,
      "learning_rate": 4.527220630372493e-05,
      "loss": 7.4564,
      "step": 68
    },
    {
      "epoch": 0.3942857142857143,
      "grad_norm": 1.4008692502975464,
      "learning_rate": 4.520057306590258e-05,
      "loss": 7.3967,
      "step": 69
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.3335516452789307,
      "learning_rate": 4.512893982808023e-05,
      "loss": 7.5856,
      "step": 70
    },
    {
      "epoch": 0.4057142857142857,
      "grad_norm": 1.47054922580719,
      "learning_rate": 4.505730659025788e-05,
      "loss": 7.348,
      "step": 71
    },
    {
      "epoch": 0.4114285714285714,
      "grad_norm": 1.5316734313964844,
      "learning_rate": 4.4985673352435534e-05,
      "loss": 7.361,
      "step": 72
    },
    {
      "epoch": 0.41714285714285715,
      "grad_norm": 1.4262540340423584,
      "learning_rate": 4.491404011461318e-05,
      "loss": 7.2911,
      "step": 73
    },
    {
      "epoch": 0.4228571428571429,
      "grad_norm": 1.2611095905303955,
      "learning_rate": 4.4842406876790833e-05,
      "loss": 7.4462,
      "step": 74
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 1.4297969341278076,
      "learning_rate": 4.477077363896848e-05,
      "loss": 7.2461,
      "step": 75
    },
    {
      "epoch": 0.4342857142857143,
      "grad_norm": 1.1228480339050293,
      "learning_rate": 4.469914040114613e-05,
      "loss": 7.2901,
      "step": 76
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.3088951110839844,
      "learning_rate": 4.462750716332379e-05,
      "loss": 7.2546,
      "step": 77
    },
    {
      "epoch": 0.44571428571428573,
      "grad_norm": 1.2187793254852295,
      "learning_rate": 4.455587392550143e-05,
      "loss": 7.2242,
      "step": 78
    },
    {
      "epoch": 0.4514285714285714,
      "grad_norm": 1.2095532417297363,
      "learning_rate": 4.448424068767909e-05,
      "loss": 7.142,
      "step": 79
    },
    {
      "epoch": 0.45714285714285713,
      "grad_norm": 1.1042119264602661,
      "learning_rate": 4.441260744985673e-05,
      "loss": 7.2566,
      "step": 80
    },
    {
      "epoch": 0.46285714285714286,
      "grad_norm": 1.0748804807662964,
      "learning_rate": 4.4340974212034387e-05,
      "loss": 7.3587,
      "step": 81
    },
    {
      "epoch": 0.4685714285714286,
      "grad_norm": 1.0755345821380615,
      "learning_rate": 4.426934097421204e-05,
      "loss": 7.259,
      "step": 82
    },
    {
      "epoch": 0.4742857142857143,
      "grad_norm": 1.2544363737106323,
      "learning_rate": 4.4197707736389686e-05,
      "loss": 7.1425,
      "step": 83
    },
    {
      "epoch": 0.48,
      "grad_norm": 1.0715309381484985,
      "learning_rate": 4.412607449856734e-05,
      "loss": 7.158,
      "step": 84
    },
    {
      "epoch": 0.4857142857142857,
      "grad_norm": 0.9644688367843628,
      "learning_rate": 4.4054441260744986e-05,
      "loss": 7.0514,
      "step": 85
    },
    {
      "epoch": 0.49142857142857144,
      "grad_norm": 0.9352601170539856,
      "learning_rate": 4.398280802292264e-05,
      "loss": 7.0523,
      "step": 86
    },
    {
      "epoch": 0.49714285714285716,
      "grad_norm": 1.1635695695877075,
      "learning_rate": 4.391117478510029e-05,
      "loss": 7.0302,
      "step": 87
    },
    {
      "epoch": 0.5028571428571429,
      "grad_norm": 1.1946659088134766,
      "learning_rate": 4.383954154727794e-05,
      "loss": 6.9419,
      "step": 88
    },
    {
      "epoch": 0.5085714285714286,
      "grad_norm": 1.1479336023330688,
      "learning_rate": 4.376790830945559e-05,
      "loss": 7.2631,
      "step": 89
    },
    {
      "epoch": 0.5142857142857142,
      "grad_norm": 0.911600649356842,
      "learning_rate": 4.369627507163324e-05,
      "loss": 7.0449,
      "step": 90
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.0281305313110352,
      "learning_rate": 4.362464183381089e-05,
      "loss": 7.2579,
      "step": 91
    },
    {
      "epoch": 0.5257142857142857,
      "grad_norm": 0.9322142601013184,
      "learning_rate": 4.355300859598854e-05,
      "loss": 6.9369,
      "step": 92
    },
    {
      "epoch": 0.5314285714285715,
      "grad_norm": 0.9072622060775757,
      "learning_rate": 4.348137535816619e-05,
      "loss": 6.971,
      "step": 93
    },
    {
      "epoch": 0.5371428571428571,
      "grad_norm": 0.9329046010971069,
      "learning_rate": 4.3409742120343846e-05,
      "loss": 7.0523,
      "step": 94
    },
    {
      "epoch": 0.5428571428571428,
      "grad_norm": 0.8245787620544434,
      "learning_rate": 4.333810888252149e-05,
      "loss": 6.9394,
      "step": 95
    },
    {
      "epoch": 0.5485714285714286,
      "grad_norm": 0.8500218391418457,
      "learning_rate": 4.3266475644699146e-05,
      "loss": 6.9273,
      "step": 96
    },
    {
      "epoch": 0.5542857142857143,
      "grad_norm": 0.924957811832428,
      "learning_rate": 4.319484240687679e-05,
      "loss": 7.0148,
      "step": 97
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.1296454668045044,
      "learning_rate": 4.3123209169054446e-05,
      "loss": 7.206,
      "step": 98
    },
    {
      "epoch": 0.5657142857142857,
      "grad_norm": 0.857323408126831,
      "learning_rate": 4.305157593123209e-05,
      "loss": 6.9789,
      "step": 99
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.9510228633880615,
      "learning_rate": 4.297994269340974e-05,
      "loss": 7.14,
      "step": 100
    },
    {
      "epoch": 0.5771428571428572,
      "grad_norm": 0.8157414197921753,
      "learning_rate": 4.290830945558739e-05,
      "loss": 6.9968,
      "step": 101
    },
    {
      "epoch": 0.5828571428571429,
      "grad_norm": 1.1925296783447266,
      "learning_rate": 4.2836676217765046e-05,
      "loss": 7.1453,
      "step": 102
    },
    {
      "epoch": 0.5885714285714285,
      "grad_norm": 0.6761215925216675,
      "learning_rate": 4.276504297994269e-05,
      "loss": 6.9038,
      "step": 103
    },
    {
      "epoch": 0.5942857142857143,
      "grad_norm": 0.8489179611206055,
      "learning_rate": 4.2693409742120346e-05,
      "loss": 7.0224,
      "step": 104
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.9423996806144714,
      "learning_rate": 4.262177650429799e-05,
      "loss": 6.9464,
      "step": 105
    },
    {
      "epoch": 0.6057142857142858,
      "grad_norm": 1.1738520860671997,
      "learning_rate": 4.2550143266475646e-05,
      "loss": 6.7519,
      "step": 106
    },
    {
      "epoch": 0.6114285714285714,
      "grad_norm": 0.7213422656059265,
      "learning_rate": 4.247851002865329e-05,
      "loss": 6.9031,
      "step": 107
    },
    {
      "epoch": 0.6171428571428571,
      "grad_norm": 0.7779160141944885,
      "learning_rate": 4.2406876790830946e-05,
      "loss": 6.8868,
      "step": 108
    },
    {
      "epoch": 0.6228571428571429,
      "grad_norm": 0.6801261901855469,
      "learning_rate": 4.23352435530086e-05,
      "loss": 6.983,
      "step": 109
    },
    {
      "epoch": 0.6285714285714286,
      "grad_norm": 0.7271950244903564,
      "learning_rate": 4.2263610315186246e-05,
      "loss": 6.851,
      "step": 110
    },
    {
      "epoch": 0.6342857142857142,
      "grad_norm": 0.7313417196273804,
      "learning_rate": 4.21919770773639e-05,
      "loss": 6.8024,
      "step": 111
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.7262089252471924,
      "learning_rate": 4.2120343839541545e-05,
      "loss": 6.803,
      "step": 112
    },
    {
      "epoch": 0.6457142857142857,
      "grad_norm": 0.7670782804489136,
      "learning_rate": 4.20487106017192e-05,
      "loss": 6.7517,
      "step": 113
    },
    {
      "epoch": 0.6514285714285715,
      "grad_norm": 0.6898292899131775,
      "learning_rate": 4.197707736389685e-05,
      "loss": 6.8462,
      "step": 114
    },
    {
      "epoch": 0.6571428571428571,
      "grad_norm": 0.8018458485603333,
      "learning_rate": 4.19054441260745e-05,
      "loss": 6.8894,
      "step": 115
    },
    {
      "epoch": 0.6628571428571428,
      "grad_norm": 0.6571446061134338,
      "learning_rate": 4.183381088825215e-05,
      "loss": 6.7198,
      "step": 116
    },
    {
      "epoch": 0.6685714285714286,
      "grad_norm": 0.6533772945404053,
      "learning_rate": 4.17621776504298e-05,
      "loss": 6.7312,
      "step": 117
    },
    {
      "epoch": 0.6742857142857143,
      "grad_norm": 0.7766691446304321,
      "learning_rate": 4.169054441260745e-05,
      "loss": 6.6744,
      "step": 118
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.9710860252380371,
      "learning_rate": 4.1618911174785105e-05,
      "loss": 6.9194,
      "step": 119
    },
    {
      "epoch": 0.6857142857142857,
      "grad_norm": 0.6883358359336853,
      "learning_rate": 4.154727793696275e-05,
      "loss": 6.6232,
      "step": 120
    },
    {
      "epoch": 0.6914285714285714,
      "grad_norm": 0.7517836093902588,
      "learning_rate": 4.1475644699140405e-05,
      "loss": 6.7343,
      "step": 121
    },
    {
      "epoch": 0.6971428571428572,
      "grad_norm": 1.1593605279922485,
      "learning_rate": 4.140401146131805e-05,
      "loss": 7.1081,
      "step": 122
    },
    {
      "epoch": 0.7028571428571428,
      "grad_norm": 1.1176416873931885,
      "learning_rate": 4.1332378223495705e-05,
      "loss": 7.0675,
      "step": 123
    },
    {
      "epoch": 0.7085714285714285,
      "grad_norm": 0.9218679070472717,
      "learning_rate": 4.126074498567336e-05,
      "loss": 6.8409,
      "step": 124
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 0.635255753993988,
      "learning_rate": 4.1189111747851005e-05,
      "loss": 6.7664,
      "step": 125
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.6927530169487,
      "learning_rate": 4.111747851002866e-05,
      "loss": 6.7322,
      "step": 126
    },
    {
      "epoch": 0.7257142857142858,
      "grad_norm": 0.5889565348625183,
      "learning_rate": 4.1045845272206305e-05,
      "loss": 6.7569,
      "step": 127
    },
    {
      "epoch": 0.7314285714285714,
      "grad_norm": 0.8360705971717834,
      "learning_rate": 4.097421203438396e-05,
      "loss": 6.6195,
      "step": 128
    },
    {
      "epoch": 0.7371428571428571,
      "grad_norm": 1.1977626085281372,
      "learning_rate": 4.0902578796561605e-05,
      "loss": 6.5884,
      "step": 129
    },
    {
      "epoch": 0.7428571428571429,
      "grad_norm": 0.8061625957489014,
      "learning_rate": 4.083094555873926e-05,
      "loss": 6.6629,
      "step": 130
    },
    {
      "epoch": 0.7485714285714286,
      "grad_norm": 0.6947154998779297,
      "learning_rate": 4.075931232091691e-05,
      "loss": 6.9324,
      "step": 131
    },
    {
      "epoch": 0.7542857142857143,
      "grad_norm": 0.9913718700408936,
      "learning_rate": 4.068767908309456e-05,
      "loss": 6.7544,
      "step": 132
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.82943195104599,
      "learning_rate": 4.061604584527221e-05,
      "loss": 6.9239,
      "step": 133
    },
    {
      "epoch": 0.7657142857142857,
      "grad_norm": 0.7281494736671448,
      "learning_rate": 4.054441260744986e-05,
      "loss": 6.8097,
      "step": 134
    },
    {
      "epoch": 0.7714285714285715,
      "grad_norm": 0.6551270484924316,
      "learning_rate": 4.047277936962751e-05,
      "loss": 6.7467,
      "step": 135
    },
    {
      "epoch": 0.7771428571428571,
      "grad_norm": 0.6949110627174377,
      "learning_rate": 4.0401146131805165e-05,
      "loss": 6.6645,
      "step": 136
    },
    {
      "epoch": 0.7828571428571428,
      "grad_norm": 0.6336190700531006,
      "learning_rate": 4.032951289398281e-05,
      "loss": 6.7356,
      "step": 137
    },
    {
      "epoch": 0.7885714285714286,
      "grad_norm": 0.8758939504623413,
      "learning_rate": 4.025787965616046e-05,
      "loss": 6.7952,
      "step": 138
    },
    {
      "epoch": 0.7942857142857143,
      "grad_norm": 0.6717044711112976,
      "learning_rate": 4.018624641833811e-05,
      "loss": 6.7455,
      "step": 139
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.8366928696632385,
      "learning_rate": 4.011461318051576e-05,
      "loss": 6.7095,
      "step": 140
    },
    {
      "epoch": 0.8057142857142857,
      "grad_norm": 0.7992826104164124,
      "learning_rate": 4.004297994269341e-05,
      "loss": 6.6936,
      "step": 141
    },
    {
      "epoch": 0.8114285714285714,
      "grad_norm": 1.1432933807373047,
      "learning_rate": 3.997134670487106e-05,
      "loss": 6.8477,
      "step": 142
    },
    {
      "epoch": 0.8171428571428572,
      "grad_norm": 1.2263357639312744,
      "learning_rate": 3.989971346704871e-05,
      "loss": 6.7322,
      "step": 143
    },
    {
      "epoch": 0.8228571428571428,
      "grad_norm": 0.7938597202301025,
      "learning_rate": 3.982808022922636e-05,
      "loss": 6.8729,
      "step": 144
    },
    {
      "epoch": 0.8285714285714286,
      "grad_norm": 0.6682496070861816,
      "learning_rate": 3.975644699140401e-05,
      "loss": 6.8668,
      "step": 145
    },
    {
      "epoch": 0.8342857142857143,
      "grad_norm": 0.6647156476974487,
      "learning_rate": 3.9684813753581664e-05,
      "loss": 6.7241,
      "step": 146
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.8188453316688538,
      "learning_rate": 3.961318051575931e-05,
      "loss": 6.709,
      "step": 147
    },
    {
      "epoch": 0.8457142857142858,
      "grad_norm": 0.8193467259407043,
      "learning_rate": 3.9541547277936964e-05,
      "loss": 6.5984,
      "step": 148
    },
    {
      "epoch": 0.8514285714285714,
      "grad_norm": 1.0162949562072754,
      "learning_rate": 3.946991404011461e-05,
      "loss": 6.5654,
      "step": 149
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 0.7129755616188049,
      "learning_rate": 3.9398280802292264e-05,
      "loss": 6.8118,
      "step": 150
    },
    {
      "epoch": 0.8628571428571429,
      "grad_norm": 0.8392246961593628,
      "learning_rate": 3.932664756446992e-05,
      "loss": 6.6409,
      "step": 151
    },
    {
      "epoch": 0.8685714285714285,
      "grad_norm": 0.7728039026260376,
      "learning_rate": 3.9255014326647564e-05,
      "loss": 6.5935,
      "step": 152
    },
    {
      "epoch": 0.8742857142857143,
      "grad_norm": 0.7010040879249573,
      "learning_rate": 3.918338108882522e-05,
      "loss": 6.7357,
      "step": 153
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.6058781147003174,
      "learning_rate": 3.9111747851002864e-05,
      "loss": 6.6521,
      "step": 154
    },
    {
      "epoch": 0.8857142857142857,
      "grad_norm": 0.6386616230010986,
      "learning_rate": 3.904011461318052e-05,
      "loss": 6.6986,
      "step": 155
    },
    {
      "epoch": 0.8914285714285715,
      "grad_norm": 0.6544603109359741,
      "learning_rate": 3.896848137535817e-05,
      "loss": 6.6551,
      "step": 156
    },
    {
      "epoch": 0.8971428571428571,
      "grad_norm": 0.7378789782524109,
      "learning_rate": 3.889684813753582e-05,
      "loss": 6.8227,
      "step": 157
    },
    {
      "epoch": 0.9028571428571428,
      "grad_norm": 0.6785796284675598,
      "learning_rate": 3.882521489971347e-05,
      "loss": 6.7098,
      "step": 158
    },
    {
      "epoch": 0.9085714285714286,
      "grad_norm": 0.770007312297821,
      "learning_rate": 3.875358166189112e-05,
      "loss": 6.6767,
      "step": 159
    },
    {
      "epoch": 0.9142857142857143,
      "grad_norm": 1.0856993198394775,
      "learning_rate": 3.868194842406877e-05,
      "loss": 6.711,
      "step": 160
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.9380398988723755,
      "learning_rate": 3.8610315186246424e-05,
      "loss": 6.8292,
      "step": 161
    },
    {
      "epoch": 0.9257142857142857,
      "grad_norm": 1.338302731513977,
      "learning_rate": 3.853868194842407e-05,
      "loss": 6.9062,
      "step": 162
    },
    {
      "epoch": 0.9314285714285714,
      "grad_norm": 1.2307077646255493,
      "learning_rate": 3.8467048710601724e-05,
      "loss": 6.999,
      "step": 163
    },
    {
      "epoch": 0.9371428571428572,
      "grad_norm": 0.7361689805984497,
      "learning_rate": 3.839541547277937e-05,
      "loss": 6.5341,
      "step": 164
    },
    {
      "epoch": 0.9428571428571428,
      "grad_norm": 0.7598300576210022,
      "learning_rate": 3.8323782234957024e-05,
      "loss": 6.767,
      "step": 165
    },
    {
      "epoch": 0.9485714285714286,
      "grad_norm": 0.9014002680778503,
      "learning_rate": 3.825214899713467e-05,
      "loss": 6.6574,
      "step": 166
    },
    {
      "epoch": 0.9542857142857143,
      "grad_norm": 0.6660943627357483,
      "learning_rate": 3.8180515759312324e-05,
      "loss": 6.8013,
      "step": 167
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.7036223411560059,
      "learning_rate": 3.810888252148998e-05,
      "loss": 6.7252,
      "step": 168
    },
    {
      "epoch": 0.9657142857142857,
      "grad_norm": 0.9266822338104248,
      "learning_rate": 3.8037249283667624e-05,
      "loss": 6.8235,
      "step": 169
    },
    {
      "epoch": 0.9714285714285714,
      "grad_norm": 0.7779690027236938,
      "learning_rate": 3.796561604584528e-05,
      "loss": 6.8329,
      "step": 170
    },
    {
      "epoch": 0.9771428571428571,
      "grad_norm": 0.7245067358016968,
      "learning_rate": 3.7893982808022924e-05,
      "loss": 6.7486,
      "step": 171
    },
    {
      "epoch": 0.9828571428571429,
      "grad_norm": 0.6640521883964539,
      "learning_rate": 3.782234957020058e-05,
      "loss": 6.7657,
      "step": 172
    },
    {
      "epoch": 0.9885714285714285,
      "grad_norm": 0.7746831774711609,
      "learning_rate": 3.775071633237823e-05,
      "loss": 6.7777,
      "step": 173
    },
    {
      "epoch": 0.9942857142857143,
      "grad_norm": 0.7739783525466919,
      "learning_rate": 3.767908309455588e-05,
      "loss": 6.705,
      "step": 174
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7937064170837402,
      "learning_rate": 3.760744985673353e-05,
      "loss": 6.7244,
      "step": 175
    },
    {
      "epoch": 1.0057142857142858,
      "grad_norm": 0.7327736020088196,
      "learning_rate": 3.753581661891118e-05,
      "loss": 6.7699,
      "step": 176
    },
    {
      "epoch": 1.0114285714285713,
      "grad_norm": 0.8504862189292908,
      "learning_rate": 3.7464183381088823e-05,
      "loss": 6.7353,
      "step": 177
    },
    {
      "epoch": 1.0171428571428571,
      "grad_norm": 0.9770626425743103,
      "learning_rate": 3.739255014326648e-05,
      "loss": 6.6595,
      "step": 178
    },
    {
      "epoch": 1.022857142857143,
      "grad_norm": 0.7230507135391235,
      "learning_rate": 3.732091690544412e-05,
      "loss": 6.7226,
      "step": 179
    },
    {
      "epoch": 1.0285714285714285,
      "grad_norm": 0.8387154936790466,
      "learning_rate": 3.724928366762178e-05,
      "loss": 6.6953,
      "step": 180
    },
    {
      "epoch": 1.0342857142857143,
      "grad_norm": 0.8565619587898254,
      "learning_rate": 3.717765042979942e-05,
      "loss": 6.5274,
      "step": 181
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.6719664335250854,
      "learning_rate": 3.7106017191977077e-05,
      "loss": 6.8345,
      "step": 182
    },
    {
      "epoch": 1.0457142857142858,
      "grad_norm": 0.8839787840843201,
      "learning_rate": 3.703438395415473e-05,
      "loss": 6.7248,
      "step": 183
    },
    {
      "epoch": 1.0514285714285714,
      "grad_norm": 0.8005576133728027,
      "learning_rate": 3.6962750716332376e-05,
      "loss": 6.7302,
      "step": 184
    },
    {
      "epoch": 1.0571428571428572,
      "grad_norm": 1.216097116470337,
      "learning_rate": 3.689111747851003e-05,
      "loss": 6.8466,
      "step": 185
    },
    {
      "epoch": 1.062857142857143,
      "grad_norm": 1.103626012802124,
      "learning_rate": 3.6819484240687676e-05,
      "loss": 6.8261,
      "step": 186
    },
    {
      "epoch": 1.0685714285714285,
      "grad_norm": 0.9482969641685486,
      "learning_rate": 3.674785100286533e-05,
      "loss": 6.8243,
      "step": 187
    },
    {
      "epoch": 1.0742857142857143,
      "grad_norm": 0.7536084651947021,
      "learning_rate": 3.667621776504298e-05,
      "loss": 6.6934,
      "step": 188
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.8587086796760559,
      "learning_rate": 3.660458452722063e-05,
      "loss": 6.4772,
      "step": 189
    },
    {
      "epoch": 1.0857142857142856,
      "grad_norm": 0.7661059498786926,
      "learning_rate": 3.653295128939828e-05,
      "loss": 6.7632,
      "step": 190
    },
    {
      "epoch": 1.0914285714285714,
      "grad_norm": 0.7390215396881104,
      "learning_rate": 3.646131805157593e-05,
      "loss": 6.67,
      "step": 191
    },
    {
      "epoch": 1.0971428571428572,
      "grad_norm": 1.0895437002182007,
      "learning_rate": 3.638968481375358e-05,
      "loss": 6.858,
      "step": 192
    },
    {
      "epoch": 1.1028571428571428,
      "grad_norm": 0.8561964631080627,
      "learning_rate": 3.6318051575931236e-05,
      "loss": 6.5279,
      "step": 193
    },
    {
      "epoch": 1.1085714285714285,
      "grad_norm": 1.0015778541564941,
      "learning_rate": 3.624641833810888e-05,
      "loss": 6.7856,
      "step": 194
    },
    {
      "epoch": 1.1142857142857143,
      "grad_norm": 0.7471398711204529,
      "learning_rate": 3.6174785100286536e-05,
      "loss": 6.8753,
      "step": 195
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.1471893787384033,
      "learning_rate": 3.610315186246418e-05,
      "loss": 6.7041,
      "step": 196
    },
    {
      "epoch": 1.1257142857142857,
      "grad_norm": 0.8353062868118286,
      "learning_rate": 3.6031518624641836e-05,
      "loss": 6.7767,
      "step": 197
    },
    {
      "epoch": 1.1314285714285715,
      "grad_norm": 0.6264119744300842,
      "learning_rate": 3.595988538681949e-05,
      "loss": 6.6883,
      "step": 198
    },
    {
      "epoch": 1.1371428571428572,
      "grad_norm": 1.458296775817871,
      "learning_rate": 3.5888252148997136e-05,
      "loss": 6.6575,
      "step": 199
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 0.8273422122001648,
      "learning_rate": 3.581661891117479e-05,
      "loss": 6.8082,
      "step": 200
    },
    {
      "epoch": 1.1485714285714286,
      "grad_norm": 1.2838729619979858,
      "learning_rate": 3.5744985673352436e-05,
      "loss": 6.6563,
      "step": 201
    },
    {
      "epoch": 1.1542857142857144,
      "grad_norm": 1.097174882888794,
      "learning_rate": 3.567335243553009e-05,
      "loss": 6.5881,
      "step": 202
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.7056498527526855,
      "learning_rate": 3.5601719197707736e-05,
      "loss": 6.7784,
      "step": 203
    },
    {
      "epoch": 1.1657142857142857,
      "grad_norm": 0.9900858402252197,
      "learning_rate": 3.553008595988539e-05,
      "loss": 6.5204,
      "step": 204
    },
    {
      "epoch": 1.1714285714285715,
      "grad_norm": 0.8502009510993958,
      "learning_rate": 3.545845272206304e-05,
      "loss": 6.7989,
      "step": 205
    },
    {
      "epoch": 1.177142857142857,
      "grad_norm": 1.1370688676834106,
      "learning_rate": 3.538681948424069e-05,
      "loss": 6.7664,
      "step": 206
    },
    {
      "epoch": 1.1828571428571428,
      "grad_norm": 1.0425004959106445,
      "learning_rate": 3.531518624641834e-05,
      "loss": 6.8298,
      "step": 207
    },
    {
      "epoch": 1.1885714285714286,
      "grad_norm": 0.7937297821044922,
      "learning_rate": 3.524355300859599e-05,
      "loss": 6.4667,
      "step": 208
    },
    {
      "epoch": 1.1942857142857144,
      "grad_norm": 0.9716846346855164,
      "learning_rate": 3.517191977077364e-05,
      "loss": 6.5401,
      "step": 209
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.3434598445892334,
      "learning_rate": 3.5100286532951296e-05,
      "loss": 6.6875,
      "step": 210
    },
    {
      "epoch": 1.2057142857142857,
      "grad_norm": 1.0039243698120117,
      "learning_rate": 3.502865329512894e-05,
      "loss": 6.7008,
      "step": 211
    },
    {
      "epoch": 1.2114285714285715,
      "grad_norm": 1.0828431844711304,
      "learning_rate": 3.4957020057306596e-05,
      "loss": 6.4148,
      "step": 212
    },
    {
      "epoch": 1.217142857142857,
      "grad_norm": 1.3193392753601074,
      "learning_rate": 3.488538681948424e-05,
      "loss": 6.9416,
      "step": 213
    },
    {
      "epoch": 1.2228571428571429,
      "grad_norm": 0.9514572620391846,
      "learning_rate": 3.4813753581661896e-05,
      "loss": 6.6016,
      "step": 214
    },
    {
      "epoch": 1.2285714285714286,
      "grad_norm": 1.0624730587005615,
      "learning_rate": 3.474212034383955e-05,
      "loss": 6.5641,
      "step": 215
    },
    {
      "epoch": 1.2342857142857142,
      "grad_norm": 0.6791089177131653,
      "learning_rate": 3.467048710601719e-05,
      "loss": 6.5973,
      "step": 216
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.870177984237671,
      "learning_rate": 3.459885386819484e-05,
      "loss": 6.3275,
      "step": 217
    },
    {
      "epoch": 1.2457142857142858,
      "grad_norm": 0.9166701436042786,
      "learning_rate": 3.4527220630372495e-05,
      "loss": 6.7429,
      "step": 218
    },
    {
      "epoch": 1.2514285714285713,
      "grad_norm": 0.967290997505188,
      "learning_rate": 3.445558739255014e-05,
      "loss": 6.443,
      "step": 219
    },
    {
      "epoch": 1.2571428571428571,
      "grad_norm": 1.550162434577942,
      "learning_rate": 3.4383954154727795e-05,
      "loss": 6.4204,
      "step": 220
    },
    {
      "epoch": 1.262857142857143,
      "grad_norm": 1.1082810163497925,
      "learning_rate": 3.431232091690544e-05,
      "loss": 6.6017,
      "step": 221
    },
    {
      "epoch": 1.2685714285714287,
      "grad_norm": 1.347939133644104,
      "learning_rate": 3.4240687679083095e-05,
      "loss": 6.3817,
      "step": 222
    },
    {
      "epoch": 1.2742857142857142,
      "grad_norm": 0.6972644329071045,
      "learning_rate": 3.416905444126074e-05,
      "loss": 6.5771,
      "step": 223
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.8691173195838928,
      "learning_rate": 3.4097421203438395e-05,
      "loss": 6.5143,
      "step": 224
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 1.8002666234970093,
      "learning_rate": 3.402578796561605e-05,
      "loss": 6.8206,
      "step": 225
    },
    {
      "epoch": 1.2914285714285714,
      "grad_norm": 1.2292985916137695,
      "learning_rate": 3.3954154727793695e-05,
      "loss": 6.6166,
      "step": 226
    },
    {
      "epoch": 1.2971428571428572,
      "grad_norm": 1.4674359560012817,
      "learning_rate": 3.388252148997135e-05,
      "loss": 6.5492,
      "step": 227
    },
    {
      "epoch": 1.302857142857143,
      "grad_norm": 0.9398738741874695,
      "learning_rate": 3.3810888252148995e-05,
      "loss": 6.7388,
      "step": 228
    },
    {
      "epoch": 1.3085714285714285,
      "grad_norm": 0.9534319043159485,
      "learning_rate": 3.373925501432665e-05,
      "loss": 6.5784,
      "step": 229
    },
    {
      "epoch": 1.3142857142857143,
      "grad_norm": 1.0210641622543335,
      "learning_rate": 3.36676217765043e-05,
      "loss": 6.6753,
      "step": 230
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.6510745882987976,
      "learning_rate": 3.359598853868195e-05,
      "loss": 6.7152,
      "step": 231
    },
    {
      "epoch": 1.3257142857142856,
      "grad_norm": 0.8647696375846863,
      "learning_rate": 3.35243553008596e-05,
      "loss": 6.6671,
      "step": 232
    },
    {
      "epoch": 1.3314285714285714,
      "grad_norm": 0.83360356092453,
      "learning_rate": 3.345272206303725e-05,
      "loss": 6.4629,
      "step": 233
    },
    {
      "epoch": 1.3371428571428572,
      "grad_norm": 0.8243712782859802,
      "learning_rate": 3.33810888252149e-05,
      "loss": 6.7452,
      "step": 234
    },
    {
      "epoch": 1.342857142857143,
      "grad_norm": 0.6787171959877014,
      "learning_rate": 3.3309455587392555e-05,
      "loss": 6.6699,
      "step": 235
    },
    {
      "epoch": 1.3485714285714285,
      "grad_norm": 1.1057791709899902,
      "learning_rate": 3.32378223495702e-05,
      "loss": 6.5376,
      "step": 236
    },
    {
      "epoch": 1.3542857142857143,
      "grad_norm": 2.292498826980591,
      "learning_rate": 3.3166189111747855e-05,
      "loss": 6.3804,
      "step": 237
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.8989930152893066,
      "learning_rate": 3.30945558739255e-05,
      "loss": 6.619,
      "step": 238
    },
    {
      "epoch": 1.3657142857142857,
      "grad_norm": 0.7358668446540833,
      "learning_rate": 3.3022922636103155e-05,
      "loss": 6.72,
      "step": 239
    },
    {
      "epoch": 1.3714285714285714,
      "grad_norm": 0.6786149144172668,
      "learning_rate": 3.29512893982808e-05,
      "loss": 6.5625,
      "step": 240
    },
    {
      "epoch": 1.3771428571428572,
      "grad_norm": 0.6907031536102295,
      "learning_rate": 3.2879656160458455e-05,
      "loss": 6.5674,
      "step": 241
    },
    {
      "epoch": 1.3828571428571428,
      "grad_norm": 1.1233488321304321,
      "learning_rate": 3.280802292263611e-05,
      "loss": 6.6668,
      "step": 242
    },
    {
      "epoch": 1.3885714285714286,
      "grad_norm": 0.8414978384971619,
      "learning_rate": 3.2736389684813755e-05,
      "loss": 6.4425,
      "step": 243
    },
    {
      "epoch": 1.3942857142857144,
      "grad_norm": 1.3753366470336914,
      "learning_rate": 3.266475644699141e-05,
      "loss": 6.4768,
      "step": 244
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.3603445291519165,
      "learning_rate": 3.2593123209169055e-05,
      "loss": 6.6715,
      "step": 245
    },
    {
      "epoch": 1.4057142857142857,
      "grad_norm": 0.8640609383583069,
      "learning_rate": 3.252148997134671e-05,
      "loss": 6.5188,
      "step": 246
    },
    {
      "epoch": 1.4114285714285715,
      "grad_norm": 0.9144181609153748,
      "learning_rate": 3.244985673352436e-05,
      "loss": 6.6207,
      "step": 247
    },
    {
      "epoch": 1.4171428571428573,
      "grad_norm": 0.9102405309677124,
      "learning_rate": 3.237822349570201e-05,
      "loss": 6.4996,
      "step": 248
    },
    {
      "epoch": 1.4228571428571428,
      "grad_norm": 0.8233470320701599,
      "learning_rate": 3.230659025787966e-05,
      "loss": 6.4376,
      "step": 249
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 0.7259082198143005,
      "learning_rate": 3.223495702005731e-05,
      "loss": 6.4718,
      "step": 250
    },
    {
      "epoch": 1.4342857142857142,
      "grad_norm": 1.0148977041244507,
      "learning_rate": 3.216332378223496e-05,
      "loss": 6.7172,
      "step": 251
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.7648569345474243,
      "learning_rate": 3.2091690544412614e-05,
      "loss": 6.6413,
      "step": 252
    },
    {
      "epoch": 1.4457142857142857,
      "grad_norm": 0.9071716070175171,
      "learning_rate": 3.202005730659026e-05,
      "loss": 6.5594,
      "step": 253
    },
    {
      "epoch": 1.4514285714285715,
      "grad_norm": 0.8839707970619202,
      "learning_rate": 3.1948424068767914e-05,
      "loss": 6.4578,
      "step": 254
    },
    {
      "epoch": 1.457142857142857,
      "grad_norm": 1.0471340417861938,
      "learning_rate": 3.187679083094556e-05,
      "loss": 6.4712,
      "step": 255
    },
    {
      "epoch": 1.4628571428571429,
      "grad_norm": 0.8617306351661682,
      "learning_rate": 3.180515759312321e-05,
      "loss": 6.7429,
      "step": 256
    },
    {
      "epoch": 1.4685714285714286,
      "grad_norm": 1.0427448749542236,
      "learning_rate": 3.173352435530086e-05,
      "loss": 6.5237,
      "step": 257
    },
    {
      "epoch": 1.4742857142857142,
      "grad_norm": 0.8106498718261719,
      "learning_rate": 3.166189111747851e-05,
      "loss": 6.5647,
      "step": 258
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.9351671934127808,
      "learning_rate": 3.159025787965616e-05,
      "loss": 6.4883,
      "step": 259
    },
    {
      "epoch": 1.4857142857142858,
      "grad_norm": 0.8486219048500061,
      "learning_rate": 3.151862464183381e-05,
      "loss": 6.4719,
      "step": 260
    },
    {
      "epoch": 1.4914285714285715,
      "grad_norm": 0.813132643699646,
      "learning_rate": 3.144699140401146e-05,
      "loss": 6.5162,
      "step": 261
    },
    {
      "epoch": 1.497142857142857,
      "grad_norm": 0.7789728045463562,
      "learning_rate": 3.1375358166189114e-05,
      "loss": 6.165,
      "step": 262
    },
    {
      "epoch": 1.502857142857143,
      "grad_norm": 1.0379365682601929,
      "learning_rate": 3.130372492836676e-05,
      "loss": 6.5239,
      "step": 263
    },
    {
      "epoch": 1.5085714285714285,
      "grad_norm": 1.2551512718200684,
      "learning_rate": 3.1232091690544414e-05,
      "loss": 6.4997,
      "step": 264
    },
    {
      "epoch": 1.5142857142857142,
      "grad_norm": 1.4039740562438965,
      "learning_rate": 3.116045845272206e-05,
      "loss": 6.3691,
      "step": 265
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.6918138265609741,
      "learning_rate": 3.1088825214899714e-05,
      "loss": 6.4396,
      "step": 266
    },
    {
      "epoch": 1.5257142857142858,
      "grad_norm": 1.1954253911972046,
      "learning_rate": 3.101719197707737e-05,
      "loss": 6.538,
      "step": 267
    },
    {
      "epoch": 1.5314285714285716,
      "grad_norm": 1.5152605772018433,
      "learning_rate": 3.0945558739255014e-05,
      "loss": 6.2915,
      "step": 268
    },
    {
      "epoch": 1.5371428571428571,
      "grad_norm": 0.6900925040245056,
      "learning_rate": 3.087392550143267e-05,
      "loss": 6.3871,
      "step": 269
    },
    {
      "epoch": 1.5428571428571427,
      "grad_norm": 1.3247551918029785,
      "learning_rate": 3.0802292263610314e-05,
      "loss": 6.8319,
      "step": 270
    },
    {
      "epoch": 1.5485714285714285,
      "grad_norm": 1.0515458583831787,
      "learning_rate": 3.073065902578797e-05,
      "loss": 6.4475,
      "step": 271
    },
    {
      "epoch": 1.5542857142857143,
      "grad_norm": 1.2790125608444214,
      "learning_rate": 3.065902578796562e-05,
      "loss": 6.5918,
      "step": 272
    },
    {
      "epoch": 1.56,
      "grad_norm": 1.2568262815475464,
      "learning_rate": 3.058739255014327e-05,
      "loss": 6.6171,
      "step": 273
    },
    {
      "epoch": 1.5657142857142858,
      "grad_norm": 1.0677517652511597,
      "learning_rate": 3.0515759312320917e-05,
      "loss": 6.6128,
      "step": 274
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 0.9348042607307434,
      "learning_rate": 3.044412607449857e-05,
      "loss": 6.6205,
      "step": 275
    },
    {
      "epoch": 1.5771428571428572,
      "grad_norm": 0.9195274710655212,
      "learning_rate": 3.037249283667622e-05,
      "loss": 6.3747,
      "step": 276
    },
    {
      "epoch": 1.5828571428571427,
      "grad_norm": 1.163893699645996,
      "learning_rate": 3.030085959885387e-05,
      "loss": 6.3452,
      "step": 277
    },
    {
      "epoch": 1.5885714285714285,
      "grad_norm": 1.0591554641723633,
      "learning_rate": 3.022922636103152e-05,
      "loss": 6.455,
      "step": 278
    },
    {
      "epoch": 1.5942857142857143,
      "grad_norm": 1.2623568773269653,
      "learning_rate": 3.015759312320917e-05,
      "loss": 6.6384,
      "step": 279
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.053097128868103,
      "learning_rate": 3.0085959885386824e-05,
      "loss": 6.3753,
      "step": 280
    },
    {
      "epoch": 1.6057142857142859,
      "grad_norm": 0.9581981301307678,
      "learning_rate": 3.0014326647564473e-05,
      "loss": 6.4176,
      "step": 281
    },
    {
      "epoch": 1.6114285714285714,
      "grad_norm": 1.0711748600006104,
      "learning_rate": 2.9942693409742123e-05,
      "loss": 6.5717,
      "step": 282
    },
    {
      "epoch": 1.617142857142857,
      "grad_norm": 1.0422316789627075,
      "learning_rate": 2.9871060171919773e-05,
      "loss": 6.5998,
      "step": 283
    },
    {
      "epoch": 1.6228571428571428,
      "grad_norm": 0.8362864255905151,
      "learning_rate": 2.9799426934097423e-05,
      "loss": 6.5361,
      "step": 284
    },
    {
      "epoch": 1.6285714285714286,
      "grad_norm": 1.0734964609146118,
      "learning_rate": 2.9727793696275073e-05,
      "loss": 6.5874,
      "step": 285
    },
    {
      "epoch": 1.6342857142857143,
      "grad_norm": 1.121555209159851,
      "learning_rate": 2.9656160458452727e-05,
      "loss": 6.8036,
      "step": 286
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 1.1174023151397705,
      "learning_rate": 2.9584527220630377e-05,
      "loss": 6.7052,
      "step": 287
    },
    {
      "epoch": 1.6457142857142857,
      "grad_norm": 0.9479807615280151,
      "learning_rate": 2.9512893982808027e-05,
      "loss": 6.6395,
      "step": 288
    },
    {
      "epoch": 1.6514285714285715,
      "grad_norm": 0.9999933838844299,
      "learning_rate": 2.9441260744985677e-05,
      "loss": 6.6237,
      "step": 289
    },
    {
      "epoch": 1.657142857142857,
      "grad_norm": 1.0076006650924683,
      "learning_rate": 2.9369627507163327e-05,
      "loss": 6.4425,
      "step": 290
    },
    {
      "epoch": 1.6628571428571428,
      "grad_norm": 0.7932950854301453,
      "learning_rate": 2.9297994269340976e-05,
      "loss": 6.5278,
      "step": 291
    },
    {
      "epoch": 1.6685714285714286,
      "grad_norm": 0.8053600788116455,
      "learning_rate": 2.922636103151863e-05,
      "loss": 6.6793,
      "step": 292
    },
    {
      "epoch": 1.6742857142857144,
      "grad_norm": 1.1390734910964966,
      "learning_rate": 2.9154727793696273e-05,
      "loss": 6.515,
      "step": 293
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.6550987958908081,
      "learning_rate": 2.9083094555873923e-05,
      "loss": 6.4618,
      "step": 294
    },
    {
      "epoch": 1.6857142857142857,
      "grad_norm": 0.9840977787971497,
      "learning_rate": 2.9011461318051576e-05,
      "loss": 6.5707,
      "step": 295
    },
    {
      "epoch": 1.6914285714285713,
      "grad_norm": 0.9283097386360168,
      "learning_rate": 2.8939828080229226e-05,
      "loss": 6.5608,
      "step": 296
    },
    {
      "epoch": 1.697142857142857,
      "grad_norm": 0.9462127089500427,
      "learning_rate": 2.8868194842406876e-05,
      "loss": 6.5337,
      "step": 297
    },
    {
      "epoch": 1.7028571428571428,
      "grad_norm": 1.3549820184707642,
      "learning_rate": 2.8796561604584526e-05,
      "loss": 6.4352,
      "step": 298
    },
    {
      "epoch": 1.7085714285714286,
      "grad_norm": 0.7305532693862915,
      "learning_rate": 2.8724928366762176e-05,
      "loss": 6.6181,
      "step": 299
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 0.8049830198287964,
      "learning_rate": 2.8653295128939826e-05,
      "loss": 6.6902,
      "step": 300
    },
    {
      "epoch": 1.72,
      "grad_norm": 1.088653802871704,
      "learning_rate": 2.858166189111748e-05,
      "loss": 6.6439,
      "step": 301
    },
    {
      "epoch": 1.7257142857142858,
      "grad_norm": 1.2481476068496704,
      "learning_rate": 2.851002865329513e-05,
      "loss": 6.4814,
      "step": 302
    },
    {
      "epoch": 1.7314285714285713,
      "grad_norm": 1.5075782537460327,
      "learning_rate": 2.843839541547278e-05,
      "loss": 6.64,
      "step": 303
    },
    {
      "epoch": 1.737142857142857,
      "grad_norm": 0.7827324271202087,
      "learning_rate": 2.836676217765043e-05,
      "loss": 6.6044,
      "step": 304
    },
    {
      "epoch": 1.7428571428571429,
      "grad_norm": 0.7348993420600891,
      "learning_rate": 2.829512893982808e-05,
      "loss": 6.413,
      "step": 305
    },
    {
      "epoch": 1.7485714285714287,
      "grad_norm": 0.9989147782325745,
      "learning_rate": 2.8223495702005733e-05,
      "loss": 6.5062,
      "step": 306
    },
    {
      "epoch": 1.7542857142857144,
      "grad_norm": 0.8965410590171814,
      "learning_rate": 2.8151862464183383e-05,
      "loss": 6.5461,
      "step": 307
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.8916110396385193,
      "learning_rate": 2.8080229226361033e-05,
      "loss": 6.4507,
      "step": 308
    },
    {
      "epoch": 1.7657142857142856,
      "grad_norm": 0.9047384262084961,
      "learning_rate": 2.8008595988538683e-05,
      "loss": 6.5885,
      "step": 309
    },
    {
      "epoch": 1.7714285714285714,
      "grad_norm": 1.0499759912490845,
      "learning_rate": 2.7936962750716332e-05,
      "loss": 6.3775,
      "step": 310
    },
    {
      "epoch": 1.7771428571428571,
      "grad_norm": 0.79208904504776,
      "learning_rate": 2.7865329512893982e-05,
      "loss": 6.5903,
      "step": 311
    },
    {
      "epoch": 1.782857142857143,
      "grad_norm": 1.0782393217086792,
      "learning_rate": 2.7793696275071636e-05,
      "loss": 6.3684,
      "step": 312
    },
    {
      "epoch": 1.7885714285714287,
      "grad_norm": 0.7960412502288818,
      "learning_rate": 2.7722063037249286e-05,
      "loss": 6.6417,
      "step": 313
    },
    {
      "epoch": 1.7942857142857143,
      "grad_norm": 0.8494144082069397,
      "learning_rate": 2.7650429799426936e-05,
      "loss": 6.6433,
      "step": 314
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.9165849089622498,
      "learning_rate": 2.7578796561604586e-05,
      "loss": 6.6485,
      "step": 315
    },
    {
      "epoch": 1.8057142857142856,
      "grad_norm": 0.7345314621925354,
      "learning_rate": 2.7507163323782236e-05,
      "loss": 6.4442,
      "step": 316
    },
    {
      "epoch": 1.8114285714285714,
      "grad_norm": 0.7948485612869263,
      "learning_rate": 2.743553008595989e-05,
      "loss": 6.7505,
      "step": 317
    },
    {
      "epoch": 1.8171428571428572,
      "grad_norm": 1.3179972171783447,
      "learning_rate": 2.736389684813754e-05,
      "loss": 6.4891,
      "step": 318
    },
    {
      "epoch": 1.822857142857143,
      "grad_norm": 0.8317749500274658,
      "learning_rate": 2.729226361031519e-05,
      "loss": 6.6163,
      "step": 319
    },
    {
      "epoch": 1.8285714285714287,
      "grad_norm": 1.1596382856369019,
      "learning_rate": 2.722063037249284e-05,
      "loss": 6.4423,
      "step": 320
    },
    {
      "epoch": 1.8342857142857143,
      "grad_norm": 0.8429292440414429,
      "learning_rate": 2.714899713467049e-05,
      "loss": 6.4875,
      "step": 321
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1.4651106595993042,
      "learning_rate": 2.707736389684814e-05,
      "loss": 6.6479,
      "step": 322
    },
    {
      "epoch": 1.8457142857142856,
      "grad_norm": 1.244036316871643,
      "learning_rate": 2.7005730659025792e-05,
      "loss": 6.6339,
      "step": 323
    },
    {
      "epoch": 1.8514285714285714,
      "grad_norm": 1.0108797550201416,
      "learning_rate": 2.6934097421203442e-05,
      "loss": 6.4823,
      "step": 324
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 1.2307052612304688,
      "learning_rate": 2.6862464183381092e-05,
      "loss": 6.6277,
      "step": 325
    },
    {
      "epoch": 1.862857142857143,
      "grad_norm": 1.2170847654342651,
      "learning_rate": 2.6790830945558742e-05,
      "loss": 6.7156,
      "step": 326
    },
    {
      "epoch": 1.8685714285714285,
      "grad_norm": 1.027488112449646,
      "learning_rate": 2.6719197707736392e-05,
      "loss": 6.3151,
      "step": 327
    },
    {
      "epoch": 1.8742857142857143,
      "grad_norm": 0.7655477523803711,
      "learning_rate": 2.6647564469914045e-05,
      "loss": 6.4495,
      "step": 328
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.8463665843009949,
      "learning_rate": 2.6575931232091695e-05,
      "loss": 6.5907,
      "step": 329
    },
    {
      "epoch": 1.8857142857142857,
      "grad_norm": 0.9608197212219238,
      "learning_rate": 2.6504297994269345e-05,
      "loss": 6.732,
      "step": 330
    },
    {
      "epoch": 1.8914285714285715,
      "grad_norm": 0.7847747802734375,
      "learning_rate": 2.6432664756446995e-05,
      "loss": 6.3206,
      "step": 331
    },
    {
      "epoch": 1.8971428571428572,
      "grad_norm": 1.1621595621109009,
      "learning_rate": 2.6361031518624642e-05,
      "loss": 6.6933,
      "step": 332
    },
    {
      "epoch": 1.9028571428571428,
      "grad_norm": 1.019371509552002,
      "learning_rate": 2.6289398280802292e-05,
      "loss": 6.4627,
      "step": 333
    },
    {
      "epoch": 1.9085714285714286,
      "grad_norm": 0.9400561451911926,
      "learning_rate": 2.6217765042979942e-05,
      "loss": 6.6123,
      "step": 334
    },
    {
      "epoch": 1.9142857142857141,
      "grad_norm": 0.9269523024559021,
      "learning_rate": 2.614613180515759e-05,
      "loss": 6.5828,
      "step": 335
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.1998815536499023,
      "learning_rate": 2.607449856733524e-05,
      "loss": 6.4651,
      "step": 336
    },
    {
      "epoch": 1.9257142857142857,
      "grad_norm": 0.8266356587409973,
      "learning_rate": 2.600286532951289e-05,
      "loss": 6.5384,
      "step": 337
    },
    {
      "epoch": 1.9314285714285715,
      "grad_norm": 0.7539535164833069,
      "learning_rate": 2.5931232091690545e-05,
      "loss": 6.5971,
      "step": 338
    },
    {
      "epoch": 1.9371428571428573,
      "grad_norm": 1.0947332382202148,
      "learning_rate": 2.5859598853868195e-05,
      "loss": 6.6185,
      "step": 339
    },
    {
      "epoch": 1.9428571428571428,
      "grad_norm": 0.9737542271614075,
      "learning_rate": 2.5787965616045845e-05,
      "loss": 6.6011,
      "step": 340
    },
    {
      "epoch": 1.9485714285714286,
      "grad_norm": 0.7709317803382874,
      "learning_rate": 2.5716332378223495e-05,
      "loss": 6.666,
      "step": 341
    },
    {
      "epoch": 1.9542857142857142,
      "grad_norm": 0.9453389644622803,
      "learning_rate": 2.5644699140401145e-05,
      "loss": 6.6577,
      "step": 342
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.039215326309204,
      "learning_rate": 2.5573065902578798e-05,
      "loss": 6.5999,
      "step": 343
    },
    {
      "epoch": 1.9657142857142857,
      "grad_norm": 0.9024221897125244,
      "learning_rate": 2.5501432664756448e-05,
      "loss": 6.3987,
      "step": 344
    },
    {
      "epoch": 1.9714285714285715,
      "grad_norm": 1.0550659894943237,
      "learning_rate": 2.5429799426934098e-05,
      "loss": 6.8223,
      "step": 345
    },
    {
      "epoch": 1.977142857142857,
      "grad_norm": 1.0162485837936401,
      "learning_rate": 2.5358166189111748e-05,
      "loss": 6.7097,
      "step": 346
    },
    {
      "epoch": 1.9828571428571429,
      "grad_norm": 0.8372044563293457,
      "learning_rate": 2.5286532951289398e-05,
      "loss": 6.6162,
      "step": 347
    },
    {
      "epoch": 1.9885714285714284,
      "grad_norm": 1.4506908655166626,
      "learning_rate": 2.5214899713467048e-05,
      "loss": 6.5113,
      "step": 348
    },
    {
      "epoch": 1.9942857142857142,
      "grad_norm": 1.0330259799957275,
      "learning_rate": 2.51432664756447e-05,
      "loss": 6.4672,
      "step": 349
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.8022205233573914,
      "learning_rate": 2.507163323782235e-05,
      "loss": 6.5616,
      "step": 350
    },
    {
      "epoch": 2.005714285714286,
      "grad_norm": 0.8864841461181641,
      "learning_rate": 2.5e-05,
      "loss": 6.4308,
      "step": 351
    },
    {
      "epoch": 2.0114285714285716,
      "grad_norm": 1.0204095840454102,
      "learning_rate": 2.492836676217765e-05,
      "loss": 6.7911,
      "step": 352
    },
    {
      "epoch": 2.0171428571428573,
      "grad_norm": 1.1601698398590088,
      "learning_rate": 2.48567335243553e-05,
      "loss": 6.5064,
      "step": 353
    },
    {
      "epoch": 2.0228571428571427,
      "grad_norm": 1.2086524963378906,
      "learning_rate": 2.4785100286532954e-05,
      "loss": 6.5757,
      "step": 354
    },
    {
      "epoch": 2.0285714285714285,
      "grad_norm": 1.100976824760437,
      "learning_rate": 2.4713467048710604e-05,
      "loss": 6.6968,
      "step": 355
    },
    {
      "epoch": 2.0342857142857143,
      "grad_norm": 1.195662498474121,
      "learning_rate": 2.4641833810888254e-05,
      "loss": 6.6121,
      "step": 356
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.9885674118995667,
      "learning_rate": 2.4570200573065904e-05,
      "loss": 6.4838,
      "step": 357
    },
    {
      "epoch": 2.045714285714286,
      "grad_norm": 0.8038871884346008,
      "learning_rate": 2.4498567335243554e-05,
      "loss": 6.5931,
      "step": 358
    },
    {
      "epoch": 2.0514285714285716,
      "grad_norm": 0.9306955337524414,
      "learning_rate": 2.4426934097421204e-05,
      "loss": 6.4596,
      "step": 359
    },
    {
      "epoch": 2.057142857142857,
      "grad_norm": 1.2629653215408325,
      "learning_rate": 2.4355300859598858e-05,
      "loss": 6.4612,
      "step": 360
    },
    {
      "epoch": 2.0628571428571427,
      "grad_norm": 1.226009726524353,
      "learning_rate": 2.4283667621776504e-05,
      "loss": 6.2852,
      "step": 361
    },
    {
      "epoch": 2.0685714285714285,
      "grad_norm": 1.0805630683898926,
      "learning_rate": 2.4212034383954154e-05,
      "loss": 6.4197,
      "step": 362
    },
    {
      "epoch": 2.0742857142857143,
      "grad_norm": 0.9868965744972229,
      "learning_rate": 2.4140401146131804e-05,
      "loss": 6.3492,
      "step": 363
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.0140677690505981,
      "learning_rate": 2.4068767908309457e-05,
      "loss": 6.2099,
      "step": 364
    },
    {
      "epoch": 2.085714285714286,
      "grad_norm": 1.0065054893493652,
      "learning_rate": 2.3997134670487107e-05,
      "loss": 6.4978,
      "step": 365
    },
    {
      "epoch": 2.0914285714285716,
      "grad_norm": 1.0789443254470825,
      "learning_rate": 2.3925501432664757e-05,
      "loss": 6.5523,
      "step": 366
    },
    {
      "epoch": 2.097142857142857,
      "grad_norm": 2.017167806625366,
      "learning_rate": 2.3853868194842407e-05,
      "loss": 6.6187,
      "step": 367
    },
    {
      "epoch": 2.1028571428571428,
      "grad_norm": 1.623489260673523,
      "learning_rate": 2.3782234957020057e-05,
      "loss": 6.6583,
      "step": 368
    },
    {
      "epoch": 2.1085714285714285,
      "grad_norm": 1.7446346282958984,
      "learning_rate": 2.3710601719197707e-05,
      "loss": 6.6867,
      "step": 369
    },
    {
      "epoch": 2.1142857142857143,
      "grad_norm": 1.4957152605056763,
      "learning_rate": 2.363896848137536e-05,
      "loss": 6.5948,
      "step": 370
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.8025145530700684,
      "learning_rate": 2.356733524355301e-05,
      "loss": 6.5729,
      "step": 371
    },
    {
      "epoch": 2.125714285714286,
      "grad_norm": 0.7559846639633179,
      "learning_rate": 2.349570200573066e-05,
      "loss": 6.5317,
      "step": 372
    },
    {
      "epoch": 2.1314285714285712,
      "grad_norm": 1.1209169626235962,
      "learning_rate": 2.342406876790831e-05,
      "loss": 6.5577,
      "step": 373
    },
    {
      "epoch": 2.137142857142857,
      "grad_norm": 1.0731127262115479,
      "learning_rate": 2.335243553008596e-05,
      "loss": 6.484,
      "step": 374
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 1.1982375383377075,
      "learning_rate": 2.328080229226361e-05,
      "loss": 6.3084,
      "step": 375
    },
    {
      "epoch": 2.1485714285714286,
      "grad_norm": 0.7997266054153442,
      "learning_rate": 2.3209169054441264e-05,
      "loss": 6.4991,
      "step": 376
    },
    {
      "epoch": 2.1542857142857144,
      "grad_norm": 1.00643789768219,
      "learning_rate": 2.3137535816618914e-05,
      "loss": 6.4027,
      "step": 377
    },
    {
      "epoch": 2.16,
      "grad_norm": 1.0816012620925903,
      "learning_rate": 2.3065902578796564e-05,
      "loss": 6.5783,
      "step": 378
    },
    {
      "epoch": 2.1657142857142855,
      "grad_norm": 1.503928780555725,
      "learning_rate": 2.2994269340974214e-05,
      "loss": 6.373,
      "step": 379
    },
    {
      "epoch": 2.1714285714285713,
      "grad_norm": 1.2795029878616333,
      "learning_rate": 2.2922636103151864e-05,
      "loss": 6.4617,
      "step": 380
    },
    {
      "epoch": 2.177142857142857,
      "grad_norm": 0.9059144258499146,
      "learning_rate": 2.2851002865329514e-05,
      "loss": 6.6347,
      "step": 381
    },
    {
      "epoch": 2.182857142857143,
      "grad_norm": 1.1327437162399292,
      "learning_rate": 2.2779369627507164e-05,
      "loss": 6.6616,
      "step": 382
    },
    {
      "epoch": 2.1885714285714286,
      "grad_norm": 0.9929631352424622,
      "learning_rate": 2.2707736389684813e-05,
      "loss": 6.5159,
      "step": 383
    },
    {
      "epoch": 2.1942857142857144,
      "grad_norm": 1.0005110502243042,
      "learning_rate": 2.2636103151862463e-05,
      "loss": 6.5146,
      "step": 384
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.0008368492126465,
      "learning_rate": 2.2564469914040113e-05,
      "loss": 6.55,
      "step": 385
    },
    {
      "epoch": 2.2057142857142855,
      "grad_norm": 1.0042519569396973,
      "learning_rate": 2.2492836676217767e-05,
      "loss": 6.5664,
      "step": 386
    },
    {
      "epoch": 2.2114285714285713,
      "grad_norm": 1.3028265237808228,
      "learning_rate": 2.2421203438395417e-05,
      "loss": 6.568,
      "step": 387
    },
    {
      "epoch": 2.217142857142857,
      "grad_norm": 0.96274334192276,
      "learning_rate": 2.2349570200573067e-05,
      "loss": 6.5588,
      "step": 388
    },
    {
      "epoch": 2.222857142857143,
      "grad_norm": 1.3022395372390747,
      "learning_rate": 2.2277936962750717e-05,
      "loss": 6.4897,
      "step": 389
    },
    {
      "epoch": 2.2285714285714286,
      "grad_norm": 0.7925626039505005,
      "learning_rate": 2.2206303724928367e-05,
      "loss": 6.5386,
      "step": 390
    },
    {
      "epoch": 2.2342857142857144,
      "grad_norm": 1.1983871459960938,
      "learning_rate": 2.213467048710602e-05,
      "loss": 6.6261,
      "step": 391
    },
    {
      "epoch": 2.24,
      "grad_norm": 1.1234767436981201,
      "learning_rate": 2.206303724928367e-05,
      "loss": 6.503,
      "step": 392
    },
    {
      "epoch": 2.2457142857142856,
      "grad_norm": 0.9069517850875854,
      "learning_rate": 2.199140401146132e-05,
      "loss": 6.5785,
      "step": 393
    },
    {
      "epoch": 2.2514285714285713,
      "grad_norm": 0.8923952579498291,
      "learning_rate": 2.191977077363897e-05,
      "loss": 6.3756,
      "step": 394
    },
    {
      "epoch": 2.257142857142857,
      "grad_norm": 0.8663195371627808,
      "learning_rate": 2.184813753581662e-05,
      "loss": 6.4775,
      "step": 395
    },
    {
      "epoch": 2.262857142857143,
      "grad_norm": 0.9774727821350098,
      "learning_rate": 2.177650429799427e-05,
      "loss": 6.6114,
      "step": 396
    },
    {
      "epoch": 2.2685714285714287,
      "grad_norm": 0.851176381111145,
      "learning_rate": 2.1704871060171923e-05,
      "loss": 6.6535,
      "step": 397
    },
    {
      "epoch": 2.2742857142857145,
      "grad_norm": 0.910223662853241,
      "learning_rate": 2.1633237822349573e-05,
      "loss": 6.4043,
      "step": 398
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.7531032562255859,
      "learning_rate": 2.1561604584527223e-05,
      "loss": 6.644,
      "step": 399
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 2.003308057785034,
      "learning_rate": 2.148997134670487e-05,
      "loss": 6.5227,
      "step": 400
    },
    {
      "epoch": 2.2914285714285714,
      "grad_norm": 1.7685054540634155,
      "learning_rate": 2.1418338108882523e-05,
      "loss": 6.3338,
      "step": 401
    },
    {
      "epoch": 2.297142857142857,
      "grad_norm": 0.7933223843574524,
      "learning_rate": 2.1346704871060173e-05,
      "loss": 6.5881,
      "step": 402
    },
    {
      "epoch": 2.302857142857143,
      "grad_norm": 0.8909488320350647,
      "learning_rate": 2.1275071633237823e-05,
      "loss": 6.4976,
      "step": 403
    },
    {
      "epoch": 2.3085714285714287,
      "grad_norm": 1.9814530611038208,
      "learning_rate": 2.1203438395415473e-05,
      "loss": 6.507,
      "step": 404
    },
    {
      "epoch": 2.314285714285714,
      "grad_norm": 1.3451930284500122,
      "learning_rate": 2.1131805157593123e-05,
      "loss": 6.5052,
      "step": 405
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.6522356271743774,
      "learning_rate": 2.1060171919770773e-05,
      "loss": 6.3804,
      "step": 406
    },
    {
      "epoch": 2.3257142857142856,
      "grad_norm": 1.3253685235977173,
      "learning_rate": 2.0988538681948426e-05,
      "loss": 6.3618,
      "step": 407
    },
    {
      "epoch": 2.3314285714285714,
      "grad_norm": 0.969006359577179,
      "learning_rate": 2.0916905444126076e-05,
      "loss": 6.394,
      "step": 408
    },
    {
      "epoch": 2.337142857142857,
      "grad_norm": 1.131894826889038,
      "learning_rate": 2.0845272206303726e-05,
      "loss": 6.4316,
      "step": 409
    },
    {
      "epoch": 2.342857142857143,
      "grad_norm": 0.9776267409324646,
      "learning_rate": 2.0773638968481376e-05,
      "loss": 6.494,
      "step": 410
    },
    {
      "epoch": 2.3485714285714288,
      "grad_norm": 1.0545101165771484,
      "learning_rate": 2.0702005730659026e-05,
      "loss": 6.5506,
      "step": 411
    },
    {
      "epoch": 2.354285714285714,
      "grad_norm": 1.591834306716919,
      "learning_rate": 2.063037249283668e-05,
      "loss": 6.4442,
      "step": 412
    },
    {
      "epoch": 2.36,
      "grad_norm": 1.0696990489959717,
      "learning_rate": 2.055873925501433e-05,
      "loss": 6.4386,
      "step": 413
    },
    {
      "epoch": 2.3657142857142857,
      "grad_norm": 1.5385501384735107,
      "learning_rate": 2.048710601719198e-05,
      "loss": 6.5795,
      "step": 414
    },
    {
      "epoch": 2.3714285714285714,
      "grad_norm": 1.1406928300857544,
      "learning_rate": 2.041547277936963e-05,
      "loss": 6.4485,
      "step": 415
    },
    {
      "epoch": 2.3771428571428572,
      "grad_norm": 0.9111608266830444,
      "learning_rate": 2.034383954154728e-05,
      "loss": 6.4593,
      "step": 416
    },
    {
      "epoch": 2.382857142857143,
      "grad_norm": 1.2667001485824585,
      "learning_rate": 2.027220630372493e-05,
      "loss": 6.466,
      "step": 417
    },
    {
      "epoch": 2.388571428571429,
      "grad_norm": 0.8069669604301453,
      "learning_rate": 2.0200573065902582e-05,
      "loss": 6.2654,
      "step": 418
    },
    {
      "epoch": 2.394285714285714,
      "grad_norm": 1.8252564668655396,
      "learning_rate": 2.012893982808023e-05,
      "loss": 6.5291,
      "step": 419
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.7765423655509949,
      "learning_rate": 2.005730659025788e-05,
      "loss": 6.3432,
      "step": 420
    },
    {
      "epoch": 2.4057142857142857,
      "grad_norm": 1.5498477220535278,
      "learning_rate": 1.998567335243553e-05,
      "loss": 6.3448,
      "step": 421
    },
    {
      "epoch": 2.4114285714285715,
      "grad_norm": 0.989366888999939,
      "learning_rate": 1.991404011461318e-05,
      "loss": 6.5096,
      "step": 422
    },
    {
      "epoch": 2.4171428571428573,
      "grad_norm": 0.9936889410018921,
      "learning_rate": 1.9842406876790832e-05,
      "loss": 6.4527,
      "step": 423
    },
    {
      "epoch": 2.422857142857143,
      "grad_norm": 1.0789870023727417,
      "learning_rate": 1.9770773638968482e-05,
      "loss": 6.4524,
      "step": 424
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 1.3617050647735596,
      "learning_rate": 1.9699140401146132e-05,
      "loss": 6.2738,
      "step": 425
    },
    {
      "epoch": 2.434285714285714,
      "grad_norm": 1.1295040845870972,
      "learning_rate": 1.9627507163323782e-05,
      "loss": 6.3888,
      "step": 426
    },
    {
      "epoch": 2.44,
      "grad_norm": 1.4937052726745605,
      "learning_rate": 1.9555873925501432e-05,
      "loss": 6.3842,
      "step": 427
    },
    {
      "epoch": 2.4457142857142857,
      "grad_norm": 1.005014181137085,
      "learning_rate": 1.9484240687679085e-05,
      "loss": 6.3158,
      "step": 428
    },
    {
      "epoch": 2.4514285714285715,
      "grad_norm": 0.9153880476951599,
      "learning_rate": 1.9412607449856735e-05,
      "loss": 6.6894,
      "step": 429
    },
    {
      "epoch": 2.4571428571428573,
      "grad_norm": 1.5620912313461304,
      "learning_rate": 1.9340974212034385e-05,
      "loss": 6.2232,
      "step": 430
    },
    {
      "epoch": 2.4628571428571426,
      "grad_norm": 0.9730080962181091,
      "learning_rate": 1.9269340974212035e-05,
      "loss": 6.5973,
      "step": 431
    },
    {
      "epoch": 2.4685714285714284,
      "grad_norm": 1.0448285341262817,
      "learning_rate": 1.9197707736389685e-05,
      "loss": 6.291,
      "step": 432
    },
    {
      "epoch": 2.474285714285714,
      "grad_norm": 0.9530876874923706,
      "learning_rate": 1.9126074498567335e-05,
      "loss": 6.3626,
      "step": 433
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.8513502478599548,
      "learning_rate": 1.905444126074499e-05,
      "loss": 6.3878,
      "step": 434
    },
    {
      "epoch": 2.4857142857142858,
      "grad_norm": 1.0635465383529663,
      "learning_rate": 1.898280802292264e-05,
      "loss": 6.5727,
      "step": 435
    },
    {
      "epoch": 2.4914285714285715,
      "grad_norm": 1.2321722507476807,
      "learning_rate": 1.891117478510029e-05,
      "loss": 6.393,
      "step": 436
    },
    {
      "epoch": 2.4971428571428573,
      "grad_norm": 1.146718978881836,
      "learning_rate": 1.883954154727794e-05,
      "loss": 6.6114,
      "step": 437
    },
    {
      "epoch": 2.5028571428571427,
      "grad_norm": 2.2619853019714355,
      "learning_rate": 1.876790830945559e-05,
      "loss": 6.4016,
      "step": 438
    },
    {
      "epoch": 2.5085714285714285,
      "grad_norm": 1.235487699508667,
      "learning_rate": 1.869627507163324e-05,
      "loss": 6.5631,
      "step": 439
    },
    {
      "epoch": 2.5142857142857142,
      "grad_norm": 1.7102429866790771,
      "learning_rate": 1.862464183381089e-05,
      "loss": 6.2861,
      "step": 440
    },
    {
      "epoch": 2.52,
      "grad_norm": 1.1660749912261963,
      "learning_rate": 1.8553008595988538e-05,
      "loss": 6.4492,
      "step": 441
    },
    {
      "epoch": 2.525714285714286,
      "grad_norm": 0.9727243781089783,
      "learning_rate": 1.8481375358166188e-05,
      "loss": 6.3875,
      "step": 442
    },
    {
      "epoch": 2.5314285714285716,
      "grad_norm": 0.9217095375061035,
      "learning_rate": 1.8409742120343838e-05,
      "loss": 6.4781,
      "step": 443
    },
    {
      "epoch": 2.5371428571428574,
      "grad_norm": 0.8746240139007568,
      "learning_rate": 1.833810888252149e-05,
      "loss": 6.5449,
      "step": 444
    },
    {
      "epoch": 2.5428571428571427,
      "grad_norm": 0.9651485085487366,
      "learning_rate": 1.826647564469914e-05,
      "loss": 6.326,
      "step": 445
    },
    {
      "epoch": 2.5485714285714285,
      "grad_norm": 1.752012014389038,
      "learning_rate": 1.819484240687679e-05,
      "loss": 6.2201,
      "step": 446
    },
    {
      "epoch": 2.5542857142857143,
      "grad_norm": 0.8980342745780945,
      "learning_rate": 1.812320916905444e-05,
      "loss": 6.5247,
      "step": 447
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.8281448483467102,
      "learning_rate": 1.805157593123209e-05,
      "loss": 6.4153,
      "step": 448
    },
    {
      "epoch": 2.565714285714286,
      "grad_norm": 0.8075223565101624,
      "learning_rate": 1.7979942693409745e-05,
      "loss": 6.5024,
      "step": 449
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 1.0508853197097778,
      "learning_rate": 1.7908309455587395e-05,
      "loss": 6.421,
      "step": 450
    },
    {
      "epoch": 2.5771428571428574,
      "grad_norm": 0.8915438652038574,
      "learning_rate": 1.7836676217765045e-05,
      "loss": 6.6772,
      "step": 451
    },
    {
      "epoch": 2.5828571428571427,
      "grad_norm": 0.9351531863212585,
      "learning_rate": 1.7765042979942695e-05,
      "loss": 6.5367,
      "step": 452
    },
    {
      "epoch": 2.5885714285714285,
      "grad_norm": 0.810642421245575,
      "learning_rate": 1.7693409742120345e-05,
      "loss": 6.4752,
      "step": 453
    },
    {
      "epoch": 2.5942857142857143,
      "grad_norm": 1.2343465089797974,
      "learning_rate": 1.7621776504297995e-05,
      "loss": 6.3733,
      "step": 454
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.9726742506027222,
      "learning_rate": 1.7550143266475648e-05,
      "loss": 6.5502,
      "step": 455
    },
    {
      "epoch": 2.605714285714286,
      "grad_norm": 0.8670710325241089,
      "learning_rate": 1.7478510028653298e-05,
      "loss": 6.4311,
      "step": 456
    },
    {
      "epoch": 2.611428571428571,
      "grad_norm": 1.6242620944976807,
      "learning_rate": 1.7406876790830948e-05,
      "loss": 6.3977,
      "step": 457
    },
    {
      "epoch": 2.617142857142857,
      "grad_norm": 0.9909611940383911,
      "learning_rate": 1.7335243553008594e-05,
      "loss": 6.4566,
      "step": 458
    },
    {
      "epoch": 2.6228571428571428,
      "grad_norm": 1.4619463682174683,
      "learning_rate": 1.7263610315186248e-05,
      "loss": 6.1183,
      "step": 459
    },
    {
      "epoch": 2.6285714285714286,
      "grad_norm": 0.9397057294845581,
      "learning_rate": 1.7191977077363898e-05,
      "loss": 6.5455,
      "step": 460
    },
    {
      "epoch": 2.6342857142857143,
      "grad_norm": 1.8306304216384888,
      "learning_rate": 1.7120343839541548e-05,
      "loss": 6.5343,
      "step": 461
    },
    {
      "epoch": 2.64,
      "grad_norm": 1.0293738842010498,
      "learning_rate": 1.7048710601719198e-05,
      "loss": 6.3512,
      "step": 462
    },
    {
      "epoch": 2.645714285714286,
      "grad_norm": 0.9462872743606567,
      "learning_rate": 1.6977077363896848e-05,
      "loss": 6.5497,
      "step": 463
    },
    {
      "epoch": 2.6514285714285712,
      "grad_norm": 1.3601481914520264,
      "learning_rate": 1.6905444126074498e-05,
      "loss": 6.5939,
      "step": 464
    },
    {
      "epoch": 2.657142857142857,
      "grad_norm": 1.0692461729049683,
      "learning_rate": 1.683381088825215e-05,
      "loss": 6.4193,
      "step": 465
    },
    {
      "epoch": 2.662857142857143,
      "grad_norm": 2.150092601776123,
      "learning_rate": 1.67621776504298e-05,
      "loss": 6.5961,
      "step": 466
    },
    {
      "epoch": 2.6685714285714286,
      "grad_norm": 1.2259784936904907,
      "learning_rate": 1.669054441260745e-05,
      "loss": 6.5437,
      "step": 467
    },
    {
      "epoch": 2.6742857142857144,
      "grad_norm": 1.0231324434280396,
      "learning_rate": 1.66189111747851e-05,
      "loss": 6.3526,
      "step": 468
    },
    {
      "epoch": 2.68,
      "grad_norm": 1.3246809244155884,
      "learning_rate": 1.654727793696275e-05,
      "loss": 6.7049,
      "step": 469
    },
    {
      "epoch": 2.685714285714286,
      "grad_norm": 0.7790839672088623,
      "learning_rate": 1.64756446991404e-05,
      "loss": 6.4959,
      "step": 470
    },
    {
      "epoch": 2.6914285714285713,
      "grad_norm": 1.0145258903503418,
      "learning_rate": 1.6404011461318054e-05,
      "loss": 6.3243,
      "step": 471
    },
    {
      "epoch": 2.697142857142857,
      "grad_norm": 1.0891942977905273,
      "learning_rate": 1.6332378223495704e-05,
      "loss": 6.6997,
      "step": 472
    },
    {
      "epoch": 2.702857142857143,
      "grad_norm": 1.1111875772476196,
      "learning_rate": 1.6260744985673354e-05,
      "loss": 6.6492,
      "step": 473
    },
    {
      "epoch": 2.7085714285714286,
      "grad_norm": 0.8282580375671387,
      "learning_rate": 1.6189111747851004e-05,
      "loss": 6.4924,
      "step": 474
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 1.3766881227493286,
      "learning_rate": 1.6117478510028654e-05,
      "loss": 6.393,
      "step": 475
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 1.0022915601730347,
      "learning_rate": 1.6045845272206307e-05,
      "loss": 6.3845,
      "step": 476
    },
    {
      "epoch": 2.725714285714286,
      "grad_norm": 1.43621027469635,
      "learning_rate": 1.5974212034383957e-05,
      "loss": 6.3579,
      "step": 477
    },
    {
      "epoch": 2.7314285714285713,
      "grad_norm": 1.0022966861724854,
      "learning_rate": 1.5902578796561604e-05,
      "loss": 6.4576,
      "step": 478
    },
    {
      "epoch": 2.737142857142857,
      "grad_norm": 0.8978508114814758,
      "learning_rate": 1.5830945558739254e-05,
      "loss": 6.4377,
      "step": 479
    },
    {
      "epoch": 2.742857142857143,
      "grad_norm": 0.895642101764679,
      "learning_rate": 1.5759312320916904e-05,
      "loss": 6.6648,
      "step": 480
    },
    {
      "epoch": 2.7485714285714287,
      "grad_norm": 0.9620312452316284,
      "learning_rate": 1.5687679083094557e-05,
      "loss": 6.5864,
      "step": 481
    },
    {
      "epoch": 2.7542857142857144,
      "grad_norm": 1.6907916069030762,
      "learning_rate": 1.5616045845272207e-05,
      "loss": 6.4495,
      "step": 482
    },
    {
      "epoch": 2.76,
      "grad_norm": 1.8304070234298706,
      "learning_rate": 1.5544412607449857e-05,
      "loss": 6.1749,
      "step": 483
    },
    {
      "epoch": 2.7657142857142856,
      "grad_norm": 1.895301342010498,
      "learning_rate": 1.5472779369627507e-05,
      "loss": 6.2903,
      "step": 484
    },
    {
      "epoch": 2.7714285714285714,
      "grad_norm": 0.9142917394638062,
      "learning_rate": 1.5401146131805157e-05,
      "loss": 6.5239,
      "step": 485
    },
    {
      "epoch": 2.777142857142857,
      "grad_norm": 0.9730989933013916,
      "learning_rate": 1.532951289398281e-05,
      "loss": 6.58,
      "step": 486
    },
    {
      "epoch": 2.782857142857143,
      "grad_norm": 1.1946083307266235,
      "learning_rate": 1.5257879656160458e-05,
      "loss": 6.3866,
      "step": 487
    },
    {
      "epoch": 2.7885714285714287,
      "grad_norm": 0.9061421751976013,
      "learning_rate": 1.518624641833811e-05,
      "loss": 6.3629,
      "step": 488
    },
    {
      "epoch": 2.7942857142857145,
      "grad_norm": 0.7742664813995361,
      "learning_rate": 1.511461318051576e-05,
      "loss": 6.4996,
      "step": 489
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.9038740992546082,
      "learning_rate": 1.5042979942693412e-05,
      "loss": 6.3813,
      "step": 490
    },
    {
      "epoch": 2.8057142857142856,
      "grad_norm": 0.840945839881897,
      "learning_rate": 1.4971346704871062e-05,
      "loss": 6.4427,
      "step": 491
    },
    {
      "epoch": 2.8114285714285714,
      "grad_norm": 0.9172019958496094,
      "learning_rate": 1.4899713467048712e-05,
      "loss": 6.4415,
      "step": 492
    },
    {
      "epoch": 2.817142857142857,
      "grad_norm": 0.9660903811454773,
      "learning_rate": 1.4828080229226363e-05,
      "loss": 6.6776,
      "step": 493
    },
    {
      "epoch": 2.822857142857143,
      "grad_norm": 0.7333164811134338,
      "learning_rate": 1.4756446991404013e-05,
      "loss": 6.3612,
      "step": 494
    },
    {
      "epoch": 2.8285714285714287,
      "grad_norm": 1.2153434753417969,
      "learning_rate": 1.4684813753581663e-05,
      "loss": 6.3107,
      "step": 495
    },
    {
      "epoch": 2.8342857142857145,
      "grad_norm": 1.02610182762146,
      "learning_rate": 1.4613180515759315e-05,
      "loss": 6.4296,
      "step": 496
    },
    {
      "epoch": 2.84,
      "grad_norm": 1.3198039531707764,
      "learning_rate": 1.4541547277936961e-05,
      "loss": 6.3373,
      "step": 497
    },
    {
      "epoch": 2.8457142857142856,
      "grad_norm": 1.0521643161773682,
      "learning_rate": 1.4469914040114613e-05,
      "loss": 6.4754,
      "step": 498
    },
    {
      "epoch": 2.8514285714285714,
      "grad_norm": 0.9171225428581238,
      "learning_rate": 1.4398280802292263e-05,
      "loss": 6.6188,
      "step": 499
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 1.3500430583953857,
      "learning_rate": 1.4326647564469913e-05,
      "loss": 6.6174,
      "step": 500
    },
    {
      "epoch": 2.862857142857143,
      "grad_norm": 0.9818475842475891,
      "learning_rate": 1.4255014326647565e-05,
      "loss": 6.6011,
      "step": 501
    },
    {
      "epoch": 2.8685714285714283,
      "grad_norm": 0.9927942752838135,
      "learning_rate": 1.4183381088825215e-05,
      "loss": 6.6523,
      "step": 502
    },
    {
      "epoch": 2.8742857142857146,
      "grad_norm": 1.2391276359558105,
      "learning_rate": 1.4111747851002866e-05,
      "loss": 6.451,
      "step": 503
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.7987445592880249,
      "learning_rate": 1.4040114613180516e-05,
      "loss": 6.4742,
      "step": 504
    },
    {
      "epoch": 2.8857142857142857,
      "grad_norm": 0.8845332860946655,
      "learning_rate": 1.3968481375358166e-05,
      "loss": 6.317,
      "step": 505
    },
    {
      "epoch": 2.8914285714285715,
      "grad_norm": 1.4875879287719727,
      "learning_rate": 1.3896848137535818e-05,
      "loss": 6.4887,
      "step": 506
    },
    {
      "epoch": 2.8971428571428572,
      "grad_norm": 0.9502572417259216,
      "learning_rate": 1.3825214899713468e-05,
      "loss": 6.4512,
      "step": 507
    },
    {
      "epoch": 2.902857142857143,
      "grad_norm": 0.7897675633430481,
      "learning_rate": 1.3753581661891118e-05,
      "loss": 6.6187,
      "step": 508
    },
    {
      "epoch": 2.9085714285714284,
      "grad_norm": 1.2007720470428467,
      "learning_rate": 1.368194842406877e-05,
      "loss": 6.5637,
      "step": 509
    },
    {
      "epoch": 2.914285714285714,
      "grad_norm": 0.93313068151474,
      "learning_rate": 1.361031518624642e-05,
      "loss": 6.3682,
      "step": 510
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.7760981321334839,
      "learning_rate": 1.353868194842407e-05,
      "loss": 6.461,
      "step": 511
    },
    {
      "epoch": 2.9257142857142857,
      "grad_norm": 1.1638435125350952,
      "learning_rate": 1.3467048710601721e-05,
      "loss": 6.442,
      "step": 512
    },
    {
      "epoch": 2.9314285714285715,
      "grad_norm": 1.111535906791687,
      "learning_rate": 1.3395415472779371e-05,
      "loss": 6.4323,
      "step": 513
    },
    {
      "epoch": 2.9371428571428573,
      "grad_norm": 1.273122787475586,
      "learning_rate": 1.3323782234957023e-05,
      "loss": 6.4571,
      "step": 514
    },
    {
      "epoch": 2.942857142857143,
      "grad_norm": 0.8992041349411011,
      "learning_rate": 1.3252148997134673e-05,
      "loss": 6.3941,
      "step": 515
    },
    {
      "epoch": 2.9485714285714284,
      "grad_norm": 0.9895095825195312,
      "learning_rate": 1.3180515759312321e-05,
      "loss": 6.5776,
      "step": 516
    },
    {
      "epoch": 2.954285714285714,
      "grad_norm": 0.8348529934883118,
      "learning_rate": 1.3108882521489971e-05,
      "loss": 6.5503,
      "step": 517
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.0728434324264526,
      "learning_rate": 1.303724928366762e-05,
      "loss": 6.6694,
      "step": 518
    },
    {
      "epoch": 2.9657142857142857,
      "grad_norm": 1.0754367113113403,
      "learning_rate": 1.2965616045845272e-05,
      "loss": 6.414,
      "step": 519
    },
    {
      "epoch": 2.9714285714285715,
      "grad_norm": 1.5991706848144531,
      "learning_rate": 1.2893982808022922e-05,
      "loss": 6.4991,
      "step": 520
    },
    {
      "epoch": 2.977142857142857,
      "grad_norm": 1.179283857345581,
      "learning_rate": 1.2822349570200572e-05,
      "loss": 6.5424,
      "step": 521
    },
    {
      "epoch": 2.982857142857143,
      "grad_norm": 1.3169270753860474,
      "learning_rate": 1.2750716332378224e-05,
      "loss": 6.2736,
      "step": 522
    },
    {
      "epoch": 2.9885714285714284,
      "grad_norm": 0.9325336217880249,
      "learning_rate": 1.2679083094555874e-05,
      "loss": 6.4819,
      "step": 523
    },
    {
      "epoch": 2.994285714285714,
      "grad_norm": 0.9988261461257935,
      "learning_rate": 1.2607449856733524e-05,
      "loss": 6.4611,
      "step": 524
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.9107108116149902,
      "learning_rate": 1.2535816618911176e-05,
      "loss": 6.6366,
      "step": 525
    },
    {
      "epoch": 3.005714285714286,
      "grad_norm": 1.3133060932159424,
      "learning_rate": 1.2464183381088826e-05,
      "loss": 6.3431,
      "step": 526
    },
    {
      "epoch": 3.0114285714285716,
      "grad_norm": 0.866185188293457,
      "learning_rate": 1.2392550143266477e-05,
      "loss": 6.5672,
      "step": 527
    },
    {
      "epoch": 3.0171428571428573,
      "grad_norm": 0.9196680188179016,
      "learning_rate": 1.2320916905444127e-05,
      "loss": 6.3998,
      "step": 528
    },
    {
      "epoch": 3.0228571428571427,
      "grad_norm": 1.0500668287277222,
      "learning_rate": 1.2249283667621777e-05,
      "loss": 6.4044,
      "step": 529
    },
    {
      "epoch": 3.0285714285714285,
      "grad_norm": 0.9378418326377869,
      "learning_rate": 1.2177650429799429e-05,
      "loss": 6.5313,
      "step": 530
    },
    {
      "epoch": 3.0342857142857143,
      "grad_norm": 0.9664347767829895,
      "learning_rate": 1.2106017191977077e-05,
      "loss": 6.3784,
      "step": 531
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.9257510900497437,
      "learning_rate": 1.2034383954154729e-05,
      "loss": 6.4616,
      "step": 532
    },
    {
      "epoch": 3.045714285714286,
      "grad_norm": 1.2636284828186035,
      "learning_rate": 1.1962750716332379e-05,
      "loss": 6.2584,
      "step": 533
    },
    {
      "epoch": 3.0514285714285716,
      "grad_norm": 0.7834349870681763,
      "learning_rate": 1.1891117478510029e-05,
      "loss": 6.4461,
      "step": 534
    },
    {
      "epoch": 3.057142857142857,
      "grad_norm": 1.2453442811965942,
      "learning_rate": 1.181948424068768e-05,
      "loss": 6.7635,
      "step": 535
    },
    {
      "epoch": 3.0628571428571427,
      "grad_norm": 0.7992921471595764,
      "learning_rate": 1.174785100286533e-05,
      "loss": 6.5591,
      "step": 536
    },
    {
      "epoch": 3.0685714285714285,
      "grad_norm": 0.8517186641693115,
      "learning_rate": 1.167621776504298e-05,
      "loss": 6.5094,
      "step": 537
    },
    {
      "epoch": 3.0742857142857143,
      "grad_norm": 1.1987688541412354,
      "learning_rate": 1.1604584527220632e-05,
      "loss": 6.3249,
      "step": 538
    },
    {
      "epoch": 3.08,
      "grad_norm": 2.0452730655670166,
      "learning_rate": 1.1532951289398282e-05,
      "loss": 6.2275,
      "step": 539
    },
    {
      "epoch": 3.085714285714286,
      "grad_norm": 1.0514181852340698,
      "learning_rate": 1.1461318051575932e-05,
      "loss": 6.3968,
      "step": 540
    },
    {
      "epoch": 3.0914285714285716,
      "grad_norm": 1.0207053422927856,
      "learning_rate": 1.1389684813753582e-05,
      "loss": 6.3857,
      "step": 541
    },
    {
      "epoch": 3.097142857142857,
      "grad_norm": 1.0293421745300293,
      "learning_rate": 1.1318051575931232e-05,
      "loss": 6.4083,
      "step": 542
    },
    {
      "epoch": 3.1028571428571428,
      "grad_norm": 0.8093587756156921,
      "learning_rate": 1.1246418338108883e-05,
      "loss": 6.4136,
      "step": 543
    },
    {
      "epoch": 3.1085714285714285,
      "grad_norm": 1.4156163930892944,
      "learning_rate": 1.1174785100286533e-05,
      "loss": 6.3691,
      "step": 544
    },
    {
      "epoch": 3.1142857142857143,
      "grad_norm": 0.8761957287788391,
      "learning_rate": 1.1103151862464183e-05,
      "loss": 6.5085,
      "step": 545
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.8063040971755981,
      "learning_rate": 1.1031518624641835e-05,
      "loss": 6.6247,
      "step": 546
    },
    {
      "epoch": 3.125714285714286,
      "grad_norm": 1.4337873458862305,
      "learning_rate": 1.0959885386819485e-05,
      "loss": 6.4861,
      "step": 547
    },
    {
      "epoch": 3.1314285714285712,
      "grad_norm": 0.948204755783081,
      "learning_rate": 1.0888252148997135e-05,
      "loss": 6.5047,
      "step": 548
    },
    {
      "epoch": 3.137142857142857,
      "grad_norm": 0.9827516674995422,
      "learning_rate": 1.0816618911174787e-05,
      "loss": 6.4833,
      "step": 549
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 0.976682186126709,
      "learning_rate": 1.0744985673352435e-05,
      "loss": 6.5855,
      "step": 550
    },
    {
      "epoch": 3.1485714285714286,
      "grad_norm": 0.9955751299858093,
      "learning_rate": 1.0673352435530086e-05,
      "loss": 6.5294,
      "step": 551
    },
    {
      "epoch": 3.1542857142857144,
      "grad_norm": 1.6808784008026123,
      "learning_rate": 1.0601719197707736e-05,
      "loss": 6.5033,
      "step": 552
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.9451839923858643,
      "learning_rate": 1.0530085959885386e-05,
      "loss": 6.4321,
      "step": 553
    },
    {
      "epoch": 3.1657142857142855,
      "grad_norm": 1.086367130279541,
      "learning_rate": 1.0458452722063038e-05,
      "loss": 6.4765,
      "step": 554
    },
    {
      "epoch": 3.1714285714285713,
      "grad_norm": 0.9369428753852844,
      "learning_rate": 1.0386819484240688e-05,
      "loss": 6.493,
      "step": 555
    },
    {
      "epoch": 3.177142857142857,
      "grad_norm": 0.9459913969039917,
      "learning_rate": 1.031518624641834e-05,
      "loss": 6.375,
      "step": 556
    },
    {
      "epoch": 3.182857142857143,
      "grad_norm": 1.6413941383361816,
      "learning_rate": 1.024355300859599e-05,
      "loss": 6.6146,
      "step": 557
    },
    {
      "epoch": 3.1885714285714286,
      "grad_norm": 1.65854811668396,
      "learning_rate": 1.017191977077364e-05,
      "loss": 6.4631,
      "step": 558
    },
    {
      "epoch": 3.1942857142857144,
      "grad_norm": 0.8920109272003174,
      "learning_rate": 1.0100286532951291e-05,
      "loss": 6.549,
      "step": 559
    },
    {
      "epoch": 3.2,
      "grad_norm": 1.3018380403518677,
      "learning_rate": 1.002865329512894e-05,
      "loss": 6.1627,
      "step": 560
    },
    {
      "epoch": 3.2057142857142855,
      "grad_norm": 1.0375946760177612,
      "learning_rate": 9.95702005730659e-06,
      "loss": 6.4323,
      "step": 561
    },
    {
      "epoch": 3.2114285714285713,
      "grad_norm": 1.1077619791030884,
      "learning_rate": 9.885386819484241e-06,
      "loss": 6.3823,
      "step": 562
    },
    {
      "epoch": 3.217142857142857,
      "grad_norm": 0.8312742114067078,
      "learning_rate": 9.813753581661891e-06,
      "loss": 6.4454,
      "step": 563
    },
    {
      "epoch": 3.222857142857143,
      "grad_norm": 0.9540815949440002,
      "learning_rate": 9.742120343839543e-06,
      "loss": 6.4297,
      "step": 564
    },
    {
      "epoch": 3.2285714285714286,
      "grad_norm": 0.9446364641189575,
      "learning_rate": 9.670487106017193e-06,
      "loss": 6.2438,
      "step": 565
    },
    {
      "epoch": 3.2342857142857144,
      "grad_norm": 1.0587183237075806,
      "learning_rate": 9.598853868194843e-06,
      "loss": 6.4343,
      "step": 566
    },
    {
      "epoch": 3.24,
      "grad_norm": 1.251670479774475,
      "learning_rate": 9.527220630372494e-06,
      "loss": 6.4041,
      "step": 567
    },
    {
      "epoch": 3.2457142857142856,
      "grad_norm": 0.8904223442077637,
      "learning_rate": 9.455587392550144e-06,
      "loss": 6.3023,
      "step": 568
    },
    {
      "epoch": 3.2514285714285713,
      "grad_norm": 0.881635844707489,
      "learning_rate": 9.383954154727794e-06,
      "loss": 6.3942,
      "step": 569
    },
    {
      "epoch": 3.257142857142857,
      "grad_norm": 0.9376688599586487,
      "learning_rate": 9.312320916905444e-06,
      "loss": 6.4031,
      "step": 570
    },
    {
      "epoch": 3.262857142857143,
      "grad_norm": 1.19094979763031,
      "learning_rate": 9.240687679083094e-06,
      "loss": 6.4327,
      "step": 571
    },
    {
      "epoch": 3.2685714285714287,
      "grad_norm": 1.1384581327438354,
      "learning_rate": 9.169054441260746e-06,
      "loss": 6.2413,
      "step": 572
    },
    {
      "epoch": 3.2742857142857145,
      "grad_norm": 2.0865514278411865,
      "learning_rate": 9.097421203438396e-06,
      "loss": 6.3241,
      "step": 573
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 1.332867980003357,
      "learning_rate": 9.025787965616046e-06,
      "loss": 6.7167,
      "step": 574
    },
    {
      "epoch": 3.2857142857142856,
      "grad_norm": 1.5069411993026733,
      "learning_rate": 8.954154727793697e-06,
      "loss": 6.2236,
      "step": 575
    },
    {
      "epoch": 3.2914285714285714,
      "grad_norm": 1.2494869232177734,
      "learning_rate": 8.882521489971347e-06,
      "loss": 6.5441,
      "step": 576
    },
    {
      "epoch": 3.297142857142857,
      "grad_norm": 0.9424384236335754,
      "learning_rate": 8.810888252148997e-06,
      "loss": 6.3852,
      "step": 577
    },
    {
      "epoch": 3.302857142857143,
      "grad_norm": 1.1506266593933105,
      "learning_rate": 8.739255014326649e-06,
      "loss": 6.4282,
      "step": 578
    },
    {
      "epoch": 3.3085714285714287,
      "grad_norm": 0.9956189393997192,
      "learning_rate": 8.667621776504297e-06,
      "loss": 6.5683,
      "step": 579
    },
    {
      "epoch": 3.314285714285714,
      "grad_norm": 1.3287652730941772,
      "learning_rate": 8.595988538681949e-06,
      "loss": 6.6626,
      "step": 580
    },
    {
      "epoch": 3.32,
      "grad_norm": 1.0982167720794678,
      "learning_rate": 8.524355300859599e-06,
      "loss": 6.4873,
      "step": 581
    },
    {
      "epoch": 3.3257142857142856,
      "grad_norm": 0.9635410308837891,
      "learning_rate": 8.452722063037249e-06,
      "loss": 6.3974,
      "step": 582
    },
    {
      "epoch": 3.3314285714285714,
      "grad_norm": 1.5901296138763428,
      "learning_rate": 8.3810888252149e-06,
      "loss": 6.1349,
      "step": 583
    },
    {
      "epoch": 3.337142857142857,
      "grad_norm": 1.1468819379806519,
      "learning_rate": 8.30945558739255e-06,
      "loss": 6.4542,
      "step": 584
    },
    {
      "epoch": 3.342857142857143,
      "grad_norm": 1.135256052017212,
      "learning_rate": 8.2378223495702e-06,
      "loss": 6.1953,
      "step": 585
    },
    {
      "epoch": 3.3485714285714288,
      "grad_norm": 0.993869423866272,
      "learning_rate": 8.166189111747852e-06,
      "loss": 6.2013,
      "step": 586
    },
    {
      "epoch": 3.354285714285714,
      "grad_norm": 1.6156691312789917,
      "learning_rate": 8.094555873925502e-06,
      "loss": 6.4132,
      "step": 587
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.9757428765296936,
      "learning_rate": 8.022922636103154e-06,
      "loss": 6.5022,
      "step": 588
    },
    {
      "epoch": 3.3657142857142857,
      "grad_norm": 1.0834310054779053,
      "learning_rate": 7.951289398280802e-06,
      "loss": 6.3621,
      "step": 589
    },
    {
      "epoch": 3.3714285714285714,
      "grad_norm": 0.9066211581230164,
      "learning_rate": 7.879656160458452e-06,
      "loss": 6.4472,
      "step": 590
    },
    {
      "epoch": 3.3771428571428572,
      "grad_norm": 1.6239171028137207,
      "learning_rate": 7.808022922636103e-06,
      "loss": 6.1004,
      "step": 591
    },
    {
      "epoch": 3.382857142857143,
      "grad_norm": 0.7974079251289368,
      "learning_rate": 7.736389684813753e-06,
      "loss": 6.3952,
      "step": 592
    },
    {
      "epoch": 3.388571428571429,
      "grad_norm": 1.7767813205718994,
      "learning_rate": 7.664756446991405e-06,
      "loss": 6.5451,
      "step": 593
    },
    {
      "epoch": 3.394285714285714,
      "grad_norm": 1.1430038213729858,
      "learning_rate": 7.593123209169055e-06,
      "loss": 6.4564,
      "step": 594
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.9671055674552917,
      "learning_rate": 7.521489971346706e-06,
      "loss": 6.5883,
      "step": 595
    },
    {
      "epoch": 3.4057142857142857,
      "grad_norm": 1.0275545120239258,
      "learning_rate": 7.449856733524356e-06,
      "loss": 6.3249,
      "step": 596
    },
    {
      "epoch": 3.4114285714285715,
      "grad_norm": 1.1866381168365479,
      "learning_rate": 7.378223495702007e-06,
      "loss": 6.3442,
      "step": 597
    },
    {
      "epoch": 3.4171428571428573,
      "grad_norm": 0.9294628500938416,
      "learning_rate": 7.3065902578796575e-06,
      "loss": 6.5253,
      "step": 598
    },
    {
      "epoch": 3.422857142857143,
      "grad_norm": 1.565146565437317,
      "learning_rate": 7.2349570200573066e-06,
      "loss": 6.4518,
      "step": 599
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 1.0748226642608643,
      "learning_rate": 7.1633237822349565e-06,
      "loss": 6.4268,
      "step": 600
    },
    {
      "epoch": 3.434285714285714,
      "grad_norm": 0.9523041248321533,
      "learning_rate": 7.091690544412607e-06,
      "loss": 6.6202,
      "step": 601
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.8331044316291809,
      "learning_rate": 7.020057306590258e-06,
      "loss": 6.313,
      "step": 602
    },
    {
      "epoch": 3.4457142857142857,
      "grad_norm": 1.0370538234710693,
      "learning_rate": 6.948424068767909e-06,
      "loss": 6.4304,
      "step": 603
    },
    {
      "epoch": 3.4514285714285715,
      "grad_norm": 0.9437944889068604,
      "learning_rate": 6.876790830945559e-06,
      "loss": 6.4689,
      "step": 604
    },
    {
      "epoch": 3.4571428571428573,
      "grad_norm": 0.9100984930992126,
      "learning_rate": 6.80515759312321e-06,
      "loss": 6.4748,
      "step": 605
    },
    {
      "epoch": 3.4628571428571426,
      "grad_norm": 0.9552579522132874,
      "learning_rate": 6.7335243553008605e-06,
      "loss": 6.5595,
      "step": 606
    },
    {
      "epoch": 3.4685714285714284,
      "grad_norm": 0.9809760451316833,
      "learning_rate": 6.661891117478511e-06,
      "loss": 6.3931,
      "step": 607
    },
    {
      "epoch": 3.474285714285714,
      "grad_norm": 0.8838033676147461,
      "learning_rate": 6.5902578796561604e-06,
      "loss": 6.4787,
      "step": 608
    },
    {
      "epoch": 3.48,
      "grad_norm": 1.0004992485046387,
      "learning_rate": 6.51862464183381e-06,
      "loss": 6.6363,
      "step": 609
    },
    {
      "epoch": 3.4857142857142858,
      "grad_norm": 0.9555551409721375,
      "learning_rate": 6.446991404011461e-06,
      "loss": 6.1948,
      "step": 610
    },
    {
      "epoch": 3.4914285714285715,
      "grad_norm": 1.140609622001648,
      "learning_rate": 6.375358166189112e-06,
      "loss": 6.3933,
      "step": 611
    },
    {
      "epoch": 3.4971428571428573,
      "grad_norm": 0.8988497257232666,
      "learning_rate": 6.303724928366762e-06,
      "loss": 6.5339,
      "step": 612
    },
    {
      "epoch": 3.5028571428571427,
      "grad_norm": 1.5174949169158936,
      "learning_rate": 6.232091690544413e-06,
      "loss": 6.2771,
      "step": 613
    },
    {
      "epoch": 3.5085714285714285,
      "grad_norm": 0.7727956175804138,
      "learning_rate": 6.160458452722064e-06,
      "loss": 6.4301,
      "step": 614
    },
    {
      "epoch": 3.5142857142857142,
      "grad_norm": 0.8336285352706909,
      "learning_rate": 6.088825214899714e-06,
      "loss": 6.6766,
      "step": 615
    },
    {
      "epoch": 3.52,
      "grad_norm": 1.073362112045288,
      "learning_rate": 6.017191977077364e-06,
      "loss": 6.4232,
      "step": 616
    },
    {
      "epoch": 3.525714285714286,
      "grad_norm": 1.6448904275894165,
      "learning_rate": 5.945558739255014e-06,
      "loss": 6.3332,
      "step": 617
    },
    {
      "epoch": 3.5314285714285716,
      "grad_norm": 0.8917251825332642,
      "learning_rate": 5.873925501432665e-06,
      "loss": 6.3389,
      "step": 618
    },
    {
      "epoch": 3.5371428571428574,
      "grad_norm": 0.7692221999168396,
      "learning_rate": 5.802292263610316e-06,
      "loss": 6.4873,
      "step": 619
    },
    {
      "epoch": 3.5428571428571427,
      "grad_norm": 1.5893062353134155,
      "learning_rate": 5.730659025787966e-06,
      "loss": 6.5538,
      "step": 620
    },
    {
      "epoch": 3.5485714285714285,
      "grad_norm": 1.4599310159683228,
      "learning_rate": 5.659025787965616e-06,
      "loss": 6.1633,
      "step": 621
    },
    {
      "epoch": 3.5542857142857143,
      "grad_norm": 1.112967848777771,
      "learning_rate": 5.587392550143267e-06,
      "loss": 6.3461,
      "step": 622
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.9081758856773376,
      "learning_rate": 5.5157593123209175e-06,
      "loss": 6.5363,
      "step": 623
    },
    {
      "epoch": 3.565714285714286,
      "grad_norm": 0.8683820366859436,
      "learning_rate": 5.4441260744985674e-06,
      "loss": 6.4206,
      "step": 624
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 1.0208280086517334,
      "learning_rate": 5.372492836676217e-06,
      "loss": 6.5004,
      "step": 625
    },
    {
      "epoch": 3.5771428571428574,
      "grad_norm": 1.0433721542358398,
      "learning_rate": 5.300859598853868e-06,
      "loss": 6.5954,
      "step": 626
    },
    {
      "epoch": 3.5828571428571427,
      "grad_norm": 1.2709734439849854,
      "learning_rate": 5.229226361031519e-06,
      "loss": 6.421,
      "step": 627
    },
    {
      "epoch": 3.5885714285714285,
      "grad_norm": 1.179551124572754,
      "learning_rate": 5.15759312320917e-06,
      "loss": 6.4546,
      "step": 628
    },
    {
      "epoch": 3.5942857142857143,
      "grad_norm": 0.946735680103302,
      "learning_rate": 5.08595988538682e-06,
      "loss": 6.5028,
      "step": 629
    },
    {
      "epoch": 3.6,
      "grad_norm": 1.2074192762374878,
      "learning_rate": 5.01432664756447e-06,
      "loss": 6.4933,
      "step": 630
    },
    {
      "epoch": 3.605714285714286,
      "grad_norm": 0.8730539083480835,
      "learning_rate": 4.9426934097421205e-06,
      "loss": 6.391,
      "step": 631
    },
    {
      "epoch": 3.611428571428571,
      "grad_norm": 0.9299424290657043,
      "learning_rate": 4.871060171919771e-06,
      "loss": 6.4662,
      "step": 632
    },
    {
      "epoch": 3.617142857142857,
      "grad_norm": 0.8544332981109619,
      "learning_rate": 4.799426934097421e-06,
      "loss": 6.3014,
      "step": 633
    },
    {
      "epoch": 3.6228571428571428,
      "grad_norm": 1.2015129327774048,
      "learning_rate": 4.727793696275072e-06,
      "loss": 6.3574,
      "step": 634
    },
    {
      "epoch": 3.6285714285714286,
      "grad_norm": 0.9298632144927979,
      "learning_rate": 4.656160458452722e-06,
      "loss": 6.4373,
      "step": 635
    },
    {
      "epoch": 3.6342857142857143,
      "grad_norm": 1.4077963829040527,
      "learning_rate": 4.584527220630373e-06,
      "loss": 6.3825,
      "step": 636
    },
    {
      "epoch": 3.64,
      "grad_norm": 1.270696997642517,
      "learning_rate": 4.512893982808023e-06,
      "loss": 6.4286,
      "step": 637
    },
    {
      "epoch": 3.645714285714286,
      "grad_norm": 0.8763418793678284,
      "learning_rate": 4.441260744985674e-06,
      "loss": 6.4776,
      "step": 638
    },
    {
      "epoch": 3.6514285714285712,
      "grad_norm": 0.8712356686592102,
      "learning_rate": 4.3696275071633245e-06,
      "loss": 6.376,
      "step": 639
    },
    {
      "epoch": 3.657142857142857,
      "grad_norm": 1.1169865131378174,
      "learning_rate": 4.2979942693409744e-06,
      "loss": 6.4501,
      "step": 640
    },
    {
      "epoch": 3.662857142857143,
      "grad_norm": 0.9887862205505371,
      "learning_rate": 4.226361031518624e-06,
      "loss": 6.4554,
      "step": 641
    },
    {
      "epoch": 3.6685714285714286,
      "grad_norm": 1.2047755718231201,
      "learning_rate": 4.154727793696275e-06,
      "loss": 6.6952,
      "step": 642
    },
    {
      "epoch": 3.6742857142857144,
      "grad_norm": 0.9493082165718079,
      "learning_rate": 4.083094555873926e-06,
      "loss": 6.5038,
      "step": 643
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.9289478063583374,
      "learning_rate": 4.011461318051577e-06,
      "loss": 6.4883,
      "step": 644
    },
    {
      "epoch": 3.685714285714286,
      "grad_norm": 1.3705198764801025,
      "learning_rate": 3.939828080229226e-06,
      "loss": 6.5397,
      "step": 645
    },
    {
      "epoch": 3.6914285714285713,
      "grad_norm": 1.5849549770355225,
      "learning_rate": 3.868194842406877e-06,
      "loss": 6.4769,
      "step": 646
    },
    {
      "epoch": 3.697142857142857,
      "grad_norm": 1.0378841161727905,
      "learning_rate": 3.7965616045845275e-06,
      "loss": 6.3658,
      "step": 647
    },
    {
      "epoch": 3.702857142857143,
      "grad_norm": 0.8181402683258057,
      "learning_rate": 3.724928366762178e-06,
      "loss": 6.3862,
      "step": 648
    },
    {
      "epoch": 3.7085714285714286,
      "grad_norm": 1.1784873008728027,
      "learning_rate": 3.6532951289398287e-06,
      "loss": 6.4644,
      "step": 649
    },
    {
      "epoch": 3.7142857142857144,
      "grad_norm": 0.9645995497703552,
      "learning_rate": 3.5816618911174783e-06,
      "loss": 6.3843,
      "step": 650
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 0.8308417201042175,
      "learning_rate": 3.510028653295129e-06,
      "loss": 6.5486,
      "step": 651
    },
    {
      "epoch": 3.725714285714286,
      "grad_norm": 0.911817729473114,
      "learning_rate": 3.4383954154727795e-06,
      "loss": 6.4658,
      "step": 652
    },
    {
      "epoch": 3.7314285714285713,
      "grad_norm": 0.951978862285614,
      "learning_rate": 3.3667621776504303e-06,
      "loss": 6.3526,
      "step": 653
    },
    {
      "epoch": 3.737142857142857,
      "grad_norm": 1.42562735080719,
      "learning_rate": 3.2951289398280802e-06,
      "loss": 6.4771,
      "step": 654
    },
    {
      "epoch": 3.742857142857143,
      "grad_norm": 1.1700232028961182,
      "learning_rate": 3.2234957020057306e-06,
      "loss": 6.3813,
      "step": 655
    },
    {
      "epoch": 3.7485714285714287,
      "grad_norm": 1.3200175762176514,
      "learning_rate": 3.151862464183381e-06,
      "loss": 6.526,
      "step": 656
    },
    {
      "epoch": 3.7542857142857144,
      "grad_norm": 1.3409428596496582,
      "learning_rate": 3.080229226361032e-06,
      "loss": 6.3005,
      "step": 657
    },
    {
      "epoch": 3.76,
      "grad_norm": 1.2824711799621582,
      "learning_rate": 3.008595988538682e-06,
      "loss": 6.3948,
      "step": 658
    },
    {
      "epoch": 3.7657142857142856,
      "grad_norm": 1.1156233549118042,
      "learning_rate": 2.9369627507163326e-06,
      "loss": 6.3808,
      "step": 659
    },
    {
      "epoch": 3.7714285714285714,
      "grad_norm": 0.7807052135467529,
      "learning_rate": 2.865329512893983e-06,
      "loss": 6.4392,
      "step": 660
    },
    {
      "epoch": 3.777142857142857,
      "grad_norm": 0.9158913493156433,
      "learning_rate": 2.7936962750716333e-06,
      "loss": 6.3589,
      "step": 661
    },
    {
      "epoch": 3.782857142857143,
      "grad_norm": 1.0496058464050293,
      "learning_rate": 2.7220630372492837e-06,
      "loss": 6.4827,
      "step": 662
    },
    {
      "epoch": 3.7885714285714287,
      "grad_norm": 1.10104238986969,
      "learning_rate": 2.650429799426934e-06,
      "loss": 6.4049,
      "step": 663
    },
    {
      "epoch": 3.7942857142857145,
      "grad_norm": 0.9167174696922302,
      "learning_rate": 2.578796561604585e-06,
      "loss": 6.4161,
      "step": 664
    },
    {
      "epoch": 3.8,
      "grad_norm": 1.0269542932510376,
      "learning_rate": 2.507163323782235e-06,
      "loss": 6.3381,
      "step": 665
    },
    {
      "epoch": 3.8057142857142856,
      "grad_norm": 1.0368422269821167,
      "learning_rate": 2.4355300859598857e-06,
      "loss": 6.4809,
      "step": 666
    },
    {
      "epoch": 3.8114285714285714,
      "grad_norm": 1.0033549070358276,
      "learning_rate": 2.363896848137536e-06,
      "loss": 6.2784,
      "step": 667
    },
    {
      "epoch": 3.817142857142857,
      "grad_norm": 1.2146466970443726,
      "learning_rate": 2.2922636103151864e-06,
      "loss": 6.1709,
      "step": 668
    },
    {
      "epoch": 3.822857142857143,
      "grad_norm": 0.9820989370346069,
      "learning_rate": 2.220630372492837e-06,
      "loss": 6.5697,
      "step": 669
    },
    {
      "epoch": 3.8285714285714287,
      "grad_norm": 0.8025502562522888,
      "learning_rate": 2.1489971346704872e-06,
      "loss": 6.479,
      "step": 670
    },
    {
      "epoch": 3.8342857142857145,
      "grad_norm": 2.2852041721343994,
      "learning_rate": 2.0773638968481376e-06,
      "loss": 6.553,
      "step": 671
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.9263201355934143,
      "learning_rate": 2.0057306590257884e-06,
      "loss": 6.6411,
      "step": 672
    },
    {
      "epoch": 3.8457142857142856,
      "grad_norm": 1.1030559539794922,
      "learning_rate": 1.9340974212034384e-06,
      "loss": 6.4811,
      "step": 673
    },
    {
      "epoch": 3.8514285714285714,
      "grad_norm": 1.1798193454742432,
      "learning_rate": 1.862464183381089e-06,
      "loss": 6.3849,
      "step": 674
    },
    {
      "epoch": 3.857142857142857,
      "grad_norm": 1.0951348543167114,
      "learning_rate": 1.7908309455587391e-06,
      "loss": 6.5122,
      "step": 675
    },
    {
      "epoch": 3.862857142857143,
      "grad_norm": 0.8795127272605896,
      "learning_rate": 1.7191977077363897e-06,
      "loss": 6.3574,
      "step": 676
    },
    {
      "epoch": 3.8685714285714283,
      "grad_norm": 1.1240830421447754,
      "learning_rate": 1.6475644699140401e-06,
      "loss": 6.2887,
      "step": 677
    },
    {
      "epoch": 3.8742857142857146,
      "grad_norm": 1.3597445487976074,
      "learning_rate": 1.5759312320916905e-06,
      "loss": 6.5888,
      "step": 678
    },
    {
      "epoch": 3.88,
      "grad_norm": 1.041807770729065,
      "learning_rate": 1.504297994269341e-06,
      "loss": 6.4472,
      "step": 679
    },
    {
      "epoch": 3.8857142857142857,
      "grad_norm": 0.9484853744506836,
      "learning_rate": 1.4326647564469915e-06,
      "loss": 6.4868,
      "step": 680
    },
    {
      "epoch": 3.8914285714285715,
      "grad_norm": 1.106340765953064,
      "learning_rate": 1.3610315186246419e-06,
      "loss": 6.29,
      "step": 681
    },
    {
      "epoch": 3.8971428571428572,
      "grad_norm": 0.9408600330352783,
      "learning_rate": 1.2893982808022925e-06,
      "loss": 6.2998,
      "step": 682
    },
    {
      "epoch": 3.902857142857143,
      "grad_norm": 0.9641872644424438,
      "learning_rate": 1.2177650429799428e-06,
      "loss": 6.3845,
      "step": 683
    },
    {
      "epoch": 3.9085714285714284,
      "grad_norm": 2.433267116546631,
      "learning_rate": 1.1461318051575932e-06,
      "loss": 6.3425,
      "step": 684
    },
    {
      "epoch": 3.914285714285714,
      "grad_norm": 1.1918138265609741,
      "learning_rate": 1.0744985673352436e-06,
      "loss": 6.3168,
      "step": 685
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.8932953476905823,
      "learning_rate": 1.0028653295128942e-06,
      "loss": 6.5264,
      "step": 686
    },
    {
      "epoch": 3.9257142857142857,
      "grad_norm": 1.2493680715560913,
      "learning_rate": 9.312320916905445e-07,
      "loss": 6.5017,
      "step": 687
    },
    {
      "epoch": 3.9314285714285715,
      "grad_norm": 1.8519610166549683,
      "learning_rate": 8.595988538681949e-07,
      "loss": 6.1352,
      "step": 688
    },
    {
      "epoch": 3.9371428571428573,
      "grad_norm": 0.9182230234146118,
      "learning_rate": 7.879656160458452e-07,
      "loss": 6.3032,
      "step": 689
    },
    {
      "epoch": 3.942857142857143,
      "grad_norm": 1.0489720106124878,
      "learning_rate": 7.163323782234957e-07,
      "loss": 6.6251,
      "step": 690
    },
    {
      "epoch": 3.9485714285714284,
      "grad_norm": 0.9060836434364319,
      "learning_rate": 6.446991404011462e-07,
      "loss": 6.4843,
      "step": 691
    },
    {
      "epoch": 3.954285714285714,
      "grad_norm": 0.8958390355110168,
      "learning_rate": 5.730659025787966e-07,
      "loss": 6.5021,
      "step": 692
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.8874601721763611,
      "learning_rate": 5.014326647564471e-07,
      "loss": 6.5173,
      "step": 693
    },
    {
      "epoch": 3.9657142857142857,
      "grad_norm": 1.0577969551086426,
      "learning_rate": 4.2979942693409743e-07,
      "loss": 6.3953,
      "step": 694
    },
    {
      "epoch": 3.9714285714285715,
      "grad_norm": 1.037325382232666,
      "learning_rate": 3.5816618911174787e-07,
      "loss": 6.4908,
      "step": 695
    },
    {
      "epoch": 3.977142857142857,
      "grad_norm": 0.9087362885475159,
      "learning_rate": 2.865329512893983e-07,
      "loss": 6.4119,
      "step": 696
    },
    {
      "epoch": 3.982857142857143,
      "grad_norm": 1.0680299997329712,
      "learning_rate": 2.1489971346704872e-07,
      "loss": 6.4627,
      "step": 697
    },
    {
      "epoch": 3.9885714285714284,
      "grad_norm": 1.05901038646698,
      "learning_rate": 1.4326647564469915e-07,
      "loss": 6.5431,
      "step": 698
    },
    {
      "epoch": 3.994285714285714,
      "grad_norm": 1.1798481941223145,
      "learning_rate": 7.163323782234958e-08,
      "loss": 6.5795,
      "step": 699
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.0650255680084229,
      "learning_rate": 0.0,
      "loss": 6.5217,
      "step": 700
    }
  ],
  "logging_steps": 1,
  "max_steps": 700,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1846484140032000.0,
  "train_batch_size": 80,
  "trial_name": null,
  "trial_params": null
}
