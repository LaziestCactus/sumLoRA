{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 146,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00684931506849315,
      "grad_norm": 4.3563313484191895,
      "learning_rate": 2.5e-05,
      "loss": 13.9165,
      "step": 1
    },
    {
      "epoch": 0.0136986301369863,
      "grad_norm": 4.756106853485107,
      "learning_rate": 5e-05,
      "loss": 14.1377,
      "step": 2
    },
    {
      "epoch": 0.02054794520547945,
      "grad_norm": 4.620753765106201,
      "learning_rate": 4.965277777777778e-05,
      "loss": 13.9125,
      "step": 3
    },
    {
      "epoch": 0.0273972602739726,
      "grad_norm": 4.184190273284912,
      "learning_rate": 4.930555555555556e-05,
      "loss": 13.6601,
      "step": 4
    },
    {
      "epoch": 0.03424657534246575,
      "grad_norm": 4.694354057312012,
      "learning_rate": 4.8958333333333335e-05,
      "loss": 14.0201,
      "step": 5
    },
    {
      "epoch": 0.0410958904109589,
      "grad_norm": 4.803966522216797,
      "learning_rate": 4.8611111111111115e-05,
      "loss": 14.1708,
      "step": 6
    },
    {
      "epoch": 0.04794520547945205,
      "grad_norm": 4.941484451293945,
      "learning_rate": 4.8263888888888895e-05,
      "loss": 13.9082,
      "step": 7
    },
    {
      "epoch": 0.0547945205479452,
      "grad_norm": 5.0602922439575195,
      "learning_rate": 4.791666666666667e-05,
      "loss": 13.6848,
      "step": 8
    },
    {
      "epoch": 0.06164383561643835,
      "grad_norm": 5.360780239105225,
      "learning_rate": 4.756944444444444e-05,
      "loss": 13.5881,
      "step": 9
    },
    {
      "epoch": 0.0684931506849315,
      "grad_norm": 5.76248025894165,
      "learning_rate": 4.722222222222222e-05,
      "loss": 13.746,
      "step": 10
    },
    {
      "epoch": 0.07534246575342465,
      "grad_norm": 6.081969738006592,
      "learning_rate": 4.6875e-05,
      "loss": 13.4216,
      "step": 11
    },
    {
      "epoch": 0.0821917808219178,
      "grad_norm": 6.10832405090332,
      "learning_rate": 4.652777777777778e-05,
      "loss": 13.2742,
      "step": 12
    },
    {
      "epoch": 0.08904109589041095,
      "grad_norm": 6.2087483406066895,
      "learning_rate": 4.618055555555556e-05,
      "loss": 13.1206,
      "step": 13
    },
    {
      "epoch": 0.0958904109589041,
      "grad_norm": 6.431140422821045,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 12.807,
      "step": 14
    },
    {
      "epoch": 0.10273972602739725,
      "grad_norm": 7.286914825439453,
      "learning_rate": 4.5486111111111114e-05,
      "loss": 12.9723,
      "step": 15
    },
    {
      "epoch": 0.1095890410958904,
      "grad_norm": 7.428272247314453,
      "learning_rate": 4.5138888888888894e-05,
      "loss": 12.6418,
      "step": 16
    },
    {
      "epoch": 0.11643835616438356,
      "grad_norm": 7.791341304779053,
      "learning_rate": 4.4791666666666673e-05,
      "loss": 12.8957,
      "step": 17
    },
    {
      "epoch": 0.1232876712328767,
      "grad_norm": 8.737076759338379,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 12.3553,
      "step": 18
    },
    {
      "epoch": 0.13013698630136986,
      "grad_norm": 8.292170524597168,
      "learning_rate": 4.4097222222222226e-05,
      "loss": 12.3123,
      "step": 19
    },
    {
      "epoch": 0.136986301369863,
      "grad_norm": 8.876721382141113,
      "learning_rate": 4.375e-05,
      "loss": 12.2319,
      "step": 20
    },
    {
      "epoch": 0.14383561643835616,
      "grad_norm": 8.913549423217773,
      "learning_rate": 4.340277777777778e-05,
      "loss": 11.813,
      "step": 21
    },
    {
      "epoch": 0.1506849315068493,
      "grad_norm": 9.479351043701172,
      "learning_rate": 4.305555555555556e-05,
      "loss": 11.761,
      "step": 22
    },
    {
      "epoch": 0.15753424657534246,
      "grad_norm": 9.98810863494873,
      "learning_rate": 4.270833333333333e-05,
      "loss": 11.6372,
      "step": 23
    },
    {
      "epoch": 0.1643835616438356,
      "grad_norm": 9.816370010375977,
      "learning_rate": 4.236111111111111e-05,
      "loss": 11.5772,
      "step": 24
    },
    {
      "epoch": 0.17123287671232876,
      "grad_norm": 10.198304176330566,
      "learning_rate": 4.201388888888889e-05,
      "loss": 11.3241,
      "step": 25
    },
    {
      "epoch": 0.1780821917808219,
      "grad_norm": 10.472955703735352,
      "learning_rate": 4.166666666666667e-05,
      "loss": 11.1301,
      "step": 26
    },
    {
      "epoch": 0.18493150684931506,
      "grad_norm": 10.46716594696045,
      "learning_rate": 4.1319444444444445e-05,
      "loss": 10.7967,
      "step": 27
    },
    {
      "epoch": 0.1917808219178082,
      "grad_norm": 10.359930992126465,
      "learning_rate": 4.0972222222222225e-05,
      "loss": 10.5881,
      "step": 28
    },
    {
      "epoch": 0.19863013698630136,
      "grad_norm": 10.85799789428711,
      "learning_rate": 4.0625000000000005e-05,
      "loss": 10.6589,
      "step": 29
    },
    {
      "epoch": 0.2054794520547945,
      "grad_norm": 10.354226112365723,
      "learning_rate": 4.027777777777778e-05,
      "loss": 10.1375,
      "step": 30
    },
    {
      "epoch": 0.21232876712328766,
      "grad_norm": 10.34731674194336,
      "learning_rate": 3.993055555555556e-05,
      "loss": 10.0222,
      "step": 31
    },
    {
      "epoch": 0.2191780821917808,
      "grad_norm": 10.587614059448242,
      "learning_rate": 3.958333333333333e-05,
      "loss": 10.1262,
      "step": 32
    },
    {
      "epoch": 0.22602739726027396,
      "grad_norm": 9.894378662109375,
      "learning_rate": 3.923611111111111e-05,
      "loss": 9.7024,
      "step": 33
    },
    {
      "epoch": 0.2328767123287671,
      "grad_norm": 10.013019561767578,
      "learning_rate": 3.888888888888889e-05,
      "loss": 9.6976,
      "step": 34
    },
    {
      "epoch": 0.23972602739726026,
      "grad_norm": 8.862292289733887,
      "learning_rate": 3.854166666666667e-05,
      "loss": 9.4166,
      "step": 35
    },
    {
      "epoch": 0.2465753424657534,
      "grad_norm": 8.226386070251465,
      "learning_rate": 3.8194444444444444e-05,
      "loss": 9.3345,
      "step": 36
    },
    {
      "epoch": 0.2534246575342466,
      "grad_norm": 7.758465766906738,
      "learning_rate": 3.7847222222222224e-05,
      "loss": 9.3195,
      "step": 37
    },
    {
      "epoch": 0.2602739726027397,
      "grad_norm": 7.012455463409424,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 9.0105,
      "step": 38
    },
    {
      "epoch": 0.2671232876712329,
      "grad_norm": 6.318299293518066,
      "learning_rate": 3.715277777777778e-05,
      "loss": 8.9095,
      "step": 39
    },
    {
      "epoch": 0.273972602739726,
      "grad_norm": 5.923127174377441,
      "learning_rate": 3.6805555555555556e-05,
      "loss": 8.8644,
      "step": 40
    },
    {
      "epoch": 0.2808219178082192,
      "grad_norm": 5.225088596343994,
      "learning_rate": 3.6458333333333336e-05,
      "loss": 8.6812,
      "step": 41
    },
    {
      "epoch": 0.2876712328767123,
      "grad_norm": 4.2572550773620605,
      "learning_rate": 3.611111111111111e-05,
      "loss": 8.525,
      "step": 42
    },
    {
      "epoch": 0.2945205479452055,
      "grad_norm": 4.1656107902526855,
      "learning_rate": 3.576388888888889e-05,
      "loss": 8.6621,
      "step": 43
    },
    {
      "epoch": 0.3013698630136986,
      "grad_norm": 3.701079845428467,
      "learning_rate": 3.541666666666667e-05,
      "loss": 8.5385,
      "step": 44
    },
    {
      "epoch": 0.3082191780821918,
      "grad_norm": 3.260115146636963,
      "learning_rate": 3.506944444444444e-05,
      "loss": 8.464,
      "step": 45
    },
    {
      "epoch": 0.3150684931506849,
      "grad_norm": 3.2593159675598145,
      "learning_rate": 3.472222222222222e-05,
      "loss": 8.4217,
      "step": 46
    },
    {
      "epoch": 0.3219178082191781,
      "grad_norm": 2.863346576690674,
      "learning_rate": 3.4375e-05,
      "loss": 8.4607,
      "step": 47
    },
    {
      "epoch": 0.3287671232876712,
      "grad_norm": 2.6119816303253174,
      "learning_rate": 3.402777777777778e-05,
      "loss": 8.495,
      "step": 48
    },
    {
      "epoch": 0.3356164383561644,
      "grad_norm": 2.3732943534851074,
      "learning_rate": 3.368055555555556e-05,
      "loss": 8.2611,
      "step": 49
    },
    {
      "epoch": 0.3424657534246575,
      "grad_norm": 2.4797558784484863,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 8.2335,
      "step": 50
    },
    {
      "epoch": 0.3493150684931507,
      "grad_norm": 2.322079658508301,
      "learning_rate": 3.2986111111111115e-05,
      "loss": 8.1899,
      "step": 51
    },
    {
      "epoch": 0.3561643835616438,
      "grad_norm": 2.3552439212799072,
      "learning_rate": 3.263888888888889e-05,
      "loss": 8.1265,
      "step": 52
    },
    {
      "epoch": 0.363013698630137,
      "grad_norm": 2.0652854442596436,
      "learning_rate": 3.229166666666667e-05,
      "loss": 8.1525,
      "step": 53
    },
    {
      "epoch": 0.3698630136986301,
      "grad_norm": 2.054536819458008,
      "learning_rate": 3.194444444444444e-05,
      "loss": 8.0497,
      "step": 54
    },
    {
      "epoch": 0.3767123287671233,
      "grad_norm": 2.116554021835327,
      "learning_rate": 3.159722222222222e-05,
      "loss": 8.0497,
      "step": 55
    },
    {
      "epoch": 0.3835616438356164,
      "grad_norm": 2.0548195838928223,
      "learning_rate": 3.125e-05,
      "loss": 8.1285,
      "step": 56
    },
    {
      "epoch": 0.3904109589041096,
      "grad_norm": 1.897540807723999,
      "learning_rate": 3.090277777777778e-05,
      "loss": 7.9613,
      "step": 57
    },
    {
      "epoch": 0.3972602739726027,
      "grad_norm": 1.8805158138275146,
      "learning_rate": 3.055555555555556e-05,
      "loss": 7.9999,
      "step": 58
    },
    {
      "epoch": 0.4041095890410959,
      "grad_norm": 1.8977915048599243,
      "learning_rate": 3.0208333333333334e-05,
      "loss": 7.9907,
      "step": 59
    },
    {
      "epoch": 0.410958904109589,
      "grad_norm": 1.9287253618240356,
      "learning_rate": 2.9861111111111113e-05,
      "loss": 7.934,
      "step": 60
    },
    {
      "epoch": 0.4178082191780822,
      "grad_norm": 1.8145076036453247,
      "learning_rate": 2.951388888888889e-05,
      "loss": 7.8843,
      "step": 61
    },
    {
      "epoch": 0.4246575342465753,
      "grad_norm": 1.757500410079956,
      "learning_rate": 2.916666666666667e-05,
      "loss": 7.9475,
      "step": 62
    },
    {
      "epoch": 0.4315068493150685,
      "grad_norm": 1.8015562295913696,
      "learning_rate": 2.8819444444444443e-05,
      "loss": 7.8379,
      "step": 63
    },
    {
      "epoch": 0.4383561643835616,
      "grad_norm": 1.7638087272644043,
      "learning_rate": 2.8472222222222223e-05,
      "loss": 7.9014,
      "step": 64
    },
    {
      "epoch": 0.4452054794520548,
      "grad_norm": 1.648460030555725,
      "learning_rate": 2.8125000000000003e-05,
      "loss": 7.8326,
      "step": 65
    },
    {
      "epoch": 0.4520547945205479,
      "grad_norm": 1.6540504693984985,
      "learning_rate": 2.777777777777778e-05,
      "loss": 7.7688,
      "step": 66
    },
    {
      "epoch": 0.4589041095890411,
      "grad_norm": 1.6286065578460693,
      "learning_rate": 2.743055555555556e-05,
      "loss": 7.8308,
      "step": 67
    },
    {
      "epoch": 0.4657534246575342,
      "grad_norm": 1.7212213277816772,
      "learning_rate": 2.7083333333333332e-05,
      "loss": 7.9298,
      "step": 68
    },
    {
      "epoch": 0.4726027397260274,
      "grad_norm": 1.6341460943222046,
      "learning_rate": 2.6736111111111112e-05,
      "loss": 7.8672,
      "step": 69
    },
    {
      "epoch": 0.4794520547945205,
      "grad_norm": 1.5886346101760864,
      "learning_rate": 2.6388888888888892e-05,
      "loss": 7.7485,
      "step": 70
    },
    {
      "epoch": 0.4863013698630137,
      "grad_norm": 1.6367560625076294,
      "learning_rate": 2.604166666666667e-05,
      "loss": 7.6538,
      "step": 71
    },
    {
      "epoch": 0.4931506849315068,
      "grad_norm": 1.597127079963684,
      "learning_rate": 2.5694444444444445e-05,
      "loss": 7.7177,
      "step": 72
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.658076524734497,
      "learning_rate": 2.534722222222222e-05,
      "loss": 7.6478,
      "step": 73
    },
    {
      "epoch": 0.5068493150684932,
      "grad_norm": 1.4313373565673828,
      "learning_rate": 2.5e-05,
      "loss": 7.717,
      "step": 74
    },
    {
      "epoch": 0.5136986301369864,
      "grad_norm": 1.4968879222869873,
      "learning_rate": 2.465277777777778e-05,
      "loss": 7.7012,
      "step": 75
    },
    {
      "epoch": 0.5205479452054794,
      "grad_norm": 1.425454020500183,
      "learning_rate": 2.4305555555555558e-05,
      "loss": 7.739,
      "step": 76
    },
    {
      "epoch": 0.5273972602739726,
      "grad_norm": 1.5728540420532227,
      "learning_rate": 2.3958333333333334e-05,
      "loss": 7.6605,
      "step": 77
    },
    {
      "epoch": 0.5342465753424658,
      "grad_norm": 1.6973985433578491,
      "learning_rate": 2.361111111111111e-05,
      "loss": 7.6759,
      "step": 78
    },
    {
      "epoch": 0.541095890410959,
      "grad_norm": 1.4710400104522705,
      "learning_rate": 2.326388888888889e-05,
      "loss": 7.6443,
      "step": 79
    },
    {
      "epoch": 0.547945205479452,
      "grad_norm": 1.6086621284484863,
      "learning_rate": 2.2916666666666667e-05,
      "loss": 7.6298,
      "step": 80
    },
    {
      "epoch": 0.5547945205479452,
      "grad_norm": 1.392512559890747,
      "learning_rate": 2.2569444444444447e-05,
      "loss": 7.6871,
      "step": 81
    },
    {
      "epoch": 0.5616438356164384,
      "grad_norm": 1.400452971458435,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 7.6865,
      "step": 82
    },
    {
      "epoch": 0.5684931506849316,
      "grad_norm": 1.4195852279663086,
      "learning_rate": 2.1875e-05,
      "loss": 7.6468,
      "step": 83
    },
    {
      "epoch": 0.5753424657534246,
      "grad_norm": 1.3400036096572876,
      "learning_rate": 2.152777777777778e-05,
      "loss": 7.5581,
      "step": 84
    },
    {
      "epoch": 0.5821917808219178,
      "grad_norm": 1.2284550666809082,
      "learning_rate": 2.1180555555555556e-05,
      "loss": 7.6455,
      "step": 85
    },
    {
      "epoch": 0.589041095890411,
      "grad_norm": 1.409593105316162,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 7.5268,
      "step": 86
    },
    {
      "epoch": 0.5958904109589042,
      "grad_norm": 1.4494010210037231,
      "learning_rate": 2.0486111111111113e-05,
      "loss": 7.6667,
      "step": 87
    },
    {
      "epoch": 0.6027397260273972,
      "grad_norm": 1.6357531547546387,
      "learning_rate": 2.013888888888889e-05,
      "loss": 7.5449,
      "step": 88
    },
    {
      "epoch": 0.6095890410958904,
      "grad_norm": 1.3820574283599854,
      "learning_rate": 1.9791666666666665e-05,
      "loss": 7.4785,
      "step": 89
    },
    {
      "epoch": 0.6164383561643836,
      "grad_norm": 1.3144266605377197,
      "learning_rate": 1.9444444444444445e-05,
      "loss": 7.4254,
      "step": 90
    },
    {
      "epoch": 0.6232876712328768,
      "grad_norm": 1.3072339296340942,
      "learning_rate": 1.9097222222222222e-05,
      "loss": 7.5085,
      "step": 91
    },
    {
      "epoch": 0.6301369863013698,
      "grad_norm": 1.1499844789505005,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 7.3836,
      "step": 92
    },
    {
      "epoch": 0.636986301369863,
      "grad_norm": 1.4164808988571167,
      "learning_rate": 1.8402777777777778e-05,
      "loss": 7.3828,
      "step": 93
    },
    {
      "epoch": 0.6438356164383562,
      "grad_norm": 1.4878038167953491,
      "learning_rate": 1.8055555555555555e-05,
      "loss": 7.3018,
      "step": 94
    },
    {
      "epoch": 0.6506849315068494,
      "grad_norm": 1.3035470247268677,
      "learning_rate": 1.7708333333333335e-05,
      "loss": 7.4017,
      "step": 95
    },
    {
      "epoch": 0.6575342465753424,
      "grad_norm": 1.196690559387207,
      "learning_rate": 1.736111111111111e-05,
      "loss": 7.4354,
      "step": 96
    },
    {
      "epoch": 0.6643835616438356,
      "grad_norm": 1.4378324747085571,
      "learning_rate": 1.701388888888889e-05,
      "loss": 7.219,
      "step": 97
    },
    {
      "epoch": 0.6712328767123288,
      "grad_norm": 1.301684856414795,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 7.3174,
      "step": 98
    },
    {
      "epoch": 0.678082191780822,
      "grad_norm": 1.2290838956832886,
      "learning_rate": 1.6319444444444444e-05,
      "loss": 7.4137,
      "step": 99
    },
    {
      "epoch": 0.684931506849315,
      "grad_norm": 1.3160234689712524,
      "learning_rate": 1.597222222222222e-05,
      "loss": 7.173,
      "step": 100
    },
    {
      "epoch": 0.6917808219178082,
      "grad_norm": 1.180755376815796,
      "learning_rate": 1.5625e-05,
      "loss": 7.3443,
      "step": 101
    },
    {
      "epoch": 0.6986301369863014,
      "grad_norm": 1.1342980861663818,
      "learning_rate": 1.527777777777778e-05,
      "loss": 7.5865,
      "step": 102
    },
    {
      "epoch": 0.7054794520547946,
      "grad_norm": 1.1676487922668457,
      "learning_rate": 1.4930555555555557e-05,
      "loss": 7.3757,
      "step": 103
    },
    {
      "epoch": 0.7123287671232876,
      "grad_norm": 1.2072261571884155,
      "learning_rate": 1.4583333333333335e-05,
      "loss": 7.2568,
      "step": 104
    },
    {
      "epoch": 0.7191780821917808,
      "grad_norm": 1.2423783540725708,
      "learning_rate": 1.4236111111111111e-05,
      "loss": 7.2978,
      "step": 105
    },
    {
      "epoch": 0.726027397260274,
      "grad_norm": 1.2167723178863525,
      "learning_rate": 1.388888888888889e-05,
      "loss": 7.234,
      "step": 106
    },
    {
      "epoch": 0.7328767123287672,
      "grad_norm": 1.2438873052597046,
      "learning_rate": 1.3541666666666666e-05,
      "loss": 7.2944,
      "step": 107
    },
    {
      "epoch": 0.7397260273972602,
      "grad_norm": 1.666400671005249,
      "learning_rate": 1.3194444444444446e-05,
      "loss": 7.0678,
      "step": 108
    },
    {
      "epoch": 0.7465753424657534,
      "grad_norm": 1.100693941116333,
      "learning_rate": 1.2847222222222222e-05,
      "loss": 7.3508,
      "step": 109
    },
    {
      "epoch": 0.7534246575342466,
      "grad_norm": 1.268784761428833,
      "learning_rate": 1.25e-05,
      "loss": 7.2921,
      "step": 110
    },
    {
      "epoch": 0.7602739726027398,
      "grad_norm": 1.100313425064087,
      "learning_rate": 1.2152777777777779e-05,
      "loss": 7.331,
      "step": 111
    },
    {
      "epoch": 0.7671232876712328,
      "grad_norm": 1.0670584440231323,
      "learning_rate": 1.1805555555555555e-05,
      "loss": 7.2528,
      "step": 112
    },
    {
      "epoch": 0.773972602739726,
      "grad_norm": 1.1895264387130737,
      "learning_rate": 1.1458333333333333e-05,
      "loss": 7.1923,
      "step": 113
    },
    {
      "epoch": 0.7808219178082192,
      "grad_norm": 1.2020260095596313,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 7.1967,
      "step": 114
    },
    {
      "epoch": 0.7876712328767124,
      "grad_norm": 1.4174565076828003,
      "learning_rate": 1.076388888888889e-05,
      "loss": 7.3053,
      "step": 115
    },
    {
      "epoch": 0.7945205479452054,
      "grad_norm": 1.1808582544326782,
      "learning_rate": 1.0416666666666668e-05,
      "loss": 7.1986,
      "step": 116
    },
    {
      "epoch": 0.8013698630136986,
      "grad_norm": 1.1117831468582153,
      "learning_rate": 1.0069444444444445e-05,
      "loss": 7.2308,
      "step": 117
    },
    {
      "epoch": 0.8082191780821918,
      "grad_norm": 1.1833468675613403,
      "learning_rate": 9.722222222222223e-06,
      "loss": 7.1843,
      "step": 118
    },
    {
      "epoch": 0.815068493150685,
      "grad_norm": 1.101541519165039,
      "learning_rate": 9.375000000000001e-06,
      "loss": 7.1852,
      "step": 119
    },
    {
      "epoch": 0.821917808219178,
      "grad_norm": 1.0875705480575562,
      "learning_rate": 9.027777777777777e-06,
      "loss": 7.3465,
      "step": 120
    },
    {
      "epoch": 0.8287671232876712,
      "grad_norm": 1.1226855516433716,
      "learning_rate": 8.680555555555556e-06,
      "loss": 7.3135,
      "step": 121
    },
    {
      "epoch": 0.8356164383561644,
      "grad_norm": 1.1946905851364136,
      "learning_rate": 8.333333333333334e-06,
      "loss": 7.2291,
      "step": 122
    },
    {
      "epoch": 0.8424657534246576,
      "grad_norm": 1.327041506767273,
      "learning_rate": 7.98611111111111e-06,
      "loss": 7.126,
      "step": 123
    },
    {
      "epoch": 0.8493150684931506,
      "grad_norm": 1.3329466581344604,
      "learning_rate": 7.63888888888889e-06,
      "loss": 7.1007,
      "step": 124
    },
    {
      "epoch": 0.8561643835616438,
      "grad_norm": 1.0668026208877563,
      "learning_rate": 7.2916666666666674e-06,
      "loss": 7.2328,
      "step": 125
    },
    {
      "epoch": 0.863013698630137,
      "grad_norm": 1.2707802057266235,
      "learning_rate": 6.944444444444445e-06,
      "loss": 7.1253,
      "step": 126
    },
    {
      "epoch": 0.8698630136986302,
      "grad_norm": 1.1646876335144043,
      "learning_rate": 6.597222222222223e-06,
      "loss": 7.0981,
      "step": 127
    },
    {
      "epoch": 0.8767123287671232,
      "grad_norm": 1.168853521347046,
      "learning_rate": 6.25e-06,
      "loss": 7.1744,
      "step": 128
    },
    {
      "epoch": 0.8835616438356164,
      "grad_norm": 1.3181095123291016,
      "learning_rate": 5.902777777777778e-06,
      "loss": 7.2279,
      "step": 129
    },
    {
      "epoch": 0.8904109589041096,
      "grad_norm": 1.1341670751571655,
      "learning_rate": 5.555555555555556e-06,
      "loss": 7.1599,
      "step": 130
    },
    {
      "epoch": 0.8972602739726028,
      "grad_norm": 1.0417979955673218,
      "learning_rate": 5.208333333333334e-06,
      "loss": 7.2313,
      "step": 131
    },
    {
      "epoch": 0.9041095890410958,
      "grad_norm": 1.1587886810302734,
      "learning_rate": 4.861111111111111e-06,
      "loss": 7.2835,
      "step": 132
    },
    {
      "epoch": 0.910958904109589,
      "grad_norm": 1.248581886291504,
      "learning_rate": 4.513888888888889e-06,
      "loss": 7.148,
      "step": 133
    },
    {
      "epoch": 0.9178082191780822,
      "grad_norm": 1.0810590982437134,
      "learning_rate": 4.166666666666667e-06,
      "loss": 7.2113,
      "step": 134
    },
    {
      "epoch": 0.9246575342465754,
      "grad_norm": 1.1235238313674927,
      "learning_rate": 3.819444444444445e-06,
      "loss": 7.3938,
      "step": 135
    },
    {
      "epoch": 0.9315068493150684,
      "grad_norm": 1.031612753868103,
      "learning_rate": 3.4722222222222224e-06,
      "loss": 7.321,
      "step": 136
    },
    {
      "epoch": 0.9383561643835616,
      "grad_norm": 1.0285117626190186,
      "learning_rate": 3.125e-06,
      "loss": 7.1539,
      "step": 137
    },
    {
      "epoch": 0.9452054794520548,
      "grad_norm": 1.2402217388153076,
      "learning_rate": 2.777777777777778e-06,
      "loss": 7.2205,
      "step": 138
    },
    {
      "epoch": 0.952054794520548,
      "grad_norm": 1.1344407796859741,
      "learning_rate": 2.4305555555555557e-06,
      "loss": 7.1487,
      "step": 139
    },
    {
      "epoch": 0.958904109589041,
      "grad_norm": 1.0560462474822998,
      "learning_rate": 2.0833333333333334e-06,
      "loss": 7.2554,
      "step": 140
    },
    {
      "epoch": 0.9657534246575342,
      "grad_norm": 0.9984347224235535,
      "learning_rate": 1.7361111111111112e-06,
      "loss": 7.2642,
      "step": 141
    },
    {
      "epoch": 0.9726027397260274,
      "grad_norm": 1.123687982559204,
      "learning_rate": 1.388888888888889e-06,
      "loss": 7.2699,
      "step": 142
    },
    {
      "epoch": 0.9794520547945206,
      "grad_norm": 0.9642463326454163,
      "learning_rate": 1.0416666666666667e-06,
      "loss": 7.2265,
      "step": 143
    },
    {
      "epoch": 0.9863013698630136,
      "grad_norm": 1.02277410030365,
      "learning_rate": 6.944444444444445e-07,
      "loss": 7.2277,
      "step": 144
    },
    {
      "epoch": 0.9931506849315068,
      "grad_norm": 0.9711361527442932,
      "learning_rate": 3.4722222222222224e-07,
      "loss": 7.2003,
      "step": 145
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.0371366739273071,
      "learning_rate": 0.0,
      "loss": 7.2355,
      "step": 146
    }
  ],
  "logging_steps": 1,
  "max_steps": 146,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 461621035008000.0,
  "train_batch_size": 96,
  "trial_name": null,
  "trial_params": null
}
