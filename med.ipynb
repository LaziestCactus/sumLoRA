{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast\n",
    "from datasets import load_dataset\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "ds = load_dataset(\"keivalya/MedQuad-MedicalQnADataset\")\n",
    "\n",
    "# Extract the training and validation subsets\n",
    "training_data = ds['train'][:15000]\n",
    "validation_data = ds['train'][15000:16000]\n",
    "\n",
    "# Convert the dataset columns into dictionary format manually\n",
    "training_question = training_data['Question']\n",
    "training_answer = training_data['Answer']\n",
    "validation_question = validation_data['Question']\n",
    "validation_answer = validation_data['Answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class makeDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract the input_ids and attention_mask for the question\n",
    "        input_ids = self.inputs['input_ids'][idx]\n",
    "        attention_mask = self.inputs['attention_mask'][idx]\n",
    "\n",
    "        # Extract the labels (input_ids for the answer)\n",
    "        labels = self.targets['input_ids'][idx]\n",
    "\n",
    "        # Return the input and output as a dictionary\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Tokenized Training Questions Shape: torch.Size([15000, 64])\n",
      "Tokenized Training Answers Shape: torch.Size([15000, 64])\n",
      "Training dataset size: 15000\n",
      "Validation dataset size: 1000\n",
      "Model is on device: cuda:0\n",
      "Model size: 0.049 MB\n",
      "Before training trainable parameters: 811,008/125,250,816 (0.65%)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # Default to GPT small\n",
    "GPTmodel = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,  # Rank of the low-rank adaptation matrices\n",
    "    lora_alpha=32,  # LoRA scaling factor\n",
    "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
    "    target_modules = [\"c_attn\", \"c_proj\"]\n",
    ")\n",
    "\n",
    "# Prepare model for LoRA tuning\n",
    "model = get_peft_model(GPTmodel, lora_config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "#tokenize dataset\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "maxLength = 64\n",
    "tokenized_training_question = tokenizer(training_question, truncation=True, padding='max_length', return_tensors=\"pt\", max_length = maxLength)\n",
    "tokenized_training_answer = tokenizer(training_answer, truncation=True, padding=True, return_tensors=\"pt\", max_length = maxLength)\n",
    "tokenized_validation_question = tokenizer(validation_question, truncation=True, padding='max_length', return_tensors=\"pt\", max_length = maxLength)\n",
    "tokenized_validation_answer = tokenizer(validation_answer, truncation=True, padding=True, return_tensors=\"pt\", max_length = maxLength)\n",
    "\n",
    "print(f\"Tokenized Training Questions Shape: {tokenized_training_question['input_ids'].shape}\")\n",
    "print(f\"Tokenized Training Answers Shape: {tokenized_training_answer['input_ids'].shape}\")\n",
    "\n",
    "# Make sure it's divisible by batch size so last batch works fine\n",
    "batch_size = 32\n",
    "train_dataset = makeDataset(tokenized_training_question, tokenized_training_answer)\n",
    "val_dataset = makeDataset(tokenized_validation_question, tokenized_validation_answer)\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Define training arguments\n",
    "num_epochs = 1  # Number of training epochs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',        # Directory to save model checkpoints\n",
    "    num_train_epochs=num_epochs,            # Number of training epochs\n",
    "    per_device_train_batch_size=batch_size, # Batch size per device\n",
    "    per_device_eval_batch_size=batch_size,  # Batch size for evaluation\n",
    "    warmup_steps=2,              # Number of warmup steps\n",
    "    weight_decay=0.01,             # Weight decay\n",
    "    logging_dir='./logs',          # Directory to save logs\n",
    "    logging_steps=50,              # Log every X steps\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,                     # The model you are fine-tuning\n",
    "    args=training_args,              # Training arguments\n",
    "    train_dataset=train_dataset,     # Your training dataset\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Get model sizes\n",
    "def print_model_size(path):\n",
    "    size = 0\n",
    "    for f in os.scandir(path):\n",
    "        size += os.path.getsize(f)\n",
    "    print(f\"Model size: {(size / 1e6):.2} MB\")\n",
    "\n",
    "def print_trainable_parameters(model, label):\n",
    "    parameters, trainable = 0, 0    \n",
    "    for _, p in model.named_parameters():\n",
    "        parameters += p.numel()\n",
    "        trainable += p.numel() if p.requires_grad else 0\n",
    "    print(f\"{label} trainable parameters: {trainable:,}/{parameters:,} ({100 * trainable / parameters:.2f}%)\")\n",
    "\n",
    "#Fine-tune the model\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "print_model_size(training_args.output_dir)\n",
    "print_trainable_parameters(model, \"Before training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./medLora-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Optional\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Pass input_ids as labels for loss calculation\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     27\u001b[0m loss_hist\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "File \u001b[0;32m/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1272\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1272\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1287\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1133\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1121\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1122\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1123\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         output_attentions,\n\u001b[1;32m   1131\u001b[0m     )\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:615\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    613\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    614\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 615\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    624\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:518\u001b[0m, in \u001b[0;36mGPT2SdpaAttention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    516\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m encoder_attention_mask\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 518\u001b[0m     query, key, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_size, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    520\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_heads(query, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    521\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_heads(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n",
      "File \u001b[0;32m/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/peft/tuners/lora/layer.py:609\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 609\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     torch_result_dtype \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m active_adapter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters:\n",
      "File \u001b[0;32m/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/transformers/pytorch_utils.py:121\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    120\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m--> 121\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TESTING MODEL\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"medLora-model\"  # Replace with your model's path\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "num_batches = 0\n",
    "batch_size = 8  # Adjust based on your memory constraints\n",
    "loss_hist = [] # Storing it, no use for now\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(val_dataset), batch_size):\n",
    "        if(i+batch_size >= len(val_dataset)):\n",
    "            break\n",
    "        batch = val_dataset[i:i + batch_size]\n",
    "        # Get input_ids and attention_mask from the batch\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask'] if 'attention_mask' in batch else None  # Optional\n",
    "\n",
    "        # Pass input_ids as labels for loss calculation\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels= batch['labels'])\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss_hist.append(loss)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "# Calculate average loss and perplexity\n",
    "average_loss = total_loss / num_batches\n",
    "perplexity = torch.exp(torch.tensor(average_loss)).item()\n",
    "\n",
    "print(f\"Average Loss: {average_loss:.4f}\")\n",
    "print(f\"Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/richard/GPTLoRA/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 14.5246\n",
      "Perplexity: 2032163.3750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f01374857c0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the default GPT-2 Small model and tokenizer\n",
    "# TESTING BASEMODEL\n",
    "GPTmodel = \"gpt2\"  # This points to the default GPT-2 Small model\n",
    "tokenizer = AutoTokenizer.from_pretrained(GPTmodel)\n",
    "model = AutoModelForCausalLM.from_pretrained(GPTmodel)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "num_batches = 0\n",
    "batch_size = 8  # Adjust based on your memory constraints\n",
    "\n",
    "loss_hist = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(val_dataset), batch_size):\n",
    "        if(i+batch_size >= len(val_dataset)):\n",
    "            break\n",
    "        batch = val_dataset[i:i + batch_size]\n",
    "        # Get input_ids and attention_mask from the batch\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask'] if 'attention_mask' in batch else None  # Optional\n",
    "\n",
    "        # Pass input_ids as labels for loss calculation\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels= batch['labels'])\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss_hist.append(loss)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "# Calculate average loss and perplexity\n",
    "average_loss = total_loss / num_batches\n",
    "perplexity = torch.exp(torch.tensor(average_loss)).item()\n",
    "\n",
    "print(f\"Average Loss: {average_loss:.4f}\")\n",
    "print(f\"Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting LoRA Layer: base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.0.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.0.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.0.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.0.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.1.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.1.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.1.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.1.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.2.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.2.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.2.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.2.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.3.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.3.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.3.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.3.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.4.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.4.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.4.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.4.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.5.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.5.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.5.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.5.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.6.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.6.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.6.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.6.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.7.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.7.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.7.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.7.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.8.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.8.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.8.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.8.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.9.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.9.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.9.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.9.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.10.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.10.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.10.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.10.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.11.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.11.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.11.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.11.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "The number of modified parameter is 72.\n",
      "LoRA weights extracted and saved to lora_weights.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Load the base GPT-2 model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the fine-tuned model with LoRA layers\n",
    "model = PeftModel.from_pretrained(base_model, \"medLora-model\")\n",
    "\n",
    "# Initialize a dictionary to store LoRA weights\n",
    "lora_weights = {}\n",
    "\n",
    "# Iterate through model parameters and extract LoRA layers\n",
    "count = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        count = count + 1\n",
    "        print(f\"Extracting LoRA Layer: {name}, Shape: {param.shape}\")\n",
    "        lora_weights[name] = param.detach().cpu().numpy()\n",
    "        \n",
    "print(f\"The number of modified parameter is {count}.\")\n",
    "\n",
    "# Save the LoRA weights to a file\n",
    "with open(\"lora_weights.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lora_weights, f)\n",
    "\n",
    "print(\"LoRA weights extracted and saved to lora_weights.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/richa/Desktop/SumOfLoRA/sumOfLoRA/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA weight for: base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.0.attn.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.0.attn.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.0.mlp.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.0.mlp.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.1.attn.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.1.attn.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.1.mlp.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.1.mlp.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.2.attn.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.2.attn.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.2.mlp.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.2.mlp.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.3.attn.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.3.attn.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.3.mlp.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.3.mlp.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.4.attn.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.4.attn.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.4.mlp.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.4.mlp.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.5.attn.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.5.attn.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.5.mlp.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.5.mlp.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.6.attn.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.6.attn.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.6.mlp.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.6.mlp.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.7.attn.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.7.attn.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.7.mlp.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.7.mlp.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.8.attn.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.8.attn.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.8.mlp.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.8.mlp.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.9.attn.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.9.attn.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.9.mlp.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.9.mlp.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.10.attn.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.10.attn.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.10.mlp.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.10.mlp.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.11.attn.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.11.attn.c_proj.lora_B.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.11.mlp.c_proj.lora_A.default.weight\n",
      "Loading LoRA weight for: base_model.model.transformer.h.11.mlp.c_proj.lora_B.default.weight\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "test_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")  # Make sure this is the correct model (gpt2, not gp3)\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,  # Rank of the low-rank adaptation matrices\n",
    "    lora_alpha=32,  # LoRA scaling factor\n",
    "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
    "    target_modules = [\"c_attn\", \"c_proj\"]\n",
    ")\n",
    "# Prepare model for LoRA tuning\n",
    "test_model = get_peft_model(test_model, lora_config)\n",
    "\n",
    "# To reload the LoRA weights for a model:\n",
    "with open(\"mathAndMed_lora_weights.pkl\", \"rb\") as f:\n",
    "    loaded_lora_weights = pickle.load(f)\n",
    "count = 0\n",
    "# Apply the LoRA weights to the model\n",
    "for name, param in test_model.named_parameters():\n",
    "    if name in loaded_lora_weights:\n",
    "        count = count + 1\n",
    "        param.requires_grad = True\n",
    "        print(f\"Loading LoRA weight for: {name}\")\n",
    "        param.data.copy_(torch.tensor(loaded_lora_weights[name]))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 12.4952\n",
      "Perplexity: 267043.2188\n"
     ]
    }
   ],
   "source": [
    "#TESTING MODEL\n",
    "model = test_model\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "num_batches = 0\n",
    "batch_size = 8  # Adjust based on your memory constraints\n",
    "loss_hist = [] # Storing it, no use for now\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(val_dataset), batch_size):\n",
    "        if(i+batch_size >= len(val_dataset)):\n",
    "            break\n",
    "        batch = val_dataset[i:i + batch_size]\n",
    "        # Get input_ids and attention_mask from the batch\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask'] if 'attention_mask' in batch else None\n",
    "\n",
    "        # Pass input_ids as labels for loss calculation\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels= batch['labels'])\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss_hist.append(loss)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "# Calculate average loss and perplexity\n",
    "average_loss = total_loss / num_batches\n",
    "perplexity = torch.exp(torch.tensor(average_loss)).item()\n",
    "\n",
    "print(f\"Average Loss: {average_loss:.4f}\")\n",
    "print(f\"Perplexity: {perplexity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sumOfLoRA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
