{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/richard/GPTLoRA/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fifth training question: What are the treatments for Lymphocytic Choriomeningitis (LCM) ?\n",
      "Fifth training answer: Aseptic meningitis, encephalitis, or meningoencephalitis requires hospitalization and supportive treatment based on severity. Anti-inflammatory drugs, such as corticosteroids, may be considered under specific circumstances. Although studies have shown that ribavirin, a drug used to treat several other viral diseases, is effective against LCMV in vitro, there is no established evidence to support its routine use for treatment of LCM in humans.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"keivalya/MedQuad-MedicalQnADataset\")\n",
    "\n",
    "# Extract the training and validation subsets\n",
    "training_data = ds['train'][:14000]\n",
    "validation_data = ds['train'][14000:15000]\n",
    "\n",
    "# Convert the dataset columns into dictionary format manually\n",
    "training_question = training_data['Question']\n",
    "training_answer = training_data['Answer']\n",
    "validation_question = validation_data['Question']\n",
    "validation_answer = validation_data['Answer']\n",
    "\n",
    "print(f\"Fifth training question: {training_question[4]}\")\n",
    "print(f\"Fifth training answer: {training_answer[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class makeDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract the input_ids and attention_mask for the question\n",
    "        input_ids = self.inputs['input_ids'][idx]\n",
    "        attention_mask = self.inputs['attention_mask'][idx]\n",
    "\n",
    "        # Extract the labels (input_ids for the answer)\n",
    "        labels = self.targets['input_ids'][idx]\n",
    "\n",
    "        # Return the input and output as a dictionary\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/richard/GPTLoRA/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/richard/GPTLoRA/lib/python3.8/site-packages/peft/tuners/lora/layer.py:1091: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Training Questions Shape: torch.Size([14000, 64])\n",
      "Tokenized Training Answers Shape: torch.Size([14000, 64])\n",
      "Training dataset size: 14000\n",
      "Validation dataset size: 1000\n",
      "Model is on device: cuda:0\n",
      "Model size: 0.016 MB\n",
      "Before training trainable parameters: 811,008/125,250,816 (0.65%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/richard/GPTLoRA/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/richard/GPTLoRA/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='146' max='146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [146/146 02:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>13.916500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14.137700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>13.660100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>14.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>14.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>13.908200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>13.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>13.588100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>13.746000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>13.421600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>13.274200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>12.807000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>12.972300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>12.641800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>12.895700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>12.355300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>12.312300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>12.231900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>11.813000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>11.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>11.637200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>11.577200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>11.324100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>11.130100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>10.796700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>10.588100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>10.658900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>10.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>10.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>10.126200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>9.702400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>9.697600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>9.416600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>9.334500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>9.319500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>9.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>8.909500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>8.864400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>8.681200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>8.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>8.662100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>8.538500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>8.464000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>8.421700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>8.460700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>8.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>8.261100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>8.233500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>8.189900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>8.126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>8.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>8.049700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>8.049700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>8.128500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>7.961300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>7.999900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>7.990700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>7.884300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>7.947500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>7.837900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>7.901400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>7.832600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>7.768800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>7.830800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>7.929800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>7.867200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>7.748500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>7.653800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>7.717700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>7.647800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>7.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>7.701200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>7.739000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>7.660500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>7.675900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>7.644300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>7.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>7.687100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>7.686500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>7.646800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>7.558100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>7.645500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>7.526800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>7.666700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>7.544900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>7.478500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>7.425400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>7.508500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>7.383600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>7.382800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>7.301800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>7.401700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>7.435400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>7.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>7.317400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>7.413700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.173000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>7.344300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>7.586500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>7.375700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>7.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>7.297800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>7.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>7.294400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>7.067800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>7.350800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>7.292100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>7.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>7.252800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>7.192300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>7.196700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>7.305300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>7.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>7.230800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>7.184300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>7.185200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>7.346500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>7.313500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>7.229100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>7.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>7.100700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>7.232800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>7.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>7.098100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>7.174400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>7.227900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>7.159900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>7.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>7.283500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>7.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>7.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>7.393800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>7.321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>7.153900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>7.220500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>7.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>7.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>7.264200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>7.269900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>7.226500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>7.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>7.200300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>7.235500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # Default to GPT small\n",
    "GPTmodel = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,  # Rank of the low-rank adaptation matrices\n",
    "    lora_alpha=32,  # LoRA scaling factor\n",
    "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
    "    target_modules = [\"c_attn\", \"c_proj\"]\n",
    ")\n",
    "\n",
    "# Prepare model for LoRA tuning\n",
    "model = get_peft_model(GPTmodel, lora_config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "#tokenize dataset\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "maxLength = 64\n",
    "tokenized_training_question = tokenizer(training_question, truncation=True, padding='max_length', return_tensors=\"pt\", max_length = maxLength)\n",
    "tokenized_training_answer = tokenizer(training_answer, truncation=True, padding=True, return_tensors=\"pt\", max_length = maxLength)\n",
    "tokenized_validation_question = tokenizer(validation_question, truncation=True, padding='max_length', return_tensors=\"pt\", max_length = maxLength)\n",
    "tokenized_validation_answer = tokenizer(validation_answer, truncation=True, padding=True, return_tensors=\"pt\", max_length = maxLength)\n",
    "\n",
    "print(f\"Tokenized Training Questions Shape: {tokenized_training_question['input_ids'].shape}\")\n",
    "print(f\"Tokenized Training Answers Shape: {tokenized_training_answer['input_ids'].shape}\")\n",
    "\n",
    "# Make sure it's divisible by batch size so last batch works fine\n",
    "batch_size = 20\n",
    "# tokenized_training_question = tokenized_training_question[:len(tokenized_training_question) // batch_size * batch_size]\n",
    "# print(\"Training Question Tokenized Shape:\", tokenized_training_question['input_ids'].shape)\n",
    "# print(\"Training Question Tokenized Example:\", tokenized_training_question['input_ids'][0])\n",
    "# tokenized_training_answer = tokenized_training_answer[:len(tokenized_training_answer) // batch_size * batch_size]\n",
    "# tokenized_validation_question = tokenized_validation_question[:len(tokenized_validation_question) // batch_size * batch_size]\n",
    "# tokenized_validation_answer = tokenized_validation_answer[:len(tokenized_validation_answer) // batch_size * batch_size]\n",
    "\n",
    "train_dataset = makeDataset(tokenized_training_question, tokenized_training_answer)\n",
    "val_dataset = makeDataset(tokenized_validation_question, tokenized_validation_answer)\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Define training arguments\n",
    "num_epochs = 2  # Number of training epochs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',        # Directory to save model checkpoints\n",
    "    num_train_epochs=num_epochs,            # Number of training epochs\n",
    "    per_device_train_batch_size=batch_size, # Batch size per device\n",
    "    per_device_eval_batch_size=batch_size,  # Batch size for evaluation\n",
    "    warmup_steps=2,              # Number of warmup steps\n",
    "    weight_decay=0.01,             # Weight decay\n",
    "    logging_dir='./logs',          # Directory to save logs\n",
    "    logging_steps=1,              # Log every X steps\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,                     # The model you are fine-tuning\n",
    "    args=training_args,              # Training arguments\n",
    "    train_dataset=train_dataset,     # Your training dataset\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Get model sizes\n",
    "def print_model_size(path):\n",
    "    size = 0\n",
    "    for f in os.scandir(path):\n",
    "        size += os.path.getsize(f)\n",
    "    print(f\"Model size: {(size / 1e6):.2} MB\")\n",
    "\n",
    "def print_trainable_parameters(model, label):\n",
    "    parameters, trainable = 0, 0    \n",
    "    for _, p in model.named_parameters():\n",
    "        parameters += p.numel()\n",
    "        trainable += p.numel() if p.requires_grad else 0\n",
    "    print(f\"{label} trainable parameters: {trainable:,}/{parameters:,} ({100 * trainable / parameters:.2f}%)\")\n",
    "\n",
    "#Fine-tune the model\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "print_model_size(training_args.output_dir)\n",
    "print_trainable_parameters(model, \"Before training\")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./medLora-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.3183)\n",
      "tensor(7.3294)\n",
      "tensor(7.2995)\n",
      "tensor(7.3135)\n",
      "tensor(7.2060)\n",
      "tensor(7.3806)\n",
      "tensor(7.0278)\n",
      "tensor(7.2487)\n",
      "tensor(6.6319)\n",
      "tensor(7.2249)\n",
      "tensor(7.6197)\n",
      "tensor(7.1879)\n",
      "tensor(7.6788)\n",
      "tensor(6.9931)\n",
      "tensor(7.7484)\n",
      "tensor(7.5299)\n",
      "tensor(7.2924)\n",
      "tensor(6.9563)\n",
      "tensor(7.0148)\n",
      "tensor(6.7854)\n",
      "tensor(7.3179)\n",
      "tensor(7.3432)\n",
      "tensor(7.3739)\n",
      "tensor(7.7498)\n",
      "tensor(7.3745)\n",
      "tensor(7.3744)\n",
      "tensor(7.4357)\n",
      "tensor(7.4930)\n",
      "tensor(7.2250)\n",
      "tensor(7.2402)\n",
      "tensor(7.3346)\n",
      "tensor(7.7874)\n",
      "tensor(7.7306)\n",
      "tensor(7.5272)\n",
      "tensor(7.0317)\n",
      "tensor(7.4222)\n",
      "tensor(6.9020)\n",
      "tensor(7.4866)\n",
      "tensor(7.2764)\n",
      "tensor(7.5234)\n",
      "tensor(7.1299)\n",
      "tensor(7.2833)\n",
      "tensor(7.4841)\n",
      "tensor(7.3719)\n",
      "tensor(7.0758)\n",
      "tensor(7.8136)\n",
      "tensor(7.0310)\n",
      "tensor(7.2850)\n",
      "tensor(7.0177)\n",
      "tensor(7.0615)\n",
      "tensor(7.2285)\n",
      "tensor(7.3028)\n",
      "tensor(7.4852)\n",
      "tensor(7.7306)\n",
      "tensor(7.2781)\n",
      "tensor(7.2267)\n",
      "tensor(7.3286)\n",
      "tensor(7.5175)\n",
      "tensor(6.9162)\n",
      "tensor(7.1635)\n",
      "tensor(7.3636)\n",
      "tensor(7.1507)\n",
      "tensor(7.5057)\n",
      "tensor(7.0711)\n",
      "tensor(7.1043)\n",
      "tensor(7.2963)\n",
      "tensor(7.3723)\n",
      "tensor(7.3373)\n",
      "tensor(7.4136)\n",
      "tensor(7.2130)\n",
      "tensor(7.1345)\n",
      "tensor(7.5014)\n",
      "tensor(7.3935)\n",
      "tensor(7.1986)\n",
      "tensor(7.3141)\n",
      "tensor(7.3283)\n",
      "tensor(7.1732)\n",
      "tensor(7.4901)\n",
      "tensor(7.2413)\n",
      "tensor(7.3041)\n",
      "tensor(6.9439)\n",
      "tensor(7.3355)\n",
      "tensor(7.1031)\n",
      "tensor(7.3439)\n",
      "tensor(7.2539)\n",
      "tensor(7.4874)\n",
      "tensor(7.2266)\n",
      "tensor(7.4673)\n",
      "tensor(7.2339)\n",
      "tensor(7.3197)\n",
      "tensor(7.6110)\n",
      "tensor(7.0607)\n",
      "tensor(7.2572)\n",
      "tensor(7.3058)\n",
      "tensor(7.7730)\n",
      "tensor(7.4493)\n",
      "tensor(7.3443)\n",
      "tensor(7.0960)\n",
      "tensor(7.2423)\n",
      "tensor(7.3481)\n",
      "tensor(7.5857)\n",
      "tensor(7.3713)\n",
      "tensor(7.3035)\n",
      "tensor(7.5729)\n",
      "tensor(7.1994)\n",
      "tensor(6.9461)\n",
      "tensor(7.3388)\n",
      "tensor(7.4445)\n",
      "tensor(7.0160)\n",
      "tensor(7.0586)\n",
      "tensor(7.1327)\n",
      "tensor(7.5205)\n",
      "tensor(7.6336)\n",
      "tensor(7.5068)\n",
      "tensor(7.4003)\n",
      "tensor(7.2243)\n",
      "tensor(7.4245)\n",
      "tensor(7.1263)\n",
      "tensor(7.4870)\n",
      "tensor(7.3939)\n",
      "tensor(7.4432)\n",
      "tensor(7.4555)\n",
      "tensor(7.5358)\n",
      "tensor(6.6404)\n",
      "Average Loss: 7.3092\n",
      "Perplexity: 1493.9741\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"medLora-model\"  # Replace with your model's path\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "num_batches = 0\n",
    "batch_size = 8  # Adjust based on your memory constraints\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(val_dataset), batch_size):\n",
    "        if(i+batch_size >= len(val_dataset)):\n",
    "            break\n",
    "        batch = val_dataset[i:i + batch_size]\n",
    "        # Get input_ids and attention_mask from the batch\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask'] if 'attention_mask' in batch else None  # Optional\n",
    "\n",
    "        # Pass input_ids as labels for loss calculation\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels= batch['labels'])\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        print(loss)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "# Calculate average loss and perplexity\n",
    "average_loss = total_loss / num_batches\n",
    "perplexity = torch.exp(torch.tensor(average_loss)).item()\n",
    "\n",
    "print(f\"Average Loss: {average_loss:.4f}\")\n",
    "print(f\"Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/richard/GPTLoRA/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.2888)\n",
      "tensor(14.9782)\n",
      "tensor(15.1596)\n",
      "tensor(14.1524)\n",
      "tensor(14.4816)\n",
      "tensor(14.5441)\n",
      "tensor(14.2112)\n",
      "tensor(14.7085)\n",
      "tensor(13.4263)\n",
      "tensor(14.1028)\n",
      "tensor(15.1642)\n",
      "tensor(15.2056)\n",
      "tensor(14.7975)\n",
      "tensor(13.5777)\n",
      "tensor(14.9535)\n",
      "tensor(15.0056)\n",
      "tensor(15.0133)\n",
      "tensor(14.6804)\n",
      "tensor(14.3289)\n",
      "tensor(14.1136)\n",
      "tensor(14.9169)\n",
      "tensor(14.7038)\n",
      "tensor(14.5362)\n",
      "tensor(15.1375)\n",
      "tensor(14.9126)\n",
      "tensor(14.8563)\n",
      "tensor(14.3381)\n",
      "tensor(14.8082)\n",
      "tensor(14.2995)\n",
      "tensor(14.7002)\n",
      "tensor(14.8756)\n",
      "tensor(15.0656)\n",
      "tensor(14.5521)\n",
      "tensor(14.6520)\n",
      "tensor(14.3047)\n",
      "tensor(14.7539)\n",
      "tensor(14.2916)\n",
      "tensor(14.9533)\n",
      "tensor(14.4720)\n",
      "tensor(15.0784)\n",
      "tensor(14.3151)\n",
      "tensor(14.1776)\n",
      "tensor(14.9997)\n",
      "tensor(14.7585)\n",
      "tensor(14.1314)\n",
      "tensor(15.3629)\n",
      "tensor(14.5348)\n",
      "tensor(14.7129)\n",
      "tensor(14.2549)\n",
      "tensor(14.2690)\n",
      "tensor(14.9771)\n",
      "tensor(14.5972)\n",
      "tensor(14.8759)\n",
      "tensor(14.3634)\n",
      "tensor(14.2969)\n",
      "tensor(14.1811)\n",
      "tensor(15.1462)\n",
      "tensor(14.9558)\n",
      "tensor(14.5929)\n",
      "tensor(15.2540)\n",
      "tensor(15.2891)\n",
      "tensor(14.1834)\n",
      "tensor(14.7639)\n",
      "tensor(14.0707)\n",
      "tensor(14.1433)\n",
      "tensor(13.7532)\n",
      "tensor(14.8831)\n",
      "tensor(14.6926)\n",
      "tensor(14.1725)\n",
      "tensor(14.0443)\n",
      "tensor(15.4172)\n",
      "tensor(15.7057)\n",
      "tensor(14.6967)\n",
      "tensor(14.6559)\n",
      "tensor(14.9153)\n",
      "tensor(14.2674)\n",
      "tensor(14.3732)\n",
      "tensor(14.5003)\n",
      "tensor(15.1628)\n",
      "tensor(14.4933)\n",
      "tensor(14.4451)\n",
      "tensor(14.0454)\n",
      "tensor(14.4656)\n",
      "tensor(13.7092)\n",
      "tensor(14.2647)\n",
      "tensor(14.7948)\n",
      "tensor(13.6793)\n",
      "tensor(14.1892)\n",
      "tensor(14.3261)\n",
      "tensor(13.8373)\n",
      "tensor(14.8986)\n",
      "tensor(14.6287)\n",
      "tensor(14.6467)\n",
      "tensor(14.6216)\n",
      "tensor(14.6556)\n",
      "tensor(14.3791)\n",
      "tensor(14.0878)\n",
      "tensor(14.6459)\n",
      "tensor(14.5410)\n",
      "tensor(14.2926)\n",
      "tensor(14.4593)\n",
      "tensor(14.6821)\n",
      "tensor(14.5282)\n",
      "tensor(15.0024)\n",
      "tensor(14.3078)\n",
      "tensor(13.8274)\n",
      "tensor(14.7872)\n",
      "tensor(14.8721)\n",
      "tensor(14.3416)\n",
      "tensor(14.1441)\n",
      "tensor(14.2562)\n",
      "tensor(14.3095)\n",
      "tensor(15.1051)\n",
      "tensor(15.4212)\n",
      "tensor(14.2056)\n",
      "tensor(14.2530)\n",
      "tensor(14.6680)\n",
      "tensor(14.4301)\n",
      "tensor(15.4630)\n",
      "tensor(15.1475)\n",
      "tensor(13.7628)\n",
      "tensor(14.6501)\n",
      "tensor(14.8483)\n",
      "tensor(13.9820)\n",
      "Average Loss: 14.5698\n",
      "Perplexity: 2126052.5000\n"
     ]
    }
   ],
   "source": [
    "# Load the default GPT-2 Small model and tokenizer\n",
    "GPTmodel = \"gpt2\"  # This points to the default GPT-2 Small model\n",
    "tokenizer = AutoTokenizer.from_pretrained(GPTmodel)\n",
    "model = AutoModelForCausalLM.from_pretrained(GPTmodel)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "num_batches = 0\n",
    "batch_size = 8  # Adjust based on your memory constraints\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(val_dataset), batch_size):\n",
    "        if(i+batch_size >= len(val_dataset)):\n",
    "            break\n",
    "        batch = val_dataset[i:i + batch_size]\n",
    "        # Get input_ids and attention_mask from the batch\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask'] if 'attention_mask' in batch else None  # Optional\n",
    "\n",
    "        # Pass input_ids as labels for loss calculation\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels= batch['labels'])\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        print(loss)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "# Calculate average loss and perplexity\n",
    "average_loss = total_loss / num_batches\n",
    "perplexity = torch.exp(torch.tensor(average_loss)).item()\n",
    "\n",
    "print(f\"Average Loss: {average_loss:.4f}\")\n",
    "print(f\"Perplexity: {perplexity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPTLoRA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
