{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79830260-20dc-41b9-8f4f-c1db36c8eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast\n",
    "from datasets import load_dataset\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7dd1ed5-eb43-4caf-ad9b-257080fccc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5600, Validation size: 700, Test size: 700\n",
      "{'answer': 'Reduced demand for services, primarily from macroeconomic conditions', 'question': 'With respect to FDX company What factors contributed to the decrease in total average daily volume for FedEx Ground in 2023?'}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset - https://huggingface.co/datasets/itzme091/financial-qa-10K-modified\n",
    "dataset = load_dataset(\"itzme091/financial-qa-10K-modified\")\n",
    "# Split the dataset into train (80%) and temp (20%)\n",
    "train_val_split = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Split temp (20%) into validation (10%) and test (10%)\n",
    "val_test_split = train_val_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "train_data = train_val_split[\"train\"]\n",
    "validation_data = val_test_split[\"train\"]\n",
    "test_data = val_test_split[\"test\"]\n",
    "\n",
    "print(f\"Train size: {len(train_data)}, Validation size: {len(validation_data)}, Test size: {len(test_data)}\") # Print dataset sizes\n",
    "print(train_data[0]) # Print sample Q/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fde9b49d-cfbf-483b-ab19-0518cb2498b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialQADataset(Dataset):\n",
    "\n",
    "    def __init__(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (dict): A dictionary containing the financial questions.\n",
    "            targets (dict): A dictionary containing the corresponding answers.\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples in the dataset\n",
    "        return len(self.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the data sample to fetch.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the question, attention mask, and the answer.\n",
    "        \"\"\"\n",
    "        # Extract the question (input data)\n",
    "        question_input_ids = self.inputs['input_ids'][idx]  # Tokenized version of the question\n",
    "        attention_mask = self.inputs['attention_mask'][\n",
    "            idx]  # Attention mask for the question tokens (shows which tokens in answer are relevant)\n",
    "        # Extract the corresponding answer (target data)\n",
    "        answer_labels = self.targets['input_ids'][idx]  # Tokenized version of the answer\n",
    "\n",
    "        # Return the question, attention mask, and answer as a dictionary\n",
    "        return {\n",
    "            'input_ids': question_input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': answer_labels  # Answer as the label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ca1d1c7-1e43-430a-9910-0d8ce886b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "GPTmodel = GPT2LMHeadModel.from_pretrained(model_name) \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8a3f62b-c2fc-4355-9234-eef9350e82c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, # Causal language modeling\n",
    "    r=8,  # Rank of the low-rank adaptation matrices\n",
    "    lora_alpha=32,  # LoRA scaling factor\n",
    "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
    "    target_modules=[\"c_attn\", \"c_proj\"]  # Applying LoRA to attention and projection layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae934004-9faa-47a2-93ba-df1b66b9c4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clem/DeepLearningLab/LoraModel/.venv/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2SdpaAttention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=2304, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=768, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=3072, nx=768)\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=768, nx=3072)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare model for LoRA tuning\n",
    "model = get_peft_model(GPTmodel, lora_config) # Creates new version of GPT-2 model that incorporates LoRA modifications\n",
    "device = torch.device(\"mps\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98f34f84-3854-49b8-a331-816746b8baa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Training Questions Shape: torch.Size([5600, 128])\n",
      "Tokenized Training Answers Shape: torch.Size([5600, 128])\n"
     ]
    }
   ],
   "source": [
    "# Define maximum sequence length for financial data\n",
    "max_length = 128  # Increased length to accommodate detailed financial Q&A \n",
    "                  # specifies the maximum number of tokens that the tokenizer will generate for each input text\n",
    "\n",
    "# Extract questions and answers from the dataset\n",
    "training_question = train_data[\"question\"]\n",
    "training_answer = train_data[\"answer\"]\n",
    "\n",
    "validation_question = validation_data[\"question\"]\n",
    "validation_answer = validation_data[\"answer\"]\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenizer.pad_token = tokenizer.eos_token # Defines padding token for GPT-2, which lacks it\n",
    "tokenized_training_question = tokenizer(\n",
    "    training_question,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=max_length\n",
    ")\n",
    "tokenized_training_answer = tokenizer(\n",
    "    training_answer,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=max_length\n",
    ")\n",
    "tokenized_validation_question = tokenizer(\n",
    "    validation_question,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=max_length\n",
    ")\n",
    "tokenized_validation_answer = tokenizer(\n",
    "    validation_answer,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "print(f\"Tokenized Training Questions Shape: {tokenized_training_question['input_ids'].shape}\")\n",
    "print(f\"Tokenized Training Answers Shape: {tokenized_training_answer['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b59cd61a-b833-48bf-bcb4-f2babbeb5a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 5600\n",
      "Validation dataset size: 700\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_dataset = FinancialQADataset(tokenized_training_question, tokenized_training_answer)\n",
    "val_dataset = FinancialQADataset(tokenized_validation_question, tokenized_validation_answer)\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14ce724e-ef06-4bd3-a418-392249fd12e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available: True\n",
      "MPS device: mps\n"
     ]
    }
   ],
   "source": [
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "print(\"MPS device:\", torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9627a129-6132-4344-89d6-129faa586d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training will run for 1 epochs.\n",
      "Batch size: 32\n",
      "Logging directory: ./logs\n",
      "Checkpoints will be saved in: ./financial_model_results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clem/DeepLearningLab/LoraModel/.venv/lib/python3.12/site-packages/transformers/training_args.py:1590: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "/var/folders/4b/yqq_z6813q3833dq1xdv77jh0000gp/T/ipykernel_27359/3426364955.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./financial_model_results', # Specifies directory where model checkpoints will be saved\n",
    "    overwrite_output_dir=True, # Allows the overwriting of the contents of the output_dir\n",
    "    num_train_epochs=epochs, # Specifies number of times the model will go through the entire dataset\n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=2, # Defines the number of steps to perform learning rate warmup (0 -> lr)\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50, # Specifies how often to log training information\n",
    "    fp16=False,                 \n",
    "    bf16=False,                 \n",
    "    no_cuda=True,               \n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,                     \n",
    "    args=training_args,             \n",
    "    train_dataset=train_dataset,    \n",
    "    eval_dataset=val_dataset,       \n",
    "    tokenizer=tokenizer,            \n",
    ")\n",
    "\n",
    "# Print details about the training setup\n",
    "print(f\"Training will run for {epochs} epochs.\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Logging directory: {training_args.logging_dir}\")\n",
    "print(f\"Checkpoints will be saved in: {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3d01b3c-8bc4-493b-b833-1f98f3b9e23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cpu\n",
      "Model size: 0.00 MB\n",
      "Before training trainable parameters: 811,008/125,250,816 (0.65%)\n"
     ]
    }
   ],
   "source": [
    "# Get model sizes\n",
    "def print_model_size(path):\n",
    "    size = 0\n",
    "    for f in os.scandir(path):\n",
    "        size += os.path.getsize(f)\n",
    "    print(f\"Model size: {(size / 1e6):.2f} MB\")\n",
    "\n",
    "def print_trainable_parameters(model, label):\n",
    "    parameters, trainable = 0, 0    \n",
    "    for _, p in model.named_parameters():\n",
    "        parameters += p.numel()\n",
    "        trainable += p.numel() if p.requires_grad else 0\n",
    "    print(f\"{label} trainable parameters: {trainable:,}/{parameters:,} ({100 * trainable / parameters:.2f}%)\")\n",
    "\n",
    "#Fine-tune the model\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "print_model_size(training_args.output_dir)\n",
    "print_trainable_parameters(model, \"Before training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f028015d-cee4-4224-b0f2-e1cb5dad3252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [175/175 49:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.889000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Training the model\n",
    "trainer.train()\n",
    "trainer.save_model(\"./FinancialLora-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f7c61a5-0938-4443-b44b-8d011d66df91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.7885\n",
      "Perplexity: 5.9807\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"FinancialLora-model\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name) # Loads the fine-tuned model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # Load tokenizer\n",
    "\n",
    "# Check if a GPU or MPS is available, and move model to that device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "num_batches = 0\n",
    "batch_size = 8\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(val_dataset), batch_size):\n",
    "        if i + batch_size >= len(val_dataset):\n",
    "            break\n",
    "        batch = val_dataset[i:i + batch_size]\n",
    "        \n",
    "        # Get input_ids and attention_mask from the batch and move to device\n",
    "        input_ids = batch['input_ids'].to(device) # Extracts tokenized text from device\n",
    "        attention_mask = batch['attention_mask'].to(device) if 'attention_mask' in batch else None # Extracts attention mask\n",
    "        labels = batch['labels'].to(device) # Extracts target labels\n",
    "\n",
    "        # Pass input_ids as labels for loss calculation\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "# Calculate average loss and perplexity\n",
    "average_loss = total_loss / num_batches\n",
    "perplexity = torch.exp(torch.tensor(average_loss)).item()\n",
    "\n",
    "print(f\"Average Loss: {average_loss:.4f}\")\n",
    "print(f\"Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d6924c7-0864-4736-a0ec-03bf760cd0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 11.3487\n",
      "Perplexity: 84853.9844\n"
     ]
    }
   ],
   "source": [
    "# Load the default GPT-2 Small model and tokenizer\n",
    "GPTmodel = \"gpt2\"  # This points to the default GPT-2 Small model\n",
    "tokenizer = AutoTokenizer.from_pretrained(GPTmodel)\n",
    "model = AutoModelForCausalLM.from_pretrained(GPTmodel)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "num_batches = 1\n",
    "batch_size = 8  # Adjust based on your memory constraints\n",
    "\n",
    "loss_hist = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(val_dataset), batch_size):\n",
    "        if(i+batch_size >= len(val_dataset)):\n",
    "            break\n",
    "        batch = val_dataset[i:i + batch_size]\n",
    "        # Get input_ids and attention_mask from the batch\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask'] if 'attention_mask' in batch else None\n",
    "\n",
    "        # Pass input_ids as labels for loss calculation\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels= batch['labels'])\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss_hist.append(loss)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "# Calculate average loss and perplexity\n",
    "average_loss = total_loss / num_batches\n",
    "perplexity = torch.exp(torch.tensor(average_loss)).item()\n",
    "\n",
    "print(f\"Average Loss: {average_loss:.4f}\")\n",
    "print(f\"Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db0a0488-1369-4330-9b57-8563468bb50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting LoRA Layer: base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.0.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.0.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.0.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.0.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.1.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.1.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.1.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.1.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.2.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.2.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.2.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.2.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.3.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.3.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.3.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.3.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.4.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.4.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.4.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.4.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.5.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.5.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.5.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.5.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.6.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.6.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.6.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.6.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.7.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.7.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.7.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.7.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.8.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.8.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.8.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.8.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.9.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.9.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.9.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.9.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.10.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.10.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.10.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.10.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight, Shape: torch.Size([2304, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.11.attn.c_proj.lora_A.default.weight, Shape: torch.Size([8, 768])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.11.attn.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.11.mlp.c_proj.lora_A.default.weight, Shape: torch.Size([8, 3072])\n",
      "Extracting LoRA Layer: base_model.model.transformer.h.11.mlp.c_proj.lora_B.default.weight, Shape: torch.Size([768, 8])\n",
      "The number of modified parameters is 72.\n",
      "LoRA weights extracted and saved to lora_weights.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the base GPT-2 model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\") # Unmodified base GPT-2 model\n",
    "\n",
    "# Load the fine-tuned model with LoRA layers\n",
    "model = PeftModel.from_pretrained(base_model, \"FinancialLora-model\")\n",
    "\n",
    "# Initialize a dictionary to store LoRA weights\n",
    "lora_weights = {}\n",
    "\n",
    "# Iterate through model parameters and extract LoRA layers\n",
    "count = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:  # Check if the parameter is part of LoRA layers\n",
    "        count += 1 \n",
    "        print(f\"Extracting LoRA Layer: {name}, Shape: {param.shape}\")\n",
    "        lora_weights[name] = param.detach().cpu().numpy()  # Detach and store weights as numpy arrays\n",
    "\n",
    "# Output the number of LoRA-modified parameters\n",
    "print(f\"The number of modified parameters is {count}.\")\n",
    "\n",
    "# Save the LoRA weights to a file\n",
    "with open(\"lora_weights.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lora_weights, f)\n",
    "\n",
    "print(\"LoRA weights extracted and saved to lora_weights.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742fe467-7e9c-4e90-b28b-3c0c4965ab79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
